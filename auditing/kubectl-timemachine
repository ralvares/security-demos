#!/usr/bin/env python3
import argparse
import yaml
import json
import sys
import os
from datetime import datetime, timezone

# ==============================================================================
# CONFIGURATION
# ==============================================================================

RESOURCE_ALIASES = {
    'po': 'pods', 'pod': 'pods', 'pods': 'pods',
    'svc': 'services', 'service': 'services', 'services': 'services',
    'cm': 'configmaps', 'configmap': 'configmaps', 'configmaps': 'configmaps',
    'sa': 'serviceaccounts', 'serviceaccount': 'serviceaccounts', 'serviceaccounts': 'serviceaccounts',
    'ns': 'namespaces', 'namespace': 'namespaces', 'namespaces': 'namespaces',
    'no': 'nodes', 'node': 'nodes', 'nodes': 'nodes',
    'se': 'secrets', 'secret': 'secrets', 'secrets': 'secrets',
    'pv': 'persistentvolumes', 'persistentvolume': 'persistentvolumes', 'persistentvolumes': 'persistentvolumes',
    'pvc': 'persistentvolumeclaims', 'persistentvolumeclaim': 'persistentvolumeclaims', 'persistentvolumeclaims': 'persistentvolumeclaims',
    'deploy': 'deployments', 'deployment': 'deployments', 'deployments': 'deployments',
    'ds': 'daemonsets', 'daemonset': 'daemonsets', 'daemonsets': 'daemonsets',
    'sts': 'statefulsets', 'statefulset': 'statefulsets', 'statefulsets': 'statefulsets',
    'rs': 'replicasets', 'replicaset': 'replicasets', 'replicasets': 'replicasets',
    'job': 'jobs', 'jobs': 'jobs',
    'cj': 'cronjobs', 'cronjob': 'cronjobs', 'cronjobs': 'cronjobs',
    'ing': 'ingresses', 'ingress': 'ingresses', 'ingresses': 'ingresses',
    'rt': 'routes', 'route': 'routes', 'routes': 'routes',
    'netpol': 'networkpolicies', 'networkpolicy': 'networkpolicies',
    'vm': 'virtualmachines', 'virtualmachine': 'virtualmachines', 'virtualmachines': 'virtualmachines',
    'vmi': 'virtualmachineinstances', 'virtualmachineinstance': 'virtualmachineinstances', 'virtualmachineinstances': 'virtualmachineinstances',
    'all': 'all'
}

CLUSTER_SCOPED = {
    'namespaces', 'nodes', 'persistentvolumes', 
    'clusterrolebindings', 'clusterroles'
}

REDACTED_RESOURCES = {
    'secrets', 'routes', 'oauthaccesstokens', 
    'oauthauthorizetokens', 'nodes'
}

# --- HISTORY MODE CONFIGURATION (Only used when --history is passed) ---
HTTP_STATUS_MAP = {
    101: "Stream", 200: "OK", 201: "Created", 202: "Accepted", 204: "No Content",
    304: "Not Modified", 400: "Bad Request", 401: "Unauthorized", 403: "Forbidden",
    404: "Not Found", 409: "Conflict", 422: "Invalid", 429: "Throttled",
    500: "Server Error", 503: "Unavailable", 504: "Timeout"
}

NOISY_USERS = [
    'system:node:', 'system:kube-scheduler', 'system:apiserver', 'system:kube-controller-manager',
    'system:multus', 'openshift-sdn', 'openshift-multus', 'openshift-machine-config-operator',
    ':deployment-controller', ':replicaset-controller', ':statefulset-controller',
    ':daemonset-controller', ':job-controller', ':cronjob-controller',
    ':horizontal-pod-autoscaler', ':generic-garbage-collector'
]

# ==============================================================================
# CORE FUNCTIONS
# ==============================================================================

def setup_args():
    parser = argparse.ArgumentParser(
        description="timemachine: Forensic Digital Twin for Kubernetes",
        usage="%(prog)s [flags] <command> <resource> [name]"
    )
    
    parser.add_argument("--auditlog-file", default="audit.log", help="Path to audit.log file")
    parser.add_argument("--time", help="Snapshot time (ISO8601 or YYYY-MM-DD)")
    parser.add_argument("-n", "--namespace", help="Namespace scope")
    parser.add_argument("-A", "--all-namespaces", action="store_true", help="List in all namespaces")
    parser.add_argument("-o", "--output", choices=["yaml", "json", "table", "wide"], default="table", help="Output format")
    parser.add_argument("-l", "--selector", help="Selector (label query) to filter on")
    parser.add_argument("--show-labels", action="store_true", help="Show all labels as the last column")
    
    # NEW FLAG
    parser.add_argument("--history", action="store_true", help="Trace the full lineage (User -> Deployment -> Pod)")

    parser.add_argument("args", nargs="*", help="Command and Resource")
    
    return parser

def normalize_time(time_str):
    if not time_str: return None
    if len(time_str) == 10 and '-' in time_str and 'T' not in time_str:
        time_str += "T23:59:59"

    try:
        if time_str.endswith('Z'): return time_str
        fmt = "%Y-%m-%dT%H:%M:%S"
        if len(time_str) == 16: fmt = "%Y-%m-%dT%H:%M"
        local_dt = datetime.strptime(time_str, fmt).astimezone()
        utc_dt = local_dt.astimezone(timezone.utc)
        utc_str = utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        print(f"--- ðŸ•’ Snapshot: {time_str.replace('T', ' ')} (Local) â†’ {utc_str} (UTC) ---")
        return utc_str
    except Exception:
        if not time_str.endswith('Z'): return time_str + 'Z'
        return time_str

def safe_parse_json(obj):
    if isinstance(obj, dict): return obj
    if not isinstance(obj, str): return {}
    try:
        parsed = json.loads(obj)
        if isinstance(parsed, str):
            try: return json.loads(parsed)
            except: return {}
        return parsed
    except: return {}

def get_path(d, *keys, default=None):
    current = d
    for k in keys:
        if not isinstance(current, dict): return default
        current = current.get(k)
        if current is None: return default
    return current

def calculate_age(creation_ts, snapshot_time_utc):
    if not creation_ts: return "?"
    try:
        ts = creation_ts.replace(' ', 'T')
        if not ts.endswith('Z'): ts += 'Z'
        fmt = "%Y-%m-%dT%H:%M:%SZ"
        if "." in ts: fmt = "%Y-%m-%dT%H:%M:%S.%fZ"
        created = datetime.strptime(ts, fmt).replace(tzinfo=timezone.utc)
        
        if snapshot_time_utc:
            now = datetime.strptime(snapshot_time_utc, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        else:
            now = datetime.now(timezone.utc)

        delta = now - created
        if delta.total_seconds() < 0: return "0s" 
        if delta.days > 0: return f"{delta.days}d"
        hours, remainder = divmod(delta.seconds, 3600)
        minutes, _ = divmod(remainder, 60)
        if hours > 0: return f"{hours}h{minutes}m"
        return f"{minutes}m"
    except: return "?"

# ==============================================================================
# EXTRACTION LOGIC
# ==============================================================================

def extract_status(obj):
    """Reconstructs Pod Status (Running, Error, CrashLoopBackOff, etc.)"""
    if not obj: return "Unknown"
    metadata = obj.get('metadata', {})
    if metadata.get('deletionTimestamp'): return "Terminating"
    
    status = obj.get('status', {})
    phase = status.get('phase', 'Unknown')
    
    container_statuses = status.get('containerStatuses', [])
    if not isinstance(container_statuses, list): container_statuses = []

    for cs in container_statuses:
        state = cs.get('state', {})
        if 'waiting' in state:
            reason = state['waiting'].get('reason')
            if reason: return reason
        if 'terminated' in state:
            reason = state['terminated'].get('reason')
            exit_code = state['terminated'].get('exitCode')
            if exit_code != 0: return reason if reason else "Error"
    return phase

def extract_pod_ip(obj):
    ip = get_path(obj, 'status', 'podIP')
    if ip: return ip
    try:
        ovn_annot = get_path(obj, 'metadata', 'annotations', 'k8s.ovn.org/pod-networks')
        if ovn_annot:
            ovn_data = json.loads(ovn_annot)
            if 'default' in ovn_data:
                ip = ovn_data['default'].get('ip_address')
                if ip: return ip.split('/')[0]
    except: pass
    return '<none>'

def extract_labels(obj):
    labels = get_path(obj, 'metadata', 'labels')
    if not labels or not isinstance(labels, dict): return "" 
    return ",".join([f"{k}={v}" for k, v in sorted(labels.items())])

def extract_wide_info(resource, obj):
    if not obj: return []
    info = []
    if resource == 'pods':
        ip = extract_pod_ip(obj)
        node = get_path(obj, 'spec', 'nodeName', default='<none>')
        info = [ip, node]
    elif resource == 'services':
        svc_type = get_path(obj, 'spec', 'type', default='ClusterIP')
        cluster_ip = get_path(obj, 'spec', 'clusterIP', default='<none>')
        selector = ",".join([f"{k}={v}" for k, v in get_path(obj, 'spec', 'selector', default={}).items()]) if get_path(obj, 'spec', 'selector') else "<none>"
        info = [svc_type, cluster_ip, selector]
    elif resource == 'nodes':
        os_img = get_path(obj, 'status', 'nodeInfo', 'osImage', default='<none>')
        kernel = get_path(obj, 'status', 'nodeInfo', 'kernelVersion', default='<none>')
        runtime = get_path(obj, 'status', 'nodeInfo', 'containerRuntimeVersion', default='<none>')
        info = [os_img, kernel, runtime]
    return info

def matches_selector(obj, selector):
    if not selector: return True
    labels = get_path(obj, 'metadata', 'labels') or {}
    parts = selector.split(',')
    for part in parts:
        if '=' in part:
            k, v = part.split('=', 1)
            if labels.get(k) != v: return False
    return True

# ==============================================================================
# LINEAGE & HISTORY LOGIC (ONLY USED FOR --history)
# ==============================================================================

def is_noisy_user(username):
    for noise in NOISY_USERS:
        if noise in username: return True
    return False

def build_ownership_map(log_file):
    """Scans the log to build a map of who owns whom."""
    ownership = {}
    try:
        with open(log_file, 'r') as f:
            for line in f:
                if not line.strip(): continue
                try: entry = json.loads(line)
                except: continue
                
                verb = entry.get('verb')
                if verb not in ['create', 'update', 'patch']: continue
                
                obj_ref = entry.get('objectRef', {})
                res = obj_ref.get('resource')
                ns = obj_ref.get('namespace')
                name = obj_ref.get('name')
                if not res or not name: continue

                # Parse body to find ownerReferences
                raw_obj = entry.get('responseObject') or entry.get('requestObject')
                obj = safe_parse_json(raw_obj)
                
                owner_refs = get_path(obj, 'metadata', 'ownerReferences')
                if owner_refs and isinstance(owner_refs, list) and len(owner_refs) > 0:
                    owner = owner_refs[0]
                    owner_kind = owner.get('kind', '').lower() + 's' 
                    # Fix plurals
                    if owner_kind == 'replicasetss': owner_kind = 'replicasets'
                    if owner_kind == 'deployments': owner_kind = 'deployments'
                    
                    key = (res, ns, name)
                    ownership[key] = (owner_kind, owner.get('name'))
    except FileNotFoundError: pass
    return ownership

def print_lineage_history(log_file, start_res, namespace, start_name, time_limit):
    # 1. Build Ownership Map
    print("--- ðŸ” Analyzing Lineage (this may take a moment) ---")
    ownership_map = build_ownership_map(log_file)
    
    # 2. Trace Ancestors
    relevant_objects = set()
    curr_res, curr_name = start_res, start_name
    
    while True:
        key = (curr_res, namespace, curr_name)
        relevant_objects.add(key)
        
        parent = ownership_map.get(key)
        if not parent: break
        
        curr_res, curr_name = parent
        if curr_res in CLUSTER_SCOPED: 
            relevant_objects.add((curr_res, None, curr_name))
            break
    
    # 3. Fetch Events
    history = []
    try:
        with open(log_file, 'r') as f:
            for line in f:
                if not line.strip(): continue
                try: entry = json.loads(line)
                except: continue
                
                ts = entry.get('requestReceivedTimestamp')
                if time_limit and ts and ts > time_limit: continue
                
                verb = entry.get('verb')
                if verb not in ['create', 'update', 'patch', 'delete']: continue

                obj_ref = entry.get('objectRef', {})
                res = obj_ref.get('resource')
                ns = obj_ref.get('namespace')
                name = obj_ref.get('name')
                
                # Filter lineage
                if (res, ns, name) not in relevant_objects: continue

                user = entry.get('user', {}).get('username', 'unknown')
                
                # --- APPLY NOISE FILTER (Only for history) ---
                if is_noisy_user(user): continue

                code = entry.get('responseStatus', {}).get('code', 0)
                reason = entry.get('responseStatus', {}).get('reason', '')
                sub = obj_ref.get('subresource')
                
                # Formatting
                display_name = f"{RESOURCE_ALIASES.get(res, res)}/{name}"
                if sub: display_name += f"/{sub}"
                
                if user.startswith('system:serviceaccount:'): user = f"sa:{user.split(':')[-1]}"
                
                # --- STATUS TRANSLATION ---
                code_str = HTTP_STATUS_MAP.get(code, str(code))
                if code == 101 and sub:
                    if sub == 'exec': code_str = "Exec"
                    elif sub == 'portforward': code_str = "Port-Forward"
                    elif sub == 'attach': code_str = "Attach"

                history.append({
                    'ts': ts,
                    'verb': verb.upper(),
                    'object': display_name,
                    'user': user,
                    'code': code_str,
                    'reason': reason
                })

    except FileNotFoundError:
        print(f"Error: File '{log_file}' not found."); sys.exit(1)

    history.sort(key=lambda x: x.get('ts', ''))
    
    print(f"\n--- HISTORY & LINEAGE: {start_res}/{start_name} ---")
    headers = ["TIMESTAMP", "VERB", "OBJECT", "USER", "STATUS"]
    widths = [27, 8, 45, 30, 12]
    
    header_fmt = "  ".join([f"{{:<{w}}}" for w in widths])
    print(header_fmt.format(*headers))
    
    for h in history:
        obj_str = h['object']
        if len(obj_str) > 43: obj_str = "..." + obj_str[-40:]
        
        row = [h['ts'], h['verb'], obj_str, h['user'][:28], h['code']]
        print(header_fmt.format(*row))

# ==============================================================================
# MAIN LOGIC
# ==============================================================================

def process_audit_log(log_file, resources_to_fetch, target_ns, target_name, time_limit, selector, all_namespaces):
    state = {} 
    target_resources_set = set(resources_to_fetch)
    valid_events = []
    
    try:
        with open(log_file, 'r') as f:
            for line in f:
                if not line.strip(): continue
                try: entry = json.loads(line)
                except: continue
                
                ts = entry.get('requestReceivedTimestamp')
                if time_limit and ts and ts > time_limit: continue
                
                verb = entry.get('verb')
                if verb not in ['create', 'update', 'patch', 'delete']: continue
                if entry.get('responseStatus', {}).get('status') == 'Failure': continue
                
                obj_ref = entry.get('objectRef', {})
                resource = obj_ref.get('resource')
                ns = obj_ref.get('namespace')
                name = obj_ref.get('name')
                
                if not resource or not name: continue
                if resource not in target_resources_set: continue
                
                is_global = resource in CLUSTER_SCOPED
                if not is_global:
                    if not all_namespaces:
                        if target_ns and ns != target_ns: continue
                        if not target_ns and ns != 'default': continue
                
                if target_name and name != target_name: continue
                valid_events.append(entry)

    except FileNotFoundError:
        print(f"Error: File '{log_file}' not found."); sys.exit(1)

    valid_events.sort(key=lambda x: x.get('requestReceivedTimestamp', ''))

    for entry in valid_events:
        obj_ref = entry.get('objectRef', {})
        resource = obj_ref.get('resource')
        ns = obj_ref.get('namespace')
        name = obj_ref.get('name')
        verb = entry.get('verb')
        ts = entry.get('requestReceivedTimestamp')
        
        key = (resource, ns, name)
        
        if verb == 'delete':
            state.pop(key, None)
        else:
            raw_obj = entry.get('responseObject') or entry.get('requestObject')
            obj = safe_parse_json(raw_obj)
            
            if not obj:
                if resource in REDACTED_RESOURCES:
                    obj = {'kind': resource, 'metadata': {'name': name, 'namespace': ns, 'creationTimestamp': ts, 'labels': {}}, 'spec': {}, 'status': {}}
                else: continue 
            
            if key in state:
                old_created = get_path(state[key], 'metadata', 'creationTimestamp')
                if old_created and not get_path(obj, 'metadata', 'creationTimestamp'):
                    if 'metadata' not in obj: obj['metadata'] = {}
                    obj['metadata']['creationTimestamp'] = old_created

                if resource == 'pods':
                    old_ip = extract_pod_ip(state[key])
                    new_ip = extract_pod_ip(obj)
                    if old_ip != '<none>' and new_ip == '<none>':
                        if 'status' not in obj: obj['status'] = {}
                        if isinstance(obj['status'], str): obj['status'] = safe_parse_json(obj['status'])
                        obj['status']['podIP'] = old_ip
            state[key] = obj

    results = []
    for (res, ns, name), obj in state.items():
        if matches_selector(obj, selector):
            results.append((ns, name, obj, res))
    results.sort(key=lambda x: (x[0] or "", x[1]))
    return results

def main():
    parser = setup_args()
    args, unknown = parser.parse_known_args()
    for u in unknown:
        if u.startswith("-"): print(f"Error: unrecognized argument {u}"); sys.exit(1)
        args.args.append(u)
    
    if not args.args: parser.print_help(); sys.exit(1)
    if args.args[0] != "get": print(f"Error: Only 'get' is supported."); sys.exit(1)
    if len(args.args) < 2: print("Error: Specify resource."); sys.exit(1)

    resource_input = args.args[1]
    name_input = args.args[2] if len(args.args) > 2 else None
    
    if not os.path.exists(args.auditlog_file): print(f"Error: File '{args.auditlog_file}' not found."); sys.exit(1)
    args.time = normalize_time(args.time)

    req_res = RESOURCE_ALIASES.get(resource_input.lower(), resource_input.lower())
    
    # --- HISTORY MODE ---
    if args.history:
        if not name_input:
            print("Error: --history requires a specific object name (e.g., 'get pod mypod --history').")
            sys.exit(1)
        if req_res == 'all':
            print("Error: --history cannot be used with 'all'. Specify a single resource type.")
            sys.exit(1)
            
        print_lineage_history(args.auditlog_file, req_res, args.namespace, name_input, args.time)
        sys.exit(0)
    # --------------------

    if req_res == 'all':
        resources_to_fetch = ['nodes', 'pods', 'services', 'daemonsets', 'deployments', 'replicasets', 'statefulsets', 'jobs', 'cronjobs', 'routes', 'ingresses', 'networkpolicies', 'virtualmachines', 'virtualmachineinstances']
    else:
        resources_to_fetch = [req_res]

    all_results = process_audit_log(args.auditlog_file, resources_to_fetch, args.namespace, name_input, args.time, args.selector, args.all_namespaces)
    
    results_by_resource = {r: [] for r in resources_to_fetch}
    for ns, name, obj, res in all_results:
        if res in results_by_resource: results_by_resource[res].append((ns, name, obj))

    for resource in resources_to_fetch:
        results = results_by_resource.get(resource, [])
        if not results: continue

        if len(resources_to_fetch) > 1: print(f"\n--- {resource.upper()} ---")

        if args.output == "yaml":
            for _, _, obj in results: print("---"); print(yaml.dump(safe_parse_json(obj)))
        elif args.output == "json":
            for _, _, obj in results: print(json.dumps(obj))
        else:
            is_cluster_scoped = resource in CLUSTER_SCOPED
            
            # --- HEADER LOGIC ---
            headers = ["NAMESPACE", "NAME", "AGE"]
            if is_cluster_scoped: headers = ["NAME", "AGE"]
            
            # Inject STATUS only for Pods
            if resource == 'pods':
                headers.insert(2 if not is_cluster_scoped else 1, "STATUS")

            if args.output == "wide":
                if resource == 'pods': headers.extend(["IP", "NODE"])
                elif resource == 'services': headers.extend(["TYPE", "CLUSTER-IP", "SELECTOR"])
                elif resource == 'nodes': headers.extend(["OS-IMAGE", "KERNEL-VERSION", "RUNTIME"])
            if args.show_labels: headers.append("LABELS")

            rows_to_print = []
            for ns, name, obj in results:
                payload = safe_parse_json(obj)
                created = get_path(payload, 'metadata', 'creationTimestamp')
                age = calculate_age(created, args.time)
                display_name = name
                
                # --- ROW LOGIC ---
                row = []
                if is_cluster_scoped:
                    row = [display_name, age]
                    if resource == 'pods': row.insert(1, extract_status(payload))
                else:
                    row = [ns if ns else "-", display_name, age]
                    if resource == 'pods': row.insert(2, extract_status(payload))
                
                if args.output == "wide": row.extend(extract_wide_info(resource, payload))
                if args.show_labels: row.append(extract_labels(payload))
                rows_to_print.append(row)

            widths = [len(h) for h in headers]
            for row in rows_to_print:
                for i, val in enumerate(row):
                    if i < len(widths): widths[i] = max(widths[i], len(str(val)))

            header_fmt = "  ".join([f"{{:<{w}}}" for w in widths])
            print(header_fmt.format(*headers))
            for row in rows_to_print: print(header_fmt.format(*row))

if __name__ == "__main__":
    main()